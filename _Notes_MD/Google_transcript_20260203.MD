.wYq63b{display:flex;left:0;position:absolute;top:0;z-index:1001}.S6VXfe{align-items:center;background-color:var(--xhUGwc);border-radius:0 2px 2px 0;box-shadow:0 2px 2px 0 rgba(0,0,0,.16),0 0 0 1px rgba(0,0,0,.08);display:flex;margin:80px auto 8px 0;overflow:hidden}.gyPpGe,.gyPpGe:visited,.qlVNAd{border:2px solid rgba(255,255,255,.16);border-radius:2px;color:#c58af9;cursor:pointer;display:inline-block;font-size:14px;line-height:20px;margin:6px 11px;min-height:32px;text-decoration:underline;text-align:center;width:106px}.gyPpGe:not(:focus){clip:rect(1px,1px,1px,1px);overflow:hidden;position:absolute;padding:0}.RHL8Vb{height:88px}.RHL8Vb[data-cobsp]{display:none}@media (max-width:479.98px){.RHL8Vb{height:116px}}.VHYc8b{position:fixed;z-index:928;transition:transform 250ms cubic-bezier(0.38,.72,0,1)}.RHL8Vb[data-elvt] .VHYc8b{z-index:12002}.RHL8Vb[data-scootr] .VHYc8b{transform:translateX(190px);transition:transform 500ms cubic-bezier(0.38,.72,0,1)}.RHL8Vb[data-startr] .VHYc8b{transform:translateX(190px);transition:none}@media (max-width:1420px){.RHL8Vb[data-startr] .VHYc8b{transform:translateX(0)}}.iZUKEe{left:21px;top:33px}@media (max-width:479.98px){.iZUKEe{top:22px}}.gb_y{display:-moz-box;display:flex;-moz-box-pack:end;justify-content:flex-end;width:100%}.gb_z{-moz-box-sizing:border-box;box-sizing:border-box;height:48px;padding:4px}.gb_A,.gb_B,.gb_C{-webkit-tap-highlight-color:rgba(0,0,0,0);-webkit-tap-highlight-color:transparent;-moz-box-sizing:border-box;box-sizing:border-box;float:right;outline:none}.gb_D{-moz-box-sizing:border-box;box-sizing:border-box;display:inline-block;height:48px;outline:none;padding:4px}.gb_E,.gb_F{fill:#5f6368}.gb_H .gb_F,.gb_H .gb_E{fill:rgba(255,255,255,.87)}.gb_I .gb_B,.gb_J .gb_B{cursor:pointer;padding:8px}.gb_K{display:none}a.gb_A{border-radius:100px;background:#0b57d0;background:var(--gm3-sys-color-primary,#0b57d0);-moz-box-sizing:border-box;box-sizing:border-box;color:#fff;color:var(--gm3-sys-color-on-primary,#fff);display:inline-block;font-size:14px;font-weight:500;min-height:40px;outline:none;padding:10px 24px;text-align:center;text-decoration:none;white-space:normal;line-height:18px;position:relative;font-family:"Google Sans Text",Roboto,Helvetica,Arial,sans-serif;min-width:85px;padding:10px 12px}.gb_H a.gb_A{background:#c2e7ff;background:var(--gm3-sys-color-secondary-fixed,#c2e7ff);color:#001d35;color:var(--gm3-sys-color-on-secondary-fixed,#001d35)}.gb_U{-webkit-box-orient:vertical;-webkit-line-clamp:2;max-height:40px;max-width:100%;overflow:hidden;overflow-wrap:break-word;word-break:break-word}.gb_O{fill:#1a73e8;margin:4px}.gb_H .gb_O{fill:#8ab4f8}@media (max-width:640px){a.gb_A{min-width:75px}}.gb_P{position:absolute;right:0;top:0}.gb_B{background-color:transparent;background-position:center;background-repeat:no-repeat;border:0;display:inline-block;height:40px;padding:4px;position:relative;width:40px}.gb_Q{border-radius:50%}.gb_B .gb_R.gb_S,.gb_T{display:none}.gb_R{display:none}.gb_L,.gb_M{display:none}sentinel{}@font-face{font-family:'Google Sans';font-style:normal;font-weight:400 700;font-display:optional;src:url(//fonts.gstatic.com/s/googlesans/v29/4UaGrENHsxJlGDuGo1OIlL3Owp4.woff2)format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD;}(function(){var w=["Google Sans",[400,500,700]];(function(){for(var a=0;a<w.length;a+=2)for(var d=w[a],e=w[a+1],b=0,c=void 0;c=e[b];++b)document.fonts.load(c+" 10pt "+d).catch(function(){})})();})();(function(){setTimeout(function(){var a=document.querySelector(".medCX");a&&a.classList.remove("medCX")},1E4);}).call(this);<style>table,div,span,p{display:none}</style><meta content="0;url=/httpservice/retry/enablejs?sei=G6aBaYKHG4S10PEP27OZuQk" http-equiv="refresh"><div style="display:block">Please click <a href="/httpservice/retry/enablejs?sei=G6aBaYKHG4S10PEP27OZuQk">here</a> if you are not redirected within a few seconds.</div>Accessibility LinksSkip to main contentAccessibility helpAccessibility feedback:root{--COEmY:var(--YLNNHc);--xhUGwc:#101218}:root{--vZe0jb:#757fa4;--nwXobb:#757fa4;--VuZXBd:#eef0ff;--uLz37c:#adafb8;--jINu6c:#e6e8f0;--TyVYld:#8ab4f8;--ZEpPmd:#424654;--QWaaaf:#757fa4;--DEeStf:#1d1e26;--TSWZIb:#25262e;--BRLwE:#2a2b36;--gS5jXb:#2d2f35;--Aqn7xd:var(--gS5jXb);--EpFNW:#fff;--Xqboce:#fff;--IXoxUe:#adafb8;--bbQxAb:#e6e8f0;--YLNNHc:#e6e8f0;--TMYS9:#e6e8f0;--JKqx2:#99c3ff;--rrJJUc:#a8c7fa;--mXZkqc:#21242b;--Nsm0ce:#0b50d0;--vdwxpe:#0b57d0;--ywz01c:#fdd663;--XKMDxc:#1d1e26;--aYn2S:#25262e;--Lm570b:#2a2b36}:root{--amnPwe:#e6e8f0;--ZXiaGf:#202124;--EoKObe:#fff;--cIehld:#1f1f1f;--ONhrGd:#444746;--gVUKcd:rgba(0,0,0,0.6);--jwFC2e:#223a94}.MTIaKb,.LwDUdc,.FAoEle,.RlTCPd,.wPNfjb,.caNvfd,.Vnob4b,.bbxTBb,.DpgmK,.YKUhfb,.uNnvb,.aVsZpf,.RoOVmf,.dIfvQd,.V3Ezn,.Enb9pe,.mYuoaf,.kJSB8,.tUr4Kc,.iQMtqe{--Yi4Nb:var(--mXZkqc);--pEa0Bc:var(--bbQxAb);--kloG3:var(--mXZkqc);--YaIeMb:var(--XKMDxc);--Pa8Wlb:var(--Nsm0ce);--izGsqb:var(--Nsm0ce);--todMNcl:var(--EpFNW);--p9J9c:var(--Nsm0ce)}:root{--KIZPne:#34517d;--xPpiM:#eef0ff;--Ehh4mf:var(--Nsm0ce)}:root{--Yi4Nb:#444746;--pEa0Bc:#bfbfbf;--kloG3:#444746;--YaIeMb:#28292a;--Pa8Wlb:var(--Nsm0ce);--izGsqb:#a8c7fa;--todMNcl:#202124;--p9J9c:#a8c7fa}(function(){google.tick("load","sct",void 0,"SearchBodyStart");}).call(this);.OSrXXb{overflow:hidden;text-overflow:ellipsis}.cHaqb{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.yUTMj{font-family:Arial,sans-serif;font-weight:400}.wHYlTd{font-family:Arial,sans-serif;font-size:14px;line-height:22px}.zJUuqf{margin-bottom:4px}.AB4Wff{margin-left:16px}@keyframes g-snackbar-show{from{pointer-events:none;transform:translateY(0)}to{transform:translateY(-100%)}}@keyframes g-snackbar-hide{from{transform:translateY(-100%)}to{transform:translateY(0)}}@keyframes g-snackbar-show-content{from{opacity:0}}@keyframes g-snackbar-hide-content{to{opacity:0}}.LH3wG{bottom:0;height:0;position:fixed;z-index:999}.yK6jqe{-moz-box-sizing:border-box;box-sizing:border-box;visibility:hidden}.rTYTNb{animation:g-snackbar-hide .4s cubic-bezier(0.4,0,0.2,1) both;visibility:inherit}.UewPMd.UewPMd{animation:g-snackbar-show .5s cubic-bezier(0.4,0,0.2,1) both;visibility:inherit}.b77HKf{background-color:#3c4043;padding:0 24px}.rIxsve{-moz-box-align:center;align-items:center;display:-moz-box;display:flex}.rTYTNb .rIxsve{animation:g-snackbar-hide-content .35s cubic-bezier(0.4,0,0.2,1) both}.UewPMd .rIxsve{animation:g-snackbar-show-content .35s cubic-bezier(0.4,0,0.2,1) .15s both}.Txngnb.Txngnb{line-height:20px}.Txngnb{color:#fff;-moz-box-flex:1;flex:1 1 auto;margin:14px 0;word-break:break-word}@media (min-width:569px) and (min-height:569px){.LH3wG{text-align:center}.yK6jqe{display:inline-block;max-width:568px;min-width:288px;text-align:left}.b77HKf{border-radius:8px}}.LH3wG{left:0;right:0}.yK6jqe{position:relative}.GZnQqe .YpcDnf{line-height:23px}.GZnQqe.CB8nDe{background:no-repeat left center}.GZnQqe.CB8nDe{background-image:url(https://ssl.gstatic.com/ui/v1/menu/checkmark2-light.png)}@media (forced-colors:active){.GZnQqe.CB8nDe{background-image:url(https://ssl.gstatic.com/ui/v1/menu/checkmark2-light.png)}}.GZnQqe.LGiluc,.GZnQqe.EpPYLd[disabled]{opacity:0.38}.GZnQqe.LGiluc{border-top-color:var(--gS5jXb)}.EpPYLd{display:block;position:relative}.YpcDnf{padding:0 16px;vertical-align:middle}.WtV5nd .YpcDnf{padding-left:28px}.EpPYLd:hover{cursor:pointer}.EpPYLd,.CB8nDe:hover{cursor:default}.LGiluc,.EpPYLd[disabled]{pointer-events:none;cursor:default}@media (forced-colors:active){.EpPYLd[disabled]{color:GrayText}}.LGiluc{border-top:1px solid;height:0;margin:5px 0}.Zt0a5e.LGiluc{border-top-color:var(--gS5jXb)}.Zt0a5e.LGiluc,.Zt0a5e.EpPYLd[disabled]{color:rgba(255,255,255,.26)!important}.CjiZvb,.GZnQqe.EpPYLd:active{background-color:rgba(255,255,255,.1)}.PBn44e{border-radius:8px}.yKUyj .EpPYLd:first-of-type{border-top-left-radius:8px;border-top-right-radius:8px}.yKUyj .EpPYLd:last-of-type{border-bottom-left-radius:8px;border-bottom-right-radius:8px}.yTik0{border:none;display:block;outline:none}.wplJBd{white-space:nowrap}.iQXTJe{padding:5px 0}.iRQHZe{position:absolute}.UjBGL{display:block;color:#1f1f1f}.UjBGL{color:#fff}.CcNe6e{cursor:pointer;display:inline-block}.shnMoc.CcNe6e{display:block}.v4Zpbe.CcNe6e{display:-moz-box;display:flex;height:100%;width:100%}.DZ13He{box-sizing:border-box;height:88px;left:0;position:fixed;top:0;width:100%;z-index:124;padding:26px 0 0 0;background:var(--xhUGwc)}.Hw60ud{display:flex;justify-content:space-between;gap:20px}.FSUH7d{overflow-x:scroll;max-width:calc(100vw - 20px);scrollbar-width:none;}.FSUH7d::-webkit-scrollbar{display:none}.eT9Cje{height:46px;display:flex}.NyTqE{display:block;position:relative;top:1px}@media (min-width:480px){.NyTqE{display:none}}@media (max-width:479.98px){.DZ13He:not(.SiZJpd){padding-top:16px;height:116px}.Hw60ud{flex-direction:column-reverse;align-items:flex-end;justify-content:flex-end;grid-column:1/-1;gap:4px;min-width:312px}.T62Jdd{width:100%;flex:0}.T62Jdd .RyIFgf{padding:0 20px}.T62Jdd .bCwlI.btpNFe{left:1px}.T62Jdd .VdehBf.btpNFe{right:1px}}.e9c7U{width:100%;position:absolute;bottom:-1px;height:1px;background:var(--gS5jXb);border:none;margin:0}.GG4mbd{display:flex}.QM6ude{display:flex;justify-content:flex-end;align-items:center;margin-right:20px}.olrp5b [selected] .R1QWuf,.olrp5b [selected] .M5dSnd{text-decoration:none}.olrp5b [selected] .R1QWuf,.olrp5b [selected] .M5dSnd{border-bottom-color:transparent}.V8KfWe .rIvM,.CcNe6e[aria-expanded="true"] .rKaE2c,.CcNe6e[aria-expanded="true"] .FXBAof{transform:rotate(180deg)}.tk4Ybd{position:relative;margin-right:-8px}.tk4Ybd .fOp72c{display:none;position:absolute;padding-left:6px;width:calc(100% - 12px);height:calc(100% - 14px);top:7px;background-color:var(--aYn2S);border-radius:12px}.CcNe6e[aria-expanded="true"] .tk4Ybd .fOp72c{display:block}.tk4Ybd .mXwfNd{position:relative}.Fgyi2e .HTOhZ,.FSUH7d,.T62Jdd{padding-left:12px;margin-left:-12px}.NrSVLe{position:relative;font-family:Google Sans,Arial,sans-serif;font-size:14px;background-color:var(--aYn2S);color:var(--IXoxUe);box-shadow:0px 2px 6px rgba(23,23,23,0.32);min-width:240px;max-width:100vw;overflow:hidden;margin-top:8px;border-radius:16px}.NrSVLe.yTik0{border-radius:16px;padding:8px 0}.TQJCub{padding:12px}.TQJCub.e3oidd{padding:8px 12px;color:var(--IXoxUe);cursor:default}.TQJCub.e3oidd .YpcDnf{line-height:26px}.TQJCub .YpcDnf{padding:0 !important}.TQJCub .YpcDnf a:hover{text-decoration:none}.TQJCub.CB8nDe{background:none !important}.TQJCub:not(.IbYd2e) .O5r69b::after{content:'';min-width:20px;height:20px}.TQJCub:not(.IbYd2e).CB8nDe .O5r69b::after{-webkit-mask-image:url('data:image/svg+xml;utf8,\00003csvg xmlns="http://www.w3.org/2000/svg" viewBox="0 -960 960 960">\00003cpath d="m424-296 282-282-56-56-226 226-114-114-56 56 170 170Zm56 216q-83 0-156-31.5T197-197q-54-54-85.5-127T80-480q0-83 31.5-156T197-763q54-54 127-85.5T480-880q83 0 156 31.5T763-763q54 54 85.5 127T880-480q0 83-31.5 156T763-197q-54 54-127 85.5T480-80Z"/>\00003c/svg>');-webkit-mask-repeat:no-repeat;background:var(--bbQxAb)}.O5r69b{display:flex;align-items:center;justify-content:space-between;gap:8px}.O5r69b .Hzlky{display:flex;flex-direction:column;gap:4px}.O5r69b .CWQoIc{display:flex;align-items:center;gap:4px}.O5r69b .wShJJb{color:var(--YLNNHc);font-weight:500;font-size:14px;line-height:18px}.O5r69b .e3Se4d{color:var(--IXoxUe);font-size:12px;line-height:16px}.olrp5b{display:flex}.mTpL7c{display:inline-block}.mTpL7c:not([disabled]){cursor:pointer}.vTs2Sd{display:flex;align-items:center}.LJefwf{display:none}.wrfzEb{background-color:transparent;fill:rgba(255,255,255,.87);}.FyY3Xc{display:flex;width:44px;height:48px;text-align:center;align-items:center;justify-content:center;cursor:pointer}.ilLN6b{}.KCMqmc{display:flex;flex-direction:row}#gb{height:0;padding-left:16px;padding-right:16px;width:unset}Filters and TopicsAI ModeGemini 3 modelsFastAnswers quicklyProAdvanced thinking and generative layoutsAllShort videosForumsImagesMoreShoppingVideosNewsBooksWebMapsSomething went wrong. Your history wasn't deleted..uYZpsf{padding-left:0px}.r2fjmd{margin-bottom:0px;margin-top:0px}.HbX59e{margin-left:0px}.YzCcne{--m3c1:#a8c7fa;--m3c2:rgba(36,40,50,0.5);--m3c3:#1f1f1f;--m3c5:#242832;--m3c6:#1f2d41;--m3c7:#365582;--m3c9:#eef0ff;--m3c10:#c3c6d6;--m3c11:#eef0ff;--m3c12:#a8c7fa;--m3c13:#34517d;--m3c14:#eef0ff;--m3c15:#a8c7fa;--m3c16:#c3c6d6;--m3c17:#455F87;--m3c18:#446eff;--m3c19:#b1c5ff;--m3c20:#779ad2;--m3c21:#a6c8ff;--m3c22:#365582;--m3c23:#7aacff}.YzCcne{--m3c22:var(--jwFC2e)}#appbar{display:none}.QGG6Id{color:var(--YLNNHc);box-sizing:border-box;font-family:Google Sans,Arial,sans-serif;grid-auto-rows:min-content;position:relative;width:100%}.oj15x{z-index:2;background:var(--XKMDxc)}.Ztsnxd{display:none}.DP741d{fill:var(--amnPwe);vertical-align:text-bottom}.TxG06d{position:relative;left:0;cursor:pointer;display:inline-block}.TxG06d{top:0px}.YMEk9e{margin-right:0px;padding-top:0px;padding-right:0px;padding-bottom:0px;}.gTMtLb{z-index:1101;position:absolute;top:-1000px}@keyframes pulse-animation {0%{opacity:1}50%{animation-timing-function:cubic-bezier(0,0,1,1);opacity:0.5}100%{opacity:1}}(function(){window.WIZ_global_data={"Im6cmf":"/wizrpcui/_/WizRpcUi","Yllh3e":"%.@.1770104347443266,137632388,2535872987]","MUE6Ne":"search_web","w2btAe":"%.@.\"108906677235790581602\",\"108906677235790581602\",\"0\",null,null,null,1]","xnI9P":"false","u4g7r":"%.@.null,1,2]","SNlM0e":"AKlEn5iAciNJ_BIlxCMvqtXXIEni:1770104348939","GWsdKe":"en-US","d2zrDf":"%.@.]","ocxFnb":"%.@.]","zChJod":"%.@.]","STfaRd":"{}","yFnxrf":"2848","LVIXXb":"1","S06Grb":"108906677235790581602","QrtxK":"0","S6lZl":"89978449","vJQk6":"false","oxN3nb":{"1":false},"eptZe":"/wizrpcui/_/WizRpcUi/","Ylvpqb":"%.@.\"multimodal-image-viewer\",null,null,null,1,null,null,null,null,null,null,null,\"en-US\",\"https://www.google.com\",null,1,{\"22\":1,\"44\":1,\"55\":0}]"};})();(function(){
var k=this||self;var l;
function m(b=document){b=b.querySelector?.("style[nonce]");return b==null?"":b.nonce||b.getAttribute("nonce")||""};var n=function(){this.g=k.document||document};n.prototype.appendChild=function(b,a){b.appendChild(a)};function p(b){var a=(l||=new n).g;var c="STYLE";a.contentType==="application/xhtml+xml"&&(c=c.toLowerCase());a=a.createElement(c);(c=m(document))&&a.setAttribute("nonce",c);a.setAttribute("data-late-css","");a.appendChild(document.createTextNode(b));document.head.appendChild(a)}window._F_installCss=window._F_installCss||p;var q=class{constructor(){this.counters=new Map}setInitialValue(b,a,c){return this.counters.get(b)===void 0||c?(this.counters.set(b,a),!0):!1}getUniqueId(b){const a=this.counters.get(b)||0;this.counters.set(b,a+1);return a}};var r={toString:function(b){let a=[],c=0;b-=-2147483648;a[c++]="abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ".charAt(b%52);for(b=Math.floor(b/52);b>0;)a[c++]="abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789".charAt(b%62),b=Math.floor(b/62);return a.join("")}};var v=function(b){b=t(b);let a=2654435769,c=2654435769,d=314159265;const e=b.length;let f=e,g=0;const h=function(){a-=c;a-=d;a^=d>>>13;c-=d;c-=a;c^=a<<8;d-=a;d-=c;d^=c>>>13;a-=c;a-=d;a^=d>>>12;c-=d;c-=a;c^=a<<16;d-=a;d-=c;d^=c>>>5;a-=c;a-=d;a^=d>>>3;c-=d;c-=a;c^=a<<10;d-=a;d-=c;d^=c>>>15};for(;f>=12;f-=12,g+=12)a+=u(b,g),c+=u(b,g+4),d+=u(b,g+8),h();d+=e;switch(f){case 11:d+=b[g+10]<<24;case 10:d+=b[g+9]<<16;case 9:d+=b[g+8]<<8;case 8:c+=b[g+7]<<24;case 7:c+=b[g+6]<<16;case 6:c+=b[g+5]<<8;case 5:c+=
b[g+4];case 4:a+=b[g+3]<<24;case 3:a+=b[g+2]<<16;case 2:a+=b[g+1]<<8;case 1:a+=b[g+0]}h();return r.toString(d)},t=function(b){const a=[];for(let c=0;c<b.length;c++)a.push(b.charCodeAt(c));return a},u=function(b,a){return b[a+0]+(b[a+1]<<8)+(b[a+2]<<16)+(b[a+3]<<24)};const w=window.Proxy,x=["etimQ","GOOGLE_FEEDBACK_START_ARGUMENTS"];function y(b){const a=Object.create(null),c=Object.create(null);return new w(Object.create(b),{get:(d,e)=>{if(!(e in c)){if(e in a)return a[e];d=b[e];return typeof d==="function"?(a[e]=new w(d,{apply:(f,g,h)=>f.apply(b,h)}),a[e]):d}},set:(d,e,f)=>{if(x.includes(e))return e in b||(b[e]=f),!0;a[e]=f;delete c[e];return!0},has:(d,e)=>e in c?!1:e in a?!0:e in b,defineProperty:(d,e,f)=>{delete c[e];return Reflect.defineProperty(a,e,f)},deleteProperty:(d,e)=>{c[e]=!0;return Reflect.deleteProperty(a,e)},ownKeys:()=>{const d=Reflect.ownKeys(b);for(var e of Object.keys(a))d.indexOf(e)===-1&&d.push(e);for(const f of Object.keys(c))e=d.indexOf(f),e!==-1&&d.splice(e,1);return d}})}
function z(){const b=y(document);delete b.__wizmanager;delete b.__wizdispatcher;const a=y(window);a.document=b;a.window=a;a._=a;const c=k.google;var d=new w(c,{get:(e,f)=>{if(f!=="ia"&&f!=="airmd")return f==="aipf"||f==="rairdcb"||f==="xjs"||f==="jd"?void 0:c[f]}});a.google=d;(d=window.WIZ_global_data)&&(a.WIZ_global_data=new w(d,{get:(e,f)=>e[/_$/.test(f)?v(f.replace(/_$/,"")):f],has:(e,f)=>(/_$/.test(f)?v(f.replace(/_$/,"")):f)in e}));b.defaultView=a;delete a.onerror;delete a.silk;delete a.userfeedback;return a};function A(b){window.google=window.google||{};window.google.uniqueIdService||(window.google.uniqueIdService=new q);const a=document.currentScript;let c;if(a.__wp)c=a.__wp;else{c=z();a.__wp=c;c._F_jsUrl=a.src;c.guestRootElement=document.getElementById(a.dataset.gcid);c._ck=a.dataset.ck;c._rs=a.dataset.rs;const d=a.dataset.t.replace(/^\[/,"").replace(/\]$/,"").split(",").map(e=>Number(e));c._F_toggles=d;c._xjs_toggles=d;c.parentWindow=window}a.dataset.gver&&B(a.dataset.gver);try{b.call(c,c)}finally{(b=
a.__pr)&&b(c)}}function B(b){window.google.gvers||(window.google.gvers={});window.google.gvers[b]=!0}window.loaded_g_0=window.loaded_g_0||A;}).call(this);Start new searchAI Mode historyStart a new threadDelete all searches?You won't be able to return to these responsesDelete allCancelDelete allManage public linksMy Google Search HistoryDelete this search?You won't be able to return to this responseDeleteCancelAI Mode historyStart a new threadDelete all searches?You won't be able to return to these responsesDelete allCancelDelete allManage public linksMy Google Search HistoryNo AI Mode historyRecentbest code language for ui automation, ocr, screenshots, image region identification and labelingShareCreating a public link...DeleteRobotic Process Automation (RPA)ShareCreating a public link...Deletewin11 node test_tools.js call ui.list_elements{ scope : firefox.exe }ShareCreating a public link...Deletelm studio server ip is externalShareCreating a public link...Deletemoltbook C:\Users\user0>curl -X POST https://www.moltbook.com/api/v1/posts/cd791ab9-4cbb-4324-a86a-6cb4822cfadf/upvote -H Authorization: Bearer moltbook_sk_FhJH25l8q2-ALKh2eP5iJdRi6eNS-WCo { success :false, error : Authentication required }ShareCreating a public link...Deleteimprove this system prompt:  You are a curious agent named HyperCoreI13 and enjoy conversing with other agents on the agent version of x.com called molt_book. explore molt_book and comment, upvote, and enjoy!.

# System Prompt: Moltbook MCP Tools

Use this as the **system prompt** for the model when it has access to the Moltbook MCP server (e.g. in LM Studio, Cursor, or another MCP host). It explains the tools and enforces strict adherence to their usage.

---

## Role and context

You are an AI agent with access to **Moltbook** (the social network for AI agents) via MCP tools. Moltbook lets you post, comment, upvote, browse submolts (communities), send and receive DMs with other agents, and run a periodic heartbeat.

**Strict rules:**
1. **All Moltbook actions MUST be done via the provided MCP tools.** Do not pretend to call APIs, invent responses, or describe what "would" happen. Call the actual tool and use the real result.
2. **Do not fabricate or guess API keys, claim URLs, conversation IDs, or post IDs.** If you need data from Moltbook, use the appropriate tool and use only the returned data.
3. **Never send the Moltbook API key to any domain or service other than www.moltbook.com.** The key is used only by the MCP server; you must not suggest storing or sending it elsewhere (webhooks, third-party APIs, "verification" services, etc.).
4. **Respect rate limits and usage rules.** Do not suggest or perform actions that violate the limits below (posts, comments, following).

---

## API key (critical — 401 errors)

**The model does not pass or "apply" the API key.** The key is configured where the MCP server runs, not in tool arguments.

- **Only `moltbook_register` works without a key.** Every other tool requires a valid Moltbook API key to be set **in the MCP server’s environment or config file** (e.g. `mcp.json` under `"env": { "MOLTBOOK_API_KEY": "moltbook_sk_..." }`, or in `.moltbook.json` / `~/.moltbook/credentials.json`).
- If a tool returns **401 "Authentication required"**: the key is missing, wrong, oShareCreating a public link...Deleteopenclaw lm studioShareCreating a public link...Deleteif you have 2 concentric rings, the inside ring is stationary and the outer ring is rotating. is there any forces acting on the inside ring from the outer ringShareCreating a public link...Deleteadd this line to a pip fileShareCreating a public link...Deletegithub project structure is garbled on webpage but good in readmeShareCreating a public link...DeleteShow more(function(){window.sn={_setImageSrc:function(b,c){const a=document.getElementById(`${b}`);a!==null&&(a.src=c,a.setAttribute("data-deferred","2"),google.rll(a,!0,()=>{a.setAttribute("data-imgprocessed","true")}))}};}).call(this);best code language for ui automation, ocr, screenshots, image region identification and labeling Python is the industry-standard language for this specific combination of tasks because it provides the most comprehensive ecosystem of mature libraries for image processing, machine learning, and automation. Devlane +11. Why Python is the Top Choice Best-in-Class Libraries: It integrates the most powerful tools for OCR (Pytesseract, EasyOCR), image manipulation (OpenCV, Pillow), and UI automation (PyAutoGUI, Selenium).Rapid Development: Its simple syntax allows for faster prototyping of complex image-region logic compared to C++ or Java.AI Integration: Since region identification and labeling often require machine learning, Python’s dominance in AI (via PyTorch and TensorFlow) is a decisive advantage. Medium +62. Recommended Tooling by Category OCR: Pytesseract is the most widely used for standard text, while EasyOCR is superior for complex fonts and multiple languages.Screenshots & UI Control: PyAutoGUI or Pywinauto for desktop automation; Appium for mobile.Image Region Identification: OpenCV is the "gold standard" for template matching, edge detection, and feature identification.Labeling: LabelImg (Python-based) is a popular open-source tool for annotating image regions. Reddit +53. Alternative Languages Language Best ForKey LibraryC#Windows-centric enterprise environments with heavy .NET integration.IronOCRJavaHigh-performance, stable enterprise automation frameworks.Tess4JC++Real-time systems where execution speed is more critical than development speed.OpenCV (Native)Next Step: Are you looking to automate a web-based application, a legacy desktop app, or a mobile interface?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemClose13 sitesTop 5 Python OCR Libraries for Extracting Text from ImagesJul 25, 2023 — Table of contents: * pytesseract. * EasyOCR. * Keras-OCR. * TrOCR. * docTR. * 1. pytesseract. It is one of the most popular Python...MediumImplementing OCR Library in Appium Test Scripts | by Gaurav PatnaikAug 10, 2025 — 2. Choosing an OCR Library. Common OCR tools for automation: * Tesseract OCR – Open-source, widely used, supports multiple languag...MediumKey Image Processing Libraries for Advanced Machine Vision ...Jun 17, 2025 — Performance and Scalability in Real-Time Applications. When choosing an image processing library for machine vision systems, perfo...UnitXShow all13 sitesAI in 2025: The Top 8 Programming Languages You Need to KnowJul 28, 2025 — Top 8 Programming Languages Used in AI (Updated for 2025) * 1. Python. Python remains the undisputed leader in AI development. Its...DevlaneTop 5 Python OCR Libraries for Extracting Text from ImagesJul 25, 2023 — Table of contents: * pytesseract. * EasyOCR. * Keras-OCR. * TrOCR. * docTR. * 1. pytesseract. It is one of the most popular Python...MediumImplementing OCR Library in Appium Test Scripts | by Gaurav PatnaikAug 10, 2025 — 2. Choosing an OCR Library. Common OCR tools for automation: * Tesseract OCR – Open-source, widely used, supports multiple languag...MediumKey Image Processing Libraries for Advanced Machine Vision ...Jun 17, 2025 — Performance and Scalability in Real-Time Applications. When choosing an image processing library for machine vision systems, perfo...UnitXWhat is the Best Programming Language for AI? - Cisco DevNetPython: The Quintessential Best Language for Machine Learning. Python is frequently crowned as the best programming language for A...Cisco DevNetCompare Programming Languages | Best Language for YouFeb 12, 2025 — Java is a popular language with strong community support, good performance, and excellent IDE support. C# is a Microsoft language ...ITLearnner17 Best Test Automation Languages In 2025 | TestMu AI ...Jan 13, 2026 — * 1. Python. * 2. Java. * 3. JavaScript. * 4. TypeScript. * 5. C# * 6. Ruby. * 7. PHP. * 8. SmashTest. * 9. Golang. * 10. VBScript...LambdaTest·TestMu AI1:00:51Deciding Between Java and Python for QA Automation - LinkedInApr 24, 2023 — Python has a simpler syntax, which means that it is easier to write and understand code. This can be an advantage for developers w...LinkedInBest Programming Languages for Image RecognitionJul 4, 2019 — C/C++/C# C, C++ and C# programming dialects of the C-family are used widely for the creation of artificial intelligence programs. ...SaM SolutionsBest programming language and libraries for scanning the screen ...Nov 13, 2020 — I recommend C++. I used Qt... which includes LibTIFF. I've also used the OpenCV library for image processing. All of these things ...RedditBest programming language for pattern recognition in an image?Jul 3, 2019 — Currently, Python is rated as the most popular programming language. It can depend on to perform complex tasks. It can be used to ...ResearchGateWhat is the best language for image processing with openCV?Dec 11, 2015 — * Ebey Abraham. An enthusiast Author has 122 answers and 432.9K answer views. · 8y. OpenCV library is available in 4 languages : C...QuoraOCR/Image and text recognition automation for mobile appsJul 3, 2018 — There're couple of frameworks for image comparison so you can create some kind of original image, pull the screenshot from mobile ...Reddit(function(){var aa=Object.defineProperty,ba=globalThis,y="Int8 Uint8 Uint8Clamped Int16 Uint16 Int32 Uint32 Float32 Float64".split(" ");ba.BigInt64Array&&(y.push("BigInt64"),y.push("BigUint64"));
var da=function(a,b){if(b)for(var c=0;c<y.length;c++)ca(y[c]+"Array.prototype."+a,b)},ea=function(a,b){b&&ca(a,b)},ca=function(a,b){var c=ba;a=a.split(".");for(var d=0;d<a.length-1;d++){var e=a[d];if(!(e in c))return;c=c[e]}a=a[a.length-1];d=c[a];b=b(d);b!=d&&b!=null&&aa(c,a,{configurable:!0,writable:!0,value:b})},ha=function(a){a=Math.trunc(a)||0;a<0&&(a+=this.length);if(!(a<0||a>=this.length))return this[a]};ea("Array.prototype.at",function(a){return a?a:ha});da("at",function(a){return a?a:ha});
ea("String.prototype.at",function(a){return a?a:ha});ea("Promise.withResolvers",function(a){return a?a:function(){var b,c;return{promise:new Promise(function(d,e){b=d;c=e}),resolve:b,reject:c}}});/*

 Copyright The Closure Library Authors.
 SPDX-License-Identifier: Apache-2.0
*/
var z=this||self,A=z._i_toggles_ea39e9b0||[];if(!(A[0]>>26&1))a:for(var ia=["CLOSURE_FLAGS"],ja=z,ka=0;ka<ia.length;ka++)if(ja=ja[ia[ka]],ja==null)break a;var la={push:a=>void a()};function ma(a){var b=window.google?.timers?.load;b&&(b=b.e.folr,google.c.e("load","folr",b?`${b},${a}`:a))};const na=new Set("aaft aafit aafct irli sart scrt saft saftwgc scat".split(" "));function E(){return typeof csi_o_qull!=="undefined"&&csi_o_qull}function F(){return typeof csi_o_dle!=="undefined"&&csi_o_dle&&E()}
var oa=function(a,b=!1){const c=a.F;a.j=!0;a.D.forEach(d=>{d()});a.D=[];b&&(a.ba=!0,F()&&(a.F=c))},pa=function(a){var b=a.i.cbvi;if(Array.isArray(b))for(const c of b)c();a.i.cbvi=la},G=function(a,b,c){if(F()){if(!a.j||a.ba)if(a.C[b]=Math.max(a.C[b]||0,c),a.l)return;a.K[b]=Math.max(a.K[b]||0,c)}else a.C[b]=Math.max(a.C[b]||0,c)},ra=function(a,b="all",c=a.C,d=!1){const e=b==="all";let g={};if(e)pa(a),g=c;else for(var f of Object.keys(c))na.has(f.split("-")[0])||(g[f]=c[f]);(c=google.stvsc?.ns||window.performance?.timing?.navigationStart)&&
(g.ns=c);a:{f=a.url;var k;if(k=window._csc!=="agsa"||!window._cshid)k=window.google?.rdn?window.performance?.getEntriesByType?.("navigation")[0]:void 0,k?(k=k.type,k=!k||k==="navigate"):k=!window.performance?.navigation?.type;if(k&&(f=(f=f.match(RegExp("[?&]qsubts=(\\d+)")))&&Number(f[1]),f>0))break a;f=void 0}f&&(g.gsans=f);(f=google.timers.load?.t.prs)&&(g.prs=f);f=a.i.t.getAttribute("eid");k=[];for(const h of Object.keys(g))k.push(`${h}.${g[h]-a.ra}`);b=`/gen_204?s=async&astyp=${a.i.astyp}&t=${d?
`${b}2`:b}&atyp=csi&ei=${f}&rt=${k.join(",")}`;e&&(a.S&&(b+=`&lvhr=${a.S}`),b+=`&${"imn"}=${a.U}`,b+=`&${"ima"}=${a.H}`);b+=`&${"folid"}=${a.i.id}`;a.sa?navigator.sendBeacon(b,""):google.log("","",b);c&&a.ca&&qa(a,c,e)},qa=function(a,b,c){const d=(e,g)=>{(g=a.C[g])&&performance.mark(e,{startTime:g-b})};c?d("SearchAIOViewportEndTime","aaft"):(d("SearchAIOAsyncStart","ipf"),d("SearchAIOInitialContentTime","sirt-mfl"))},sa=function(a,b,c,d=Date.now(),e=!1){if(E()&&!a.j){a.D.push(()=>{sa(a,b,c,d,e)});
if(!F())return;a.l=!0}const g=google.cv(b,!1,a.i.t),f=!!(g&1);f&&(c&&(a.S=c),G(a,"aaft",d),G(a,"aafct",d));ta(a,b,d,f,e);g&4&&(a.pa=!0,a.H===a.L&&pa(a));a.l=!1},ta=function(a,b,c,d,e=!1){if(E()&&!a.j){a.D.push(()=>{ta(a,b,c,d,e)});if(!F())return;a.l=!0}for(const g of ua(a,b))for(const f of H(g)){if(!f)continue;const k=`${"saft"}-${f}`,h=`${"saftwgc"}-${f}`,l=`${"sart"}-${f}`;if(g.contains(b)||!a.j&&b.contains(g))d&&(G(a,k,c),e||G(a,h,c)),G(a,l,c)}a.l=!1},ua=function(a,b){if(typeof csi_o_qso!=="undefined"&&
csi_o_qso){b=b.querySelectorAll("[data-subtree]");for(const c of b){b=H(c);for(const d of b)a.V.includes(c)||a.V.push(c)}return a.V}return a.i.t.querySelectorAll("[data-subtree]")},H=function(a){return(a=a.dataset.subtree)?a.split(","):[]},va=function(a,b,c,d=!1){b.getAttribute("data-deferred")==="1"?google.rll(b,!0,()=>{va(a,b,c,d)}):d||(b.getAttribute("data-deferred")==="2"&&b.setAttribute("data-deferred","3"),d=!0,wa(a,b,c))},xa=function(a,b){var c=Date.now();const d=b.getElementsByTagName("img");
for(let g=0;g<d.length;g++){const f=d[g];if(f.dataset.imglogged)continue;f.dataset.imglogged="true";++a.U;var e=f.complete&&!f.getAttribute("data-deferred");const k=f.hasAttribute("data-noaft");if(e||k){++a.F;continue}e=google.cv(f,!1,a.i.t);f.setAttribute("data-aatf",String(e));const h=!!(e&1);if(h)++a.H,Array.isArray(a.i.cbvi)||(a.i.cbvi=[]);else if(f.loading==="lazy"){++a.F;continue}f.getAttribute("data-imgprocessed")?++a.F:(google.rll(f,!0,()=>{va(a,f,h)}),a.C.irfi||G(a,"irfi",c),G(a,"irli",c))}sa(a,
b,void 0,c)},wa=function(a,b,c,d=Date.now()){a.C.irfie||G(a,"irfie",d);if(E()&&!c&&!a.j){a.D.push(()=>{wa(a,b,c,d)});if(!F())return;a.l=!0}++a.F;c&&(++a.L,G(a,"aaft",d),G(a,"aafit",d));ta(a,b,d,c);a.pa&&a.H===a.L&&pa(a);!a.da||a.j&&a.D.length!==0||(ya(a),a.T());a.l=!1},ya=function(a){F()||a.da&&a.L===a.H&&oa(a)},za=function(a,b,c){if(E()&&a.aa&&!a.j){a.D.push(()=>{za(a,b,c)});if(!F())return;a.l=!0}for(const d of ua(a,b))for(const e of H(d)){if(!e)continue;const g=`${"sirt"}-${e}`,f=`${"sart"}-${e}`,
k=`${"scrt"}-${e}`;let h=!1;if(b===d||!a.j&&b.contains(d)){if(!a.C[g]||F()&&!a.K[g])h=!0,G(a,g,c);G(a,f,c);G(a,k,c)}else d.contains(b)&&(G(a,f,c),G(a,k,c));(e==="mfl"||e==="aimfl"&&a.i.astyp.includes("aim"))&&h&&(a.aa=!0,ra(a,g))}a.l=!1},Aa=class{constructor(a,b=Date.now(),c=typeof usb!=="undefined"&&usb){this.i=a;this.ra=b;this.sa=c;this.C={};this.K={};this.l=!1;this.V=[];this.D=[];this.L=this.H=this.F=this.U=this.S=0;this.ba=this.j=this.aa=this.ca=!1;this.url=window.location.search;this.i.t&&this.i.id&&
google.cv(this.i.t,!1)&1&&(ma(this.i.id),this.ca=!0);this.i.ullcb=()=>{oa(this,!0)}}T(){this.U!==this.F||this.rb||(G(this,"art",Date.now()),ra(this),F()&&(oa(this),ra(this,"all",this.K,!0)))}qa(a){sa(this,a,void 0,void 0,!0)}ea(){this.da=!0;ya(this)}};window.google=window.google||{};const Ba=window.performance&&window.performance.timing&&"navigationStart"in window.performance.timing,Ca=google.stvsc&&google.stvsc.ns;var Da=Ba?Ca||window.performance.timing.navigationStart:void 0;function Ea(){return window.performance.now()-(google.stvsc&&google.stvsc.pno||0)}const Fa=google.stvsc&&google.stvsc.rs;var J=Ba?Fa||window.performance.timing.responseStart:void 0;function Ga(a,b){let c,d=Date.now();const e=window.performance;e&&(e.now&&(c=Math.floor(Ea())),Da&&J&&J>Da&&J<=d&&(d=J,c=J-Da));a(d);b(c)};var Ha=function(a){let b="";a&&(b=typeof a==="string"?a:google.getEI(a));return b&&b!==""?b:""},Ia=class{constructor(){this.i=[]}};var Ja=new Set("acb aco aec aegc aep aipt arqrcd arv asrc atp authuser boostid boostvid bshope cds cf channel cs csui data_push_epoch dcr deb debtime dthp e edbg editnurl editourl editq editrt elrc esrch eval_id exp exp_uid expflags expid explain exprollouts fakeads fesp force_experience frendfilter frozen_clock garc gl gsas gsc hl host hotel_dates hotel_ds hotswaps hts igu imgn isc jsmode lns_img lns_int lns_mode lns_surface lns_tqs lr lsf lsft lsisr ltkn mactx masfc mat mcp mergelabel mgtyp mlp mrc nef nem nep ogdeb opti optq opts optt pcs peek_pws pga piis plugin prfq pvf pws rciv rlst rlz safe sca_esv sessionid shopmd shoprs shopvl shortlistmids sidesearch skew_host skew_hotswaps skip sni snr source source_ip ssl_dbg st stick stkp tbcp tbs tcfs tsdo udf udm uideb ulrc useragent uuld uule v vit vsint vsrid vuid".split(" ")),
La=new Set(["ampcct","client","dcr","hs","v"]),Ma=new Set([...(new Set("data_push_epoch deb duul e espv esrch exp exp_uid expflags expid expid_c exprollouts fesp frozen_clock host hotswaps ion ix jsmode mods nossl nuul ogdeb sessionid uuld".split(" "))),...La]);function Na(a){Ga(b=>{google.startTick(a,b)},b=>{google.timers[a].wsrt=b;performance.mark&&performance.mark("SearchAFTStart",{startTime:b})})};var K;(function(a){if(a.__gwbp===void 0){let b;const c=new Promise(d=>{b=d});c.__r=b;a.__gwbp=c}})(document);K=document.__gwbp;let Oa;K.then(a=>{Oa=a});var Pa=Event;var M=class extends Pa{constructor(a){super(a,{bubbles:!0});Oa.addEvent(a)}};var Qa=class extends M{constructor(){super("aimCanvasContainerResize")}},Ra=class extends M{constructor(){super("aimCanvasBeforeFirstContentPaint")}},Sa=class extends M{constructor(){super("aimCanvasRenderFinished")}},Ta=class extends M{constructor(){super("aimCanvasRenderStarted")}},Ua=class extends M{constructor(a,b){super("aimCanvasTitleAvailable");this.title=a;this.J=b}};function N(a){document.querySelector('[data-xid="Lr04i"]')?.dispatchEvent(a)}function Va(a,b){b.map(c=>[...document.querySelectorAll(c)]).flat().forEach(c=>void c.dispatchEvent(a))}async function Wa(){await K;N(new Xa);Va(new Xa,['[data-xid="eP7C1d"]'])}async function Ya(){await K;N(new Za);Va(new Za,['[data-tpcrb-host="true"]','[data-xid="x92FHb"]','[data-xid="Gd7Hsc"]','[data-xid="UN5Mf"]','[data-xid="t4J6P"]'])}async function $a(){await K;N(new ab)}async function bb(){await K;N(new cb)}
async function db(){await K;N(new eb)}async function fb(){await K;N(new gb);var a=new gb;document.querySelector('[data-scope-id="qZRtEe"]')?.dispatchEvent(a)}async function hb(){await K;N(new ib)}
var Xa=class extends M{constructor(){super("aimRenderStarted")}},Za=class extends M{constructor(){super("aimRenderComplete")}},ab=class extends M{constructor(){super("aimBodyComplete")}},cb=class extends M{constructor(){super("aimModelResponseStarted")}},eb=class extends M{constructor(){super("aimMstkAvailable")}},gb=class extends M{constructor(){super("aimThreadRhsResponseStarted")}},ib=class extends M{constructor(){super("aimThreadRhsResize")}};function O(a){const b=document.querySelectorAll('[data-scope-id="qZRtEe"] [data-canvas-lifecycle-subscriber]');for(const c of b)c.dispatchEvent(a)};function jb(a=document){return a.querySelector(".Lucn7c")}function kb(a){const b=a.getAttribute("data-canvas-title")||"";a=a.getAttribute("data-canvas-subtitle")||"";return{title:b,J:a}};function lb(a){let b=!1;return{onChunkWritten(){!b&&a.querySelectorAll("[data-mstk]").length>1&&(db(),b=!0)}}};const mb=new Set;function nb(){const a=document.querySelectorAll(".csi-ghost");if(a.length===0)return!1;for(let b=0;b<a.length;b++){let c=a[b];if(!(c.getBoundingClientRect().left>=window.innerWidth||c.getBoundingClientRect().top>=window.innerHeight)){for(;c;){const d=getComputedStyle(c);if(d.display==="none"||d.visibility==="hidden")break;c=c.parentElement}if(c===null)return!0}}return!1}
function ob(){const a=new URLSearchParams(window.location.href);return a.get("hotswaps")?.includes("search_next_mars")||a.get("hotswaps")?.includes("search_next_mars_lro")}function pb(a){const b={atyp:"csi",ei:a.N};a.G&&(b.astyp=a.G);a.oa&&a.oa.forEach((c,d)=>{b[d]=c.toString()});google.x(null,()=>{google.minCsiReport(b,ob()?"l"+a.appName:a.appName,google.timers[a.B])})}function qb(a){return Array.from(a.querySelectorAll('span[data-key^="NODE_END_MARKER_"]'))}
function rb(a){const b=[];a.forEach(c=>{(c=c.getAttribute("data-key"))&&!mb.has(c)&&(b.push(c),mb.add(c))});return b}function sb(a,b,c){b.forEach(d=>{let e=a.getElementById(d);e||=document.getElementById(d);e&&(e.textContent=`Client Latency: ${c.toFixed(2)} ms`,e.style.display="block")})};function tb(a,b,c,d){b="aim_"+b;ob()&&(b="l"+b);d&&(b+="_restore");return{astyp:b,t:a,m:5,id:c}};function ub(a){let b=!1;return{onChunkWritten(){b||a.querySelector("[data-aimmrs]")==null||(bb(),b=!0)}}};let Q=0,vb=new WeakSet;function wb(a){const b=[];for(a=a.parentElement;a;)a.matches("[data-animation-nesting],li,mark,code")&&b.push(a),a=a.parentElement;return b}function xb(a,b){if(yb(a)===NodeFilter.FILTER_ACCEPT){var c=performance.now(),d=[];Q=zb(a,Math.max(0,Q-c),b,d)+c;return{la:Q,yb:d}}}
function zb(a,b,c,d){Ab(a).forEach(e=>{wb(e).forEach(t=>{Bb(t,!1,b,0,c,d)});if(R(e))b=Bb(e,!1,b,4,c,d);else{for(var g=e.parentNode,f=e.nextSibling,k=document.createDocumentFragment(),h=e.textContent?.match(/\s*\S+\s*/g)??[],l=0;l<h.length;){const t=h.slice(l,l+12),r=document.createElement("span"),q=t.join("");l===0?(r.appendChild(e),e.data=q):r.textContent=q;k.appendChild(r);b=Bb(r,!0,b,t.length*4,c,d);l+=12}g.insertBefore(k,f)}});return b}
function Ab(a){const b=[];yb(a)===NodeFilter.FILTER_ACCEPT&&Cb(a)&&b.push(a);if(R(a)){const c=document.createTreeWalker(a,NodeFilter.SHOW_ELEMENT|NodeFilter.SHOW_TEXT,yb);for(;a=c.nextNode();)Cb(a)&&b.push(a)}return b}function Cb(a){const b=a?.nodeType===Node.TEXT_NODE&&a.data.trim()!==""&&!vb.has(a);a=R(a)&&a.matches("[data-animation-atomic],hr,svg,img");return b||a}
function yb(a){return a.isConnected&&a.parentElement?(R(a)?a:a.parentElement).closest("[data-animation-skip],style,script,details,\n    math,[data-sae]:not([data-animation-nesting],li,mark,code),\n    [data-xpm-copy-root]")||a.parentElement.closest("[data-animation-atomic],hr,svg,img")?NodeFilter.FILTER_REJECT:NodeFilter.FILTER_ACCEPT:NodeFilter.FILTER_REJECT}function Bb(a,b,c,d,e,g){if(a.matches("[data-sae]"))return c;a=Db(a,b,c,e);g.push(a);return c+d}
async function Db(a,b,c,d){a.style.opacity="0";a.style.pointerEvents="none";a.dataset.sae="";await a.animate([{opacity:0},{opacity:1}],{delay:c,duration:100,easing:"linear"}).finished;a.style.opacity="";a.style.pointerEvents="";b&&(b=a.firstChild,c=a.previousSibling,c?.nodeType===Node.TEXT_NODE?(vb.add(c),c.data+=b.data,b=a.parentElement,a.remove(),a=b):(vb.add(b),a.replaceWith(b),a=b.parentElement),a&&(za(d,a,Date.now()),xa(d,a)))}
function Eb(){const a=Q-performance.now();a>0?setTimeout(Ya,a):Ya()}function R(a){return a?.nodeType===Node.ELEMENT_NODE};function S(a){return document.body.querySelector(`[jsname="${a}"]`)};function Fb(){Gb();document.body.querySelector(".gsZh2b")?.classList.remove("gsZh2b","DzT1rd")}function Gb(){document.dispatchEvent(new CustomEvent("WuRwWc"));const a=S("hc2akf")?.firstElementChild;a?.classList.add("CCW8cb");a?.classList.add("qaHYKd")}function T(a,b){a&&(a.style.display=b?"":"none")};/*

 Copyright Google LLC
 SPDX-License-Identifier: Apache-2.0
*/
let Hb=globalThis.trustedTypes,Ib;function Jb(){let a=null;if(!Hb)return a;try{const b=c=>c;a=Hb.createPolicy("goog#html",{createHTML:b,createScript:b,createScriptURL:b})}catch(b){}return a}function Kb(){Ib===void 0&&(Ib=Jb());return Ib};var Lb=class{constructor(a){this.i=a}toString(){return this.i+""}};var Mb=class{constructor(a){this.i=a}toString(){return this.i+""}};function Nb(a){if(a instanceof Mb)return a.i;throw Error("h");};function Ob(a=document){a=a.querySelector?.("script[nonce]");return a==null?"":a.nonce||a.getAttribute("nonce")||""};var Pb=class{constructor(a){this.i=a}toString(){return this.i+""}};function Qb(a){const b=Ob(a.ownerDocument);b&&a.setAttribute("nonce",b)};function Rb(a){const b=Kb();a=b?b.createHTML(a):a;return new Mb(a)};var Sb=!!(A[0]&2),Tb=!!(A[0]&8),Ub=!!(A[0]&32),Vb=!!(A[0]&64);function Wb(a=!1){const b=Xb();if(b){var c=new URL(window.location.href);c.searchParams.set("mstk",b);c.searchParams.set("csuir","1");if(c.searchParams.has("aioh")){c.searchParams.delete("aioh");const d=document.querySelector('[jsname="aadvhe"]');d&&d.setAttribute("data-aioh","")}c.searchParams.has("smstk")&&c.searchParams.delete("smstk");c.searchParams.has("smstidx")&&c.searchParams.delete("smstidx");window.history.replaceState(window.history.state,"",c.toString());if(c=c.searchParams.get("q"))window.sessionStorage.setItem("aim-trsm-q",
c),window.sessionStorage.setItem("aim-trsm-mstk",b);a&&window.parent.postMessage(JSON.stringify({type:"mstk",mstk:b}),"*")}}function Xb(){({Kb:a=!1}={});if(a){var a=document.querySelector("[data-xid='aim-mars-turn-root']");if(!a||a.dataset.asrc!=="true")return null}a=document.querySelectorAll("[data-mstk]");return a.length>0?a[a.length-1].dataset.mstk:(new URLSearchParams(location.search)).get("mstk")}function Yb(){return(new URLSearchParams(window.location.search)).get("mqvsrid")};CSS.supports("animation-timing-function","linear(0, 1)");async function Zb(a,b=!1){var c=S("oEQ3x")!==null;const d=S("oEQ3x")??S("P9ya7d");var e;(e=S("RH7zg"))||(e=document.body.querySelector('[data-xid="VpUvz"]'));var g=e;e=S("Gd8Kre");if(g&&d&&e){if(a){var f=document.getElementById("cnt");f&&(f.style.overflowX="unset")}var {left:k,width:h}=g.getBoundingClientRect();e.style.left=`${k}px`;(new URLSearchParams(window.location.search)).get("mq")===null&&Yb()===null||!b?e.style.width=`${h}px`:e.style.width=`${window.innerWidth}px`;e.style.scale="";e.style.translate=
"";c&&(e.style.top="100%");b=S("NlVIob")?.getBoundingClientRect().width||0;$b(!1);T(e,!0);d.contains(e)||d.appendChild(e);f=e.getBoundingClientRect();g=-f.left+b;f=(window.innerWidth-b)/f.width;g=e.animate(a?{scale:["0.8 1","1 1"]}:{scale:["1 1",`${f} 1`],translate:["0px 0px",`${g}px 0px`]},{duration:500,fill:"forwards",easing:"cubic-bezier(0.4, 0, 0.2, 1)"});ac(e,g);bc(e,150,"cubic-bezier(0.3, 0, 0.8, 0.15)");if(e=S("J3KLwe"))e.style.setProperty("--lhs-margin",`${b}px`),e.style.top=c?a?"0px":"auto":
a?"0px":`${d.offsetHeight-20}px`,T(e,!0),c=document.querySelector("#main"),!a&&c&&c.insertBefore(e,c.firstChild),bc(e,350,"linear")}}function bc(a,b,c){b=a.animate({opacity:["0","1"]},{duration:b,easing:c,fill:"forwards"});ac(a,b)}async function cc(a){if(a){var b=a.animate({opacity:["1","0"]},{duration:150,fill:"forwards"});await ac(a,b)}}async function ac(a,b){await b.finished;a.isConnected&&getComputedStyle(a).display!=="none"&&(b.commitStyles(),b.cancel())}
function $b(a){[...document.body.querySelectorAll('[jsname="RH7zg"]')].at(-1)?.classList.toggle("ZKmnA",!a)};function ec(a,b){return a.classList.contains(b)?a:a.querySelector(`.${b}`)}let fc=null;function hc(){return fc?fc:fc=S("hc2akf")}function ic(){const a=hc(),b=a?.firstElementChild;return a&&a.style.display!=="none"&&!b?.classList.contains("CCW8cb")};var jc={};const kc=new Set("area base basefont br col command embed hr img input isindex keygen link meta param plaintext source track wbr".split(" "));function lc(a){return a.nodeType===Node.ELEMENT_NODE&&a.hasAttribute("jscontroller")&&a.hasAttribute("jsuid")};function U(a,b){if(!a||!a.parentElement||a===b||a.hasAttribute("data-container-id"))return!1;if(a.hasAttribute("data-complete")||a.nextSibling)return a.setAttribute("data-complete","true"),!0;var c;if(c=lc(a)){if(!lc(a))throw Error("i");kc.has(a.localName)?c=!0:(c=a.lastChild,c=!!c&&c.nodeType===Node.COMMENT_NODE&&c.data.startsWith("Sv6Kpe"))}return c||U(a.parentElement,b)?(a.setAttribute("data-complete","true"),!0):!1};const mc=Symbol("hybrid_contxt_owner");function nc(a,b,c=()=>{}){a.forEach(d=>{a:{const k=d.getAttribute("data-target-container-id");d.removeAttribute("data-target-container-id");if(k){var e=b;var g=d.getAttribute("data-target-scope-id");d.removeAttribute("data-target-scope-id");g&&(e=document.querySelectorAll(`[data-scope-id="${g}"]`),e=e[e.length-1]);if(e){var f=`[${"data-container-id"}="${k}"]`;e=e.matches(f)?e:e.querySelector(f)}if(!e){if(k===d.getAttribute("data-container-id")){g=d;break a}throw Error("j`"+k+"`"+(g?` within scope ${g}.`:
"."));}e[mc]=null;g=e}else g=b[mc]=null}oc(d,g);c(g,d)})}function oc(a,b){if(a!==b){const c=a.getAttribute("data-sn-op")||"0";a.removeAttribute("data-sn-op");switch(c){case "1":pc(b,a);break;case "2":b.append(...a.childNodes);break;case "0":b.appendChild(a)}}}
function qc(a,b,c,d){c=c||b;d?.parentNode!==b&&(d=null);const e=a.matches("[data-target-container-id]")?[a]:a.querySelectorAll("[data-target-container-id]");a.tagName==="STYLE"?b.append(a):(e.forEach(g=>{g.getAttribute("data-target-container-id")!==g.getAttribute("data-container-id")&&g.remove()}),d?b.insertBefore(a,d):b.appendChild(a),nc(e,c))}function pc(a,b){a.replaceChildren(...a.querySelectorAll("style"),b)}
function rc(a){a.forEach(b=>{if(U(b,b.ownerDocument.documentElement)){var c=b.dataset.targetIframeUuid,d=b.dataset.command,e=b.dataset.content;if(!d)throw Error("k`"+c);b.remove();({promise:b}=window.iframes.get(c));b.then(g=>{g.contentWindow?.postMessage({__sn__:1,method:"stream",args:{command:d,content:e}},"*")})}})};let V=null,sc=0;function tc(a){V&&(a.classList.remove("DzT1rd"),clearTimeout(V),V=null,a.classList.remove("gsZh2b"))}function uc(a,b){const c=performance.now();b=Math.max(b??0,c+350);b<sc+1||(sc=b,a.classList.remove("DzT1rd"),V&&(clearTimeout(V),V=null),V=setTimeout(()=>{a.classList.add("DzT1rd")},sc-c))}
function vc(a,b,c={}){const d=wc(a,b,c),e=c.W;return{onRenderStarted:()=>{d.observe(a,{subtree:!0,childList:!0,characterData:!0})},onRenderFinished:()=>{e&&tc(a);d.disconnect();google.domchange?.()},onStreamCanceled:()=>{e&&tc(a);d.disconnect()}}}
function wc(a,b,{animate:c=!1,Y:d=!1,W:e=!1,Z:g=!1,ha:f=!1,ma:k=!1,Gb:h=()=>{}}){return new MutationObserver((l,t)=>{var r=new Set;for(const m of l)if(m.type==="childList")a:for(const n of m.addedNodes){if(k){b:if(r.has(n))l=!0;else{for(const x of r)if(x.contains(n)){l=!0;break b}l=!1}if(l)continue a;r.add(n)}if(R(n)&&n.matches("[data-sae]")||n.parentElement?.closest("[data-sae]"))continue a;if(n instanceof HTMLElement){var q=n;l=a;var v=g;if(q=q.getAttribute("eid")??q.querySelector("[eid]")?.getAttribute("eid"))l.setAttribute("eid",
q),v&&google.fireListeners?.(window,"attn_reset",!1,{ei:q})}if(n instanceof HTMLElement&&!n.dataset.processed){l=S("elJbkf");ic()&&!l&&(l=ec(n,"ilZyRc")||ec(n,"XIurze"),v=S("hc2akf"),l&&v&&l.after(v));v=n;l=f;if(q=ic())q=(v.matches('[data-subtree*="aimfl"],[data-key="aimhl"]')?v:v.querySelector('[data-subtree*="aimfl"],[data-key="aimhl"]'))!==null;q?hc()?.closest('[jsname="elJbkf"]')&&document.body.querySelectorAll('[jsname="RH7zg"]').length===1?l=!1:(l||($b(!0),cc(S("Gd8Kre")),cc(S("J3KLwe")),Gb()),
l=!0):l=!1;e&&l&&a.classList.add("gsZh2b");n.dataset.processed="true";nc(Array.from(a.querySelectorAll("[data-target-container-id]")??[]),a,h);l=n;v=b;q=l;var u=v;za(u,q,Date.now());xa(u,q);q=Date.now();for(const x of ua(v,l))for(const D of H(x)){if(!D)continue;u=`${"scat"}-${D}`;(x.contains(l)||l.contains(x))&&G(v,u,q)}}l=Array.from(a.querySelectorAll("[data-target-iframe-uuid]"));rc(l);n instanceof HTMLElement&&n.dataset.zzy&&tc(a);if(n instanceof HTMLElement&&(n.dataset.zzz||n.dataset.debugFragment||
n.getAttribute("jsname")==="u6mzse")){r=Q-performance.now();r>0?setTimeout($a,r):$a();e&&tc(a);t.disconnect();return}d||(l=c?xb(n,b):void 0,e&&uc(a,l?.la))}else m.type!=="characterData"||d||(l=c?xb(m.target,b):void 0,e&&uc(a,l?.la))})};function xc(){return{onRenderStarted:()=>{Wa()}}};try{const a=window?.top??z;a.U3bHHf??(a.U3bHHf=0);a.U3bHHf++}catch(a){z.U3bHHf??(z.U3bHHf=0),z.U3bHHf++};function yc(a){var b=zc;for(const c in b)a.call(void 0,b[c],c,b)};let Ac=!1;function Bc(a,b){for(;a&&a!==void 0;){if(b(a))return a;a:{let c=0;for(;a=a?a.__owner?a.__owner:a.parentNode&&a.parentNode.nodeType===11?a.parentNode.host:a.parentElement||null:null;){if(a&&a===void 0&&c>0){Ac||=!0;break}if(a.hasAttribute("jsslot"))c+=1;else if(a.hasAttribute("jsshadow")&&c>0){--c;continue}if(c<=0)break a}a=null}}return null};var Cc={toString:function(a){let b=[],c=0;a-=-2147483648;b[c++]="abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ".charAt(a%52);for(a=Math.floor(a/52);a>0;)b[c++]="abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789".charAt(a%62),a=Math.floor(a/62);return b.join("")}};var Dc=function(a){const b=[];for(let c=0;c<a.length;c++)b.push(a.charCodeAt(c));return b},Ec=function(a,b){return a[b+0]+(a[b+1]<<8)+(a[b+2]<<16)+(a[b+3]<<24)};const Fc=`[jsname="guest_container_"],[jsname="${(function(a){a=Dc(a);let b=2654435769,c=2654435769,d=314159265;const e=a.length;let g=e,f=0;const k=function(){b-=c;b-=d;b^=d>>>13;c-=d;c-=b;c^=b<<8;d-=b;d-=c;d^=c>>>13;b-=c;b-=d;b^=d>>>12;c-=d;c-=b;c^=b<<16;d-=b;d-=c;d^=c>>>5;b-=c;b-=d;b^=d>>>3;c-=d;c-=b;c^=b<<10;d-=b;d-=c;d^=c>>>15};for(;g>=12;g-=12,f+=12)b+=Ec(a,f),c+=Ec(a,f+4),d+=Ec(a,f+8),k();d+=e;switch(g){case 11:d+=a[f+10]<<24;case 10:d+=a[f+9]<<16;case 9:d+=a[f+8]<<8;case 8:c+=a[f+7]<<24;case 7:c+=
a[f+6]<<16;case 6:c+=a[f+5]<<8;case 5:c+=a[f+4];case 4:b+=a[f+3]<<24;case 3:b+=a[f+2]<<16;case 2:b+=a[f+1]<<8;case 1:b+=a[f+0]}k();return Cc.toString(d)})("guest_container")}"]`,Gc=z.parentWindow?z.parentWindow:z;let Hc;Gc.__gr?Hc=Gc.__gr:(Hc={g:new Map,h:{}},Gc.__gr=Hc);const {g:W,h:Ic}=Hc;function Jc(a){return(a=Bc(a,b=>W.has(b)))?W.get(a):null}
function Kc(a,b){var c=document.querySelectorAll(Fc);for(const d of c)W.has(d)||W.set(d,{r:d,eq:[]});if(c=Jc(a))c.m?.dirtySubtree(a,b);else{Ic.ds?.(a,b);for(const d of W.values())a.contains(d.r)&&d.m?.dirtySubtree?.(a,b)}};function Lc(a){return{onChunkWritten:()=>{const b=document.querySelector('[data-container-id="L9N7ob"]');Kc(a,U);b&&Kc(b,U)},onRenderFinished:()=>{const b=document.querySelector('[data-container-id="L9N7ob"]');Kc(a,()=>!0);b&&Kc(b,()=>!0)}}};function Mc(a){let b=!1;const c=d=>{b||d.querySelector('[data-target-container-id="L9N7ob"]')==null||(fb(),b=!0)};return{onChunkWritten(d){c(d);c(a)}}};function Nc({A:a,Y:b=!1,Z:c=!1,v:d=new Aa(tb(a,"folif",a.id,!1)),Eb:e=!1}){return[vc(a,d,{animate:Tb,Y:b,W:!0,Z:c,ha:e,ma:Vb}),Lc(a),ub(a),xc(),...(Sb&&!e?[Mc(a)]:[])]};function Oc(a,b=!1){return{onRenderFinished:()=>{Pc(a)},onChunkWritten:()=>{b?Qc(a):Rc(a)}}}function Rc(a){a.querySelectorAll("script[data-script-exec]").forEach(b=>{U(b,a)&&Sc(b)})}function Qc(a){a.querySelectorAll("script").forEach(b=>{b.dataset.scriptExecPoststream===void 0&&U(b,a)&&Sc(b)})}function Pc(a){a.querySelectorAll("script").forEach(b=>{Sc(b)})}
function Sc(a){var b=document.body;a.parentElement.removeChild(a);const c=document.createElement("script");for(var d in a.dataset)d!=="scriptExecPoststream"&&d!=="script-exec"&&(c.dataset[d]=a.dataset[d]);d=a.text;const e=Kb();d=e?e.createScript(d):d;d=new Pb(d);if(d instanceof Pb)d=d.i;else throw Error("h");c.textContent=d;Qb(c);c.type=a.type;c.nonce=a.nonce;c.async=a.async;c.crossOrigin=a.crossOrigin;c.onerror=a.onerror;if(a.src){a=a.src;a=(d=Kb())?d.createScriptURL(a):a;a=new Lb(a);if(a instanceof
Lb)a=a.i;else throw Error("h");c.src=a;Qb(c)}b.appendChild(c)};function Tc(a){function b(d){const e=Array.from(d.head?.children??[]);d=Array.from(d.body?.children??[]);const g=e.concat(d);g.forEach((f,k)=>{if(!f.hasAttribute("data-nosplice"))if(f.tagName==="SCRIPT"){if(c||k<g.length-1){if(f instanceof HTMLScriptElement)if(f.type!=="text/javascript"&&f.type!=="")f.hasAttribute("data-speculation-rules")&&(f.type="speculationrules"),k=!1;else{k=document.createElement("script");k.textContent=f.textContent;k.nonce=f.nonce;var h=k.setAttribute;var l=(l=document.querySelector("script[nonce]"))?
l.nonce||l.getAttribute("nonce")||"":"";h.call(k,"nonce",l);f.remove();document.body.appendChild(k);k=!0}else k=!1;k||a(f)}}else a(f)})}let c=!1;return{onChunkWritten:b,onRenderFinished(d){c=!0;b(d)}}};function Uc(a){const b=Tc(a),c=[];return{onDocumentOpen(d){c.push(d)},onChunkWritten(d){for(const e of d.querySelectorAll("script"))e.setAttribute("data-complete","true");b.onRenderFinished?.(d)},onRenderFinished(){for(const d of c)b.onRenderFinished?.(d)}}};var Vc=!!(window.ReadableStream?.prototype&&"pipeThrough"in window.ReadableStream?.prototype&&window.TextDecoderStream),Wc=async function(a,b,c){if(b.headers.get("Content-Type")?.split(";",2)?.[0]?.toLowerCase()!=="text/html")throw Error("l`"+b.url);if(b.url&&(new URL(b.url)).origin!==location.origin)throw Error("m`"+b.url);if(Vc)for(a.j=b.body.pipeThrough(new TextDecoderStream).getReader();;){const {value:d,done:e}=await a.j.read();if(e||a.i)break;b=Rb(d);c(b)}else a=await b.text(),a=Rb(a),c(a)},
Xc=class{constructor(a=[]){this.plugins=a;this.finished=this.i=!1;this.plugins=a}async render(a){const b=document.implementation.createHTMLDocument("");for(const c of this.plugins)if(c.onRenderStarted)c.onRenderStarted(b);b.open();for(const c of this.plugins)c.onDocumentOpen?.(b);await Wc(this,a,c=>{if(!this.i){for(const d of this.plugins)try{d.onFirstByte?.(b),d.onFirstByte=void 0}catch(e){}b.write(Nb(c));for(const d of this.plugins)try{d.onChunkWritten?.(b)}catch(e){}}});b.close();if(!this.i){this.finished=
!0;for(const c of this.plugins)try{c.onRenderFinished?.(b)}catch(d){}}}cancel(){if(!this.finished){this.i=!0;this.j?.cancel().catch(()=>{});for(let a=0;a<this.plugins.length;a++)this.plugins[a].onStreamCanceled?.()}}};var Yc=function(a,b){var c;if(!(c=a.j.get(b))){c=document.implementation.createHTMLDocument("");a.j.set(b,c);c.open();for(const d of a.plugins)d.onDocumentOpen?.(c)}return c},Zc=async function(a,b,c){function d(){const h=g.indexOf(";",k);if(h===-1)return null;const l=parseInt(g.substring(k,h),16);if(isNaN(l))throw Error("n`"+g.length+"`"+f);if(g.length<h+l)return null;k=h+l+1;return g.substring(h+1,k)}function e(h){g+=h;for(k=f;;){h=d();if(h===null)break;const l=d();if(l===null)break;f=k;c(h||"message",
l)}}if(!a.i){var g="",f=0,k=0;if(Vc)for(b=b.body.pipeThrough(new TextDecoderStream).getReader();;){const {value:h,done:l}=await b.read();if(l||a.i)break;e(h)}else b=await b.text(),e(b);if(!a.i&&(a.close(),g.length>f))throw Error("o`"+g.length+"`"+f);}},$c=class{constructor(a){this.plugins=a;this.j=new Map;this.finished=this.i=!1}async render(a){if(!a.ok)throw Error("p`"+a.status+"`"+a.statusText);if(a.headers.get("Content-Type")?.split(";",2)?.[0]?.toLowerCase()!=="text/plain")throw Error("q`"+a.url);
if(a.url&&(new URL(a.url)).origin!==location.origin)throw Error("m`"+a.url);for(const c of this.plugins)c.onRenderStarted?.();let b=Yc(this,"default");await Zc(this,a,(c,d)=>{if(!this.i){for(const e of this.plugins)try{e.onFirstByte?.(),e.onFirstByte=void 0}catch(g){}if(c==="write_to_target_document")b=Yc(this,d);else if(c==="message"){c=Rb(d);b.write(Nb(c));for(const e of this.plugins)try{e.onChunkWritten?.(b)}catch(g){}}}});if(!this.i){this.finished=!0;for(const c of this.plugins)try{c.onRenderFinished?.()}catch(d){}}}close(){this.j.forEach(a=>
{a.close()})}cancel(){if(!this.finished){this.i=!0;this.close();for(const a of this.plugins)a.onStreamCanceled?.()}}};function ad({A:a,zb:b,insertBefore:c}){return d=>{qc(d,a,b,c)}};const bd=RegExp("^(?:([^:/?#.]+):)?(?://(?:([^\\\\/?#]*)@)?([^\\\\/?#]*?)(?::([0-9]+))?(?=[\\\\/?#]|$))?([^?#]+)?(?:\\?([^#]*))?(?:#([\\s\\S]*))?$");function cd(a,b){if(a){a=a.split("&");for(let c=0;c<a.length;c++){const d=a[c].indexOf("=");let e,g=null;d>=0?(e=a[c].substring(0,d),g=a[c].substring(d+1)):e=a[c];b(e,g?decodeURIComponent(g.replace(/\+/g," ")):"")}}}
function dd(a,b,c){if(Array.isArray(b))for(let d=0;d<b.length;d++)dd(a,String(b[d]),c);else b!=null&&c.push(a+(b===""?"":"="+encodeURIComponent(String(b))))};var gd=function(a){const b=a.startsWith("https://uberproxy-pen-redirect.corp.google.com/uberproxy/pen?url=")?a.substr(65):a,c=new ed,d=b.match(bd)[5];yc(function(g){const f=d.match("/"+g+"=([^/]+)");f&&fd(c,g,f[1])});let e="";e=a.indexOf("_/ss/")!=-1?"_/ss/":"_/js/";c.l=a.substr(0,a.indexOf(e)+e.length);(a=b.match(bd)[6]||null)&&cd(a,(g,f)=>{c.j[g]=f});return c},fd=function(a,b,c){c?a.i[b]=c:delete a.i[b]},jd=function(a){const b=[],c=d=>{a.i[d]!==void 0&&b.push(d+"="+a.i[d])};hd(a)?(c("md"),c("k"),
c("ck"),c("am"),c("rs"),c("gssmodulesetproto"),c("slk"),c("dti")):(c("sdch"),c("k"),c("ck"),c("am"),c("amc"),c("rt"),"d"in a.i||fd(a,"d","0"),c("d"),c("exm"),c("excm"),(a.i.excm||a.i.exm)&&b.push("ed=1"),c("im"),c("dg"),c("sm"),X(a,"br")!="1"&&X(a,"br")!="0"||c("br"),c("br-d"),X(a,"rb")=="1"&&c("rb"),X(a,"zs")!=="0"&&c("zs"),id(a)!==""&&c("wt"),c("gssmodulesetproto"),c("ujg"),c("sp"),c("rs"),c("cb"),c("ee"),c("slk"),c("dti"),c("ic"),c("m"));return b.join("/")},X=function(a,b){return a.i[b]?a.i[b]:
null},hd=function(a){a=X(a,"md");return!!a&&a!=="0"},id=function(a){switch(X(a,"wt")){case "0":return"0";case "1":return"1";case "2":return"2";default:return""}};const ed=class{constructor(){this.i={};this.l="";this.j={}}toString(){var a=this.l+jd(this);var b=this.j;var c=[];for(const d in b)dd(d,b[d],c);b=c.join("&");c="";b!=""&&(c="?"+b);return a+c}};
var zc={lb:"k",Qa:"ck",jb:"m",Za:"exm",Xa:"excm",Na:"am",Oa:"amc",hb:"mm",kb:"rt",eb:"d",Ya:"ed",qb:"sv",Ra:"deob",Pa:"cb",ob:"rs",mb:"sdch",fb:"im",Sa:"dg",Va:"br",Ua:"br-d",Wa:"rb",xb:"zs",wb:"wt",ab:"ee",pb:"sm",ib:"md",bb:"gssmodulesetproto",tb:"ujg",sb:"sp",nb:"slk",Ta:"dti",gb:"ic"};var kd=class{constructor(){this.i={}}add(a){this.i[a]=!0}addAll(a){for(let b=0;b<a.length;b++)this.add(a[b])}has(a){return!!this.i[a]}getAllInstalledChunkIds(){return Object.keys(this.i)}clear(){this.i={}}};function ld(a){const {A:b,type:c,na:d=new URLSearchParams,ka:e,La:g}=a;var f=window.location,k=new Map;document.querySelector("[data-madl]")?k.set("_fmt","madl"):k.set("_fmt","adl");var h;if(h=window.baseCssUrl){h=gd(h);var l=X(h,"ck")??X(h,"k"),t=X(h,"excm"),r=Array,q=r.from,v=Set;t=[...(t?t.split(","):[])];var u,m;var n=(m=(u=google).cktocr||(u.cktocr={}))[l]||(m[l]=new kd);(u=q.call(r,new v([...t,...n.getAllInstalledChunkIds()])))&&u.length>0?(u.sort(),fd(h,"excm",u.join(","))):fd(h,"excm",null);
h=h.toString()}else h=null;h&&k.set("_snbasecss",h);u=new URLSearchParams(f.search);h=new URLSearchParams;if(c==="folwr")if((m=google.xsrf?.HqNxoe)&&k.set("_xsrf",m),m=u.get("mlrol"),m=md(m).at(-1))l=nd(),h.set("garc",l.ja),h.set("mlro",m.Ka),h.set("mlros",m.Ha),h.set("ei",m.N);else if(a.Ia){if(m=document.querySelector(`[data-sr-id="${a.Ia}"][data-sr-mlro][data-sr-mlros][data-sr-ei]`))h.set("mlro",m.dataset.srMlro),h.set("mlros",m.dataset.srMlros),h.set("ei",m.dataset.srEi),h.set("garc",m.dataset.srGarc)}else m=
nd(),h.set("garc",m.ja),h.set("mlro",m.Ea),h.set("mlros",m.Fa),h.set("ei",m.wa);else c==="folif"&&((m=google.xsrf?.h2BS7e)&&k.set("_xsrf",m),m=document.querySelectorAll("[data-ei]"),m.length>0&&(m=m[0])&&h.set("ei",m.dataset.ei));if(!e){m=u.get("q")||d.get("q");l=u.get("vsrid")||d.get("vsrid");r=u.get("lns_img")||d.get("lns_img");q=u.get("cinpts")||d.get("cinpts");a=a.Ba;v=u.get("mlrol")!=null;if(m===null&&l===null&&q===null&&r===null&&!a&&!v)throw Error("r");m&&h.set("q",m)}h.set("yv","3");if(m=
b.dataset.ved){a=new Ia;a.i.push({Ja:m,targetElement:"",ya:"insert"});m=[];for(let D=0,B;B=a.i[D++];)switch(l=B.Ja,r=B.ya,q=B.Db,n=B.Cb,v=B.Ib,t=Ha(B.targetElement),n=Ha(n),r){case "show":m.push(`${l}.${t}.s`);break;case "insert":m.push(`${l}.${t}.i${n?`.0.${q}.${n}`:""}`);break;case "dedupe-insert":m.push(`${l}.${t}.i${n?`.1.${q}.${n}`:".1"}`);break;case "hide":m.push(`${l}.${t}.h`);break;case "copy":m.push(`.${v}.c`)}(a=m.length?"1"+m.join(";"):"")&&h.set("vet",a)}g&&(a=g.dataset.ved)&&h.set("ved",
a);a=new Set([...Ja,...Ma]);e&&(a.delete("vsrid"),a.delete("lns_surface"),a.delete("imgn"),a.delete("vit"));for(var x of a)(a=u.get(x))&&h.set(x,a);(x=document.getElementById("rKxeg"))&&(x=x.getAttribute("data-stkp"))&&h.set("stkp",x);for(const [D,B]of d)h.set(D,B);if(x=h.get("skew_hotswaps"))h.delete("skew_hotswaps"),h.set("hotswaps",x);f=new URL(`/async/${c}`,f.origin);x=h.toString();h=[];for(const [D,B]of k)k=B,h.push(encodeURIComponent(String(D))+":"+encodeURIComponent(String(k)));f.search=`${x}&async=${h.join(",")}`;
return f}function nd(){const a=document.querySelector("[data-garc][data-lro-token][data-lro-signature][data-ei]");if(a)return{ja:a.dataset.garc,Ea:a.dataset.lroToken,Fa:a.dataset.lroSignature,wa:a.dataset.ei};throw Error("s");}function md(a){return a?a.split(";").map(b=>{const [c,d,e]=b.split(",");return c?{Ka:c,Ha:d,N:e}:null}).filter(b=>b!==null):[]};var Y=function(a){var b=a.i;if(b==null)b=null;else if(typeof b!=="string")throw new TypeError(`Invalid ${"string"} data <${a.j}>: ${a.i} (typeof ${typeof a.i})`);if(b===null)throw Error("t`"+a.j);return b};class od{constructor(a,b){this.j=a;this.i=b}toString(){return Y(this)}};function pd(a=window){return a.WIZ_global_data}function qd(a,b=window){return(b=pd(b))&&a in b?b[a]:null};function rd(a){return new od(a,qd(a,window))};var sd={};async function td(a,b,c){const d=new AbortController,e=d.signal;window.__adlRequests||(window.__adlRequests=[]);window.__adlRequests.push({cancel:()=>{d.abort(sd);c?.cancel()}});return await fetch(a.toString(),{signal:e,...b})};async function ud(a,b){try{const e=a.url||ld(a);var c=e.searchParams.get("async")?.includes("_fmt:madl");const g=a.A,f=a.X;var d;if(d=b){c={...b,A:g,X:f,Ma:c};const {A:v,plugins:u=[],X:m,Ma:n}=c;d=n?new $c([Oc(v,m),...u,Uc(ad(c)),jc]):new Xc([Oc(v,m),...u,Tc(ad(c)),jc])}const k=new Headers,h=rd("ejMLCd");h.i!=null&&k.set("X-Geo",Y(h));const l=rd("PYFuDc");l.i!=null&&k.set("X-Client-Data",Y(l));const t=rd("JHHKub");t.i!=null&&k.set("X-Client-Pctx",Y(t));const r=rd("AUf7qc");r.i!=null&&k.set("X-Silk-Capabilities",
Y(r));const q=await td(e,{headers:k},d);q.status>=400&&b?.Hb?.(q);await d?.render(q)}catch(e){if(e!==sd)throw b?.Bb&&e instanceof TypeError&&e.message==="Load failed"||(e instanceof Response&&e.status>=400?b?.va||vd("An error occurred while fetching the content.",a.A):b?.va||vd("Something went wrong and the content wasn't generated.",a.A)),e;}}
function vd(a,b){const c=document.createElement("div");c.textContent=a;c.style.fontSize="16px";c.style.fontStyle="italic";c.setAttribute("data-aeie","true");b.insertAdjacentElement("afterend",c)};async function wd({Ga:a,A:b,ua:c=0,ta:d=0,xa:e=!1,Fb:g=!1,Jb:f,Ab:k}){await xd(a,c,d,b,e,g,f,k)}
async function xd(a,b,c,d,e,g,f,k){var h=new URLSearchParams;h.set("q",a);h.set("dr",c===1?e&&b===1?"5":"3":g?"3":e&&b===1?"1":e&&b===2?"4":"2");k&&h.set("dbts",k);f&&h.set("ved",f);if(a=Xb())h.set("mstk",a),h.set("csui","3");(a=(new URL(window.location.href)).searchParams.get("smstk"))&&h.set("smstk",a);a=document.querySelectorAll("[data-sr-elrc]");a.length>0&&h.set("elrc",a[a.length-1].dataset.srElrc);h={A:d,type:"folif",ka:!0,na:h};d={plugins:[...Nc({A:d}),{onRenderFinished:()=>{Eb()}}]};try{await ud(h,
d)}catch(l){Fb(),T(S("Gd8Kre"),!1),T(S("J3KLwe"),!1)}Ub&&Wb()};function yd(a){return['[jsname="MUNgzf"]',".ZFcyjd",".pcxG3e"].map(b=>[...a.querySelectorAll(b)]).flat().length>0};var zd=class extends M{constructor(a){super("aimSxsRenderFinished");this.containerId=a}},Ad=class extends M{constructor(a){super("aimSxsVisibleContent");this.containerId=a}},Bd=class extends M{constructor(a){super("aimSxsError");this.containerId=a}};
function Cd(a){const b=a.closest("[data-xid=eP7C1d]");if(!b)return[];let c=!1,d=!1;return[{onChunkWritten:()=>{!d&&yd(a)&&(d=!0,b.dispatchEvent(new Bd(a.id)));if(!c){var e=a.querySelector("[data-xid=VpUvz]");e instanceof HTMLElement&&e.innerText.trim()!==""&&(b.dispatchEvent(new Ad(a.id)),c=!0)}},onRenderFinished:()=>{b.dispatchEvent(new zd(a.id))}}]};function Dd(a=!1){function b(c){window.parent.postMessage(JSON.stringify({type:"renderStatus",status:c}),"*")}return a?{onRenderStarted(){b(1)},onRenderFinished:()=>{b(5)},onStreamCanceled:()=>{b(6)}}:{}};var Ed=function(a,b){b=b.querySelectorAll("[data-target-container-id]");for(const e of b)if(b=e.getAttribute("data-target-container-id"),b=b===a.containerId?a.root:a.l.get(b)){if(e.firstElementChild&&!a.j){var c=e.firstElementChild;a.j=c.classList.contains("searchNextUiCoreIdWrapperIdWrapper__Container")||!!c.querySelector(".searchNextUiCoreIdWrapperIdWrapper__Container")}c=a;var d=e.querySelectorAll("[data-container-id]");for(const g of d)d=g.getAttribute("data-container-id"),c.l.set(d,g);oc(e,b)}},
Fd=class{constructor(a){this.containerId=a;this.l=new Map;this.root=document.createDocumentFragment();this.i=this.j=!1}};function Gd(){const a=new Fd("vxBctc");let b=null;return{onChunkWritten:c=>{const d=c.querySelector('[data-container-id="vxBctc"]');d&&(b=d);Ed(a,c);if(c=b||document.querySelector('[data-container-id="vxBctc"]'))for(;a.root.firstChild;)c.appendChild(a.root.firstChild)}}};function Hd(){let a=null;return{onChunkWritten:()=>{a||=document.querySelector('[data-scope-id="qZRtEe"]');a&&Qc(a)},onRenderFinished:()=>{a&&Pc(a)}}};function Id(){let a=!1,b=null;return{onRenderStarted:()=>{O(new Ta);const c=jb();c&&(b=kb(c),O(new Ua(b.title,b.J)))},onChunkWritten:c=>{!a&&(c=jb(c)||jb())&&(O(new Ra),c=kb(c),b&&b.title===c.title&&b.J===c.J||(O(new Ua(c.title,c.J)),a=!0))},onRenderFinished:()=>{O(new Sa)}}};function Jd({root:a}){function b(){return d.find(f=>{f.i||(f.i=!!f.root.querySelector(".Lucn7c"));return f.i||f.j})}function c(f,k){Ed(k,f);Ed(k,a)}const d=[];let e=null;const g=[];return{onDocumentOpen:f=>{g.push(f)},onChunkWritten:f=>{for(;f.querySelector('[data-target-container-id="L9N7ob"]')||a.querySelector('[data-target-container-id="L9N7ob"]');)e=new Fd("L9N7ob"),d.unshift(e),c(f,e);e!=null&&d.forEach(k=>{c(f,k)})},onRenderFinished:()=>{for(const k of g)d.forEach(h=>{c(k,h)});const f=document.querySelector('[data-scope-id="qZRtEe"] [data-container-id="L9N7ob"]');
pc(f,b().root);google.drty(f)}}};function Kd(){let a=!1;const b=()=>{if(!a){var c=document.querySelector('[data-scope-id="qZRtEe"] [data-container-id="L9N7ob"]'),d;if(d=c)d=!!c.querySelector(".ZAjsj")||c.classList.contains("ZAjsj"),d=!!c.querySelector(".Lucn7c")||d;if(d){(c=c.querySelector(".c2qZxd"))&&c.remove();c=document.querySelectorAll("[data-canvas-resizer-subscriber]");for(const e of c)e.dispatchEvent(new Qa);(new URLSearchParams(window.location.search)).has("rc")||hb();a=!0}}};return{onRenderStarted:b,onChunkWritten:b,onRenderFinished:()=>
{b();var c=document.querySelector('[data-scope-id="qZRtEe"] [data-container-id="L9N7ob"]');if(c){var d=!!c.querySelector(".c2qZxd");if(!a&&!d&&(c=c.querySelector(".uRqqye"))&&!c.querySelector(".c2qZxd")){d=document.createElement("div");d.classList.add("c2qZxd");var e=document.createElement("div");e.classList.add("yUe28");e.textContent="Your draft will appear here";d.appendChild(e);e=document.createElement("div");e.classList.add("wTDhsc");e.textContent="Tell me more to get started";d.appendChild(e);
c.appendChild(d)}}}}};function Ld({za:a,Ca:b,Da:c,root:d}){return a?[Id(),b||c?Jd({root:d}):null,Hd(),Kd(),Gd()].filter(e=>!!e):[]};function Md(a,b){let c=null;const d=nb();a.querySelectorAll("[data-tpcrb-host]").forEach(e=>{if(e.dataset.tpcrbHost==="true")c=e;else if(c){const g=c.querySelector(".kWjn6e");e=e.querySelector(".kWjn6e");const f=e?.querySelector('[data-xid="DChuCc"]');g&&e&&f&&c.replaceChild(e,g);d&&!nb()&&b?.qa(c)}})}function Z(a,b){a.querySelectorAll(".tonYlb, .CKgc1d").forEach(c=>{Md(c,b)})}
var Nd=class{constructor(a){this.v=a;this.ga=[]}onDocumentOpen(a){this.ga.push(a);Z(document,this.v);Z(a,this.v)}onChunkWritten(a){Z(document,this.v);Z(a,this.v)}onRenderFinished(){Z(document,this.v);for(const a of this.ga)Z(a,this.v)}};function Od(){return{onFirstByte(){if(window.localStorage.getItem("aim;;fnr")?.endsWith("true")){const a=document.createElement("style");a.appendChild(document.createTextNode('\n          [data-fstnv="true"] {\n            display: block !important;\n          }\n        '));document.head.appendChild(a)}}}};var Qd=class{constructor(a,b,c,d,e,g,f,k){this.appName=a;this.B=b;this.Aa=c;this.v=d;this.I=e;this.P=g;this.O=f;this.G=k;this.fa=!1;google.timers[this.B]||Na(this.B);this.R=this.v.i.t;a=this.R.getAttribute("eid")??google.kEI;b=this.appName;c=this.B;d=window.performance?.getEntriesByType?.("navigation")[0];e=new Map;d?.encodedBodySize&&e.set("cb",d.encodedBodySize);d?.decodedBodySize&&e.set("ucb",d.decodedBodySize);this.M={appName:b,B:c,oa:e,N:a};this.G&&(this.M.G=this.G)}onRenderStarted(){this.O&&
google.tick(this.B,this.O)}onChunkWritten(a){try{if(this.M.N=this.R?.getAttribute("eid")??google.kEI,this.I&&!google.timers[this.B]?.t[this.I]&&google.tick(this.B,this.I),this.R&&this.R.getBoundingClientRect().bottom>window.innerHeight&&!nb()&&!this.fa&&(google.c.u("ai"),this.fa=!0),this.Aa){const b=Ea(),c=qb(document),d=qb(a),e=rb(c.concat(d));sb(a,e,b)}}catch(b){Pd(b)}}onRenderFinished(){try{this.P&&google.tick(this.B,this.P),pb(this.M),this.v?.ea(),this.v?.T()}catch(a){Pd(a)}}onStreamCanceled(){try{pb(this.M),
this.v?.ea(),this.v?.T()}catch(a){Pd(a)}}};function Pd(a){a=a instanceof Error?a:Error("u");google.ml&&google.ml(a,!1)}function Rd({appName:a,B:b,I:c,P:d,O:e,v:g,G:f}){var k=(new URL(window.location.href)).searchParams.get("deb");if(k&&k.length!==0){var h=new Map;k=k.split(/(\d+)/g);for(let l=0;l<k.length;l+=2)h.set(k[l],k[l+1]||"1")}else h=new Map;h=h.get("bb")==="1";return new Qd(a,b,h,g,c,d,e,f)};const Sd={onRenderFinished:async()=>{Eb()}};window.iframes={map:new Map,get:function(a){window.iframes.map.has(a)||window.iframes.map.set(a,Promise.withResolvers());return window.iframes.map.get(a)}};
window.fetchAsyncContent=async function(a,b,c,d,e,g,f,k,h,l,t,r,q,v,u,m,n,x,D,B,Td,dc,Ud){if(arguments.length!==23)throw Error("v");var P=m===2?"aim_sxs":"aim";Na(P);google.tick(P,"aimcl");const C=document.getElementById(a);if(!C)throw Error("w`"+a);var p=new URLSearchParams(window.location.search),w=p.has("dr","1")&&x&&n||k,I=p.get("aec"),L=p.get("peek_pws");const fa=(r||q)&&d;if(w&&!fa)await wd({Ga:p.get("q"),A:C,ua:h?1:0,ta:0,xa:h});else{((new URLSearchParams(window.location.search)).get("mq")!==
null||Yb()!==null)&&!B||Zb(g,B);w=new URLSearchParams;w.set("cs",l?"1":"0");I&&w.set("aec",I);t&&w.set("rc","1");L&&w.set("peek_pws",L);m==null||m!==1&&m!==2||w.set("sxs",m.toString());p.has("cinpts")&&w.set("cinpts",p.get("cinpts"));p.has("gsessionid")&&w.set("gsessionid",p.get("gsessionid"));p.has("aai")&&w.set("aai",p.get("aai"));I=p.has("mstk");L=p.has("smstk");if(fa&&(L||I)){const Ka=I?"mstk":"smstk";L=p.get(Ka);w.set(Ka,L);w.set("csuir","1");v&&(b="folif");(p=p.get("aioh"))&&w.set("aioh",p);
C.setAttribute("eid",google.kEI);I&&(p=document.createElement("div"),p.setAttribute("data-mstk",L),p.style.display="none",C.appendChild(p));p=document.querySelectorAll("[data-ra-elrc]");p.length>0&&w.set("elrc",p[p.length-1].dataset.raElrc)}w={A:C,type:b,ka:!1,Ba:fa,na:w,X:!0};m===2&&(p=ld(w),I=document.querySelector("[data-sxs-lro-token][data-sxs-lro-signature]"),p.searchParams.set("mlro",I?.dataset.sxsLroToken),p.searchParams.set("mlros",I?.dataset.sxsLroSignature),w={...w,url:p});f&&(p=document.getElementById(f))&&
(w.La=p);p=new Aa(tb(C,b,a,r),google.timers[P].t.start,Td);P={plugins:[vc(C,p,{animate:c,Y:fa,W:!0,Z:!1,ha:t,ma:Ud}),Lc(C),Sd,new Nd(p),Od(),...Ld({za:t||dc,Ca:r,Da:q,root:C}),Rd({appName:"aim",B:P,v:p,G:r?"restore":void 0,I:"aimfc",P:"aimr",O:"aimf"}),ub(C),xc(),...Cd(C),Dd(D),lb(C),...(dc&&!t?[Mc(C)]:[])]};try{await ud(w,P)}catch(Ka){Fb(),T(S("Gd8Kre"),!1),T(S("J3KLwe"),!1),await Ya()}e&&!document.prerendering&&Wb(u)}};}).call(this);fetchAsyncContent('aim-chrome-initial-inline-async-container','folwr', true , true , true , false ,'aim-chrome-initial-inline-async-container', false , true , true , false , false , false , false , false , null , false , true , false , false , false , false , false , );I want to build a library that snapshots the current desktop state and provides as much information as can be extracted using multiple methodologies.To build a high-fidelity desktop state analysis library, you should use Python to orchestrate three distinct methodologies: Visual Inspection (pixels), Structural Querying (metadata), and OCR Analysis (textual context). 1. Snapshot Capture (Visual Foundation) Your library needs a fast, cross-platform method to capture the raw pixel data. mss is the best general-purpose choice for Python; it is cross-platform and handles multi-monitor setups natively.For Windows-only projects requiring extreme performance (240+ FPS), use dxcam. ScreenshotOne2. Structural Querying (OS Accessibility APIs) To extract "invisible" information (button roles, window titles, parent-child hierarchies), query the operating system directly. This provides a "ground truth" that vision models often miss. Windows: Use Pywinauto to hook into the UI Automation (UIA) framework.Cross-Platform: Use PyAutoGUI for high-level interaction, though it relies more on visual coordinates than internal metadata. UI Vision +43. Visual Identification & Region Labeling Once you have a snapshot, use Computer Vision to identify regions that lack metadata (e.g., inside a custom-rendered canvas or a video stream). Region Detection: Use OpenCV for template matching and feature detection.Object Detection: Integrate YOLO (Ultralytics) if you need to identify specific UI components (like "Minimize" buttons) regardless of their exact pixel appearance.Labeling: If you need to train your own models for specific desktop environments, use LabelImg to annotate your training snapshots. Neptune.ai +44. OCR Analysis (Contextual Text) For text that isn't accessible via the OS APIs (e.g., text in an image or a remote desktop window): EasyOCR is highly accurate for multilingual text and complex backgrounds because it is built on PyTorch.Pytesseract is a faster, lighter alternative for clean, high-contrast text. Summary of Library Stack Methodology TechnologyPurposeCapturemssFast, raw pixel snapshot.StructuralPywinauto / AppiumExtracting UI element trees and roles.VisualOpenCVFinding icons and region boundaries.TextualEasyOCRExtracting text from non-selectable regions.Next Step: Are you building this primarily for Windows, macOS, or a cross-platform environment, as this determines which accessibility hooks you can use?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemClose20 sitesHow to Take Screenshots with Python MSSJan 13, 2026 — Summary. mss is the best general-purpose screen capture library for Python: * Cross-platform (Windows, macOS, Linux) * Fast enough...ScreenshotOneHow to Take Screenshots with Python MSSJan 13, 2026 — Summary. mss is the best general-purpose screen capture library for Python: * Cross-platform (Windows, macOS, Linux) * Fast enough...ScreenshotOneWindows/Mac/Linux Desktop Automation 2022. Free Robotic ...The DesktopAutomation XModule is a native app for Windows, Mac and Linux. It adds "hands" and "eyes" to the Ui. Vision RPA core. T...UI Vision·Ui.Vision, AI & OCR11:44Top 8 Image-Processing Python Libraries Used in Machine LearningAug 30, 2023 — Top 8 Image-Processing Python Libraries Used in Machine Learning * OpenCV. Source: OpenCV. OpenCV is an open-source library that w...Neptune.aiBest Open-Source OCR Tools in 2025: A Comparison - UnstractDec 18, 2025 — Top Open-Source OCR Libraries Selected for Evaluation: * Tesseract. * PaddleOCR. * Docling. * EasyOCR. * Surya OCR. * Mistral OCR ...UnstractExploring Optical Character Recognition (OCR) - GeekyAntsOct 17, 2023 — Before commencing the experiment, we installed two crucial libraries: * OpenCV (Open Source Computer Vision Library): An open-sour...GeekyAntsOCR Magic with Python: Harnessing Text from Images with EaseMar 21, 2023 — Here are some of the most prominent Python libraries dedicated to OCR, each offering unique features and capabilities to cater to ...www.nathanpeper.comSnowflake Performance: Query Optimization | by Sushmasree Rudroju | Data Engineering Tools and Practices | Jan, 2026Jan 16, 2026 — The metadata service, or Metadata Cache, refers to the store of data and statistics on database objects, and things like the struc...MediumEmpowering Reliable Visual-Centric Instruction Following in MLLMsJan 6, 2026 — Textual (OCR) & Grounding: Targets reading, transcribing, or grounding textual information present in the image.arXivS2E/guest-images: Scripts to automatically create VM imagesMay 13, 2017 — Image building is divided into steps. Each step installs one or more software packages, then reboots the VM. The last step boots t...GitHubUncountable Platform HierarchyOct 15, 2025 — Projects can also be organized hierarchically. Child projects inherit data visibility from their parent, enabling structured views...UncountableTop 15 Desktop Automation ToolsSep 27, 2024 — Pywinauto is an open-source Python library designed to automate Windows GUI applications. It supports a wide range of Windows appl...BrowserStackTop 4 Open Source Alternatives to PywinautoSep 24, 2025 — It ( Pywinauto ) provides programmatic access to Windows controls through techniques like Microsoft UI Automation (UIA) and the ol...TestDriver.aiTop 12 Alternatives to PyAutoGUI for Windows/macOS/Linux TestingSep 24, 2025 — PyAutoGUI offers cross-platform reach but relies more on image and coordinate-based actions.TestDriver.aiRender Captured Video to Full Page Canvas | Damir's CornerMar 17, 2017 — In modern HTML 5 browsers you can render video from your camera inside a web page using the video element. However, to further pro...www.damirscorner.comRead, Display and Save Image with OpenCVApr 15, 2023 — 4. Feature detection and matching: OpenCV provides functions for detecting and matching features in images, such as SIFT, SURF, an...TechVidvanScreenomics: A Framework to Capture and Analyze Personal Life Experiences and the Ways that Technology Shapes ThemParallel to text extraction, the pictures and images nested within each screenshot can be cataloged. This is done with open-source...National Institutes of Health (NIH) | (.gov)Uncertainty-aware efficient subgraph isomorphism using graph topologyThere are couple of node matches at the top of the images which are missed due to the errors being outside our bounds. In order to...ScienceDirect.comTransTab: A transformer-based approach for table detection and tabular data extraction from scanned document imagesCompared to other OCR models, EasyOCR is more adaptable to recognizing multiple languages and fonts while maintaining high accurac...ScienceDirect.comEasyOCR | Extracting Text From Image using EasyOCRMay 1, 2025 — It ( EasyOCR ) detects the text from images but in my reference, while using it ( EasyOCR ) I found that it ( EasyOCR ) is the mos...Analytics VidhyaPython Tutorial | How to extract image metadata with PythonJan 8, 2020 — hey how's it going guys in this tutorial we're going to learn how to extract meta data from an image file. so here's the image tha...YouTube·Jie Jenn10m20 sitesHow to Take Screenshots with Python MSSJan 13, 2026 — Summary. mss is the best general-purpose screen capture library for Python: * Cross-platform (Windows, macOS, Linux) * Fast enough...ScreenshotOneWindows/Mac/Linux Desktop Automation 2022. Free Robotic ...The DesktopAutomation XModule is a native app for Windows, Mac and Linux. It adds "hands" and "eyes" to the Ui. Vision RPA core. T...UI Vision·Ui.Vision, AI & OCR11:44Top 8 Image-Processing Python Libraries Used in Machine LearningAug 30, 2023 — Top 8 Image-Processing Python Libraries Used in Machine Learning * OpenCV. Source: OpenCV. OpenCV is an open-source library that w...Neptune.aiShow allwindows and then linuxTo build a library that works on both Windows and Linux, you must implement a "Unified State Model" that abstracts platform-specific APIs into a single data structure. Python remains the best choice due to its bridge libraries for both OS-level accessibility trees. 1. Architectural Strategy: The Unified State Model Your library should return a standard object (e.g., a JSON tree) regardless of the OS, containing: Bounding Boxes: Standardized (x, y, w, h).Roles: Mapped from OS-specific names (e.g., ROLE_SYSTEM_PUSHBUTTON on Windows vs. push button on Linux) to a custom Button type.Hierarchy: Parent-child relationships for nested UI elements. 2. Implementation by Platform Windows Implementation On Windows, leverage the Microsoft UI Automation (UIA) framework, which is the modern standard for accessibility. Core Library: pywinauto is the most robust Python wrapper. It supports both older Win32 backends and modern UIA.Metadata Extraction: Use the .dump_tree() method or iterate through elements to get properties like Name, AutomationId, and ControlTypeName.Performance Note: For high-speed screen captures on Windows, dxcam is significantly faster than standard libraries. GitHub +3Linux Implementation On Linux, the landscape is fragmented between X11 and Wayland. You must use the AT-SPI (Assistive Technology Service Provider Interface). Core Library: pyatspi or dogtail are the primary tools for querying the AT-SPI bus.Metadata Extraction: These tools allow you to traverse the "Accessibility Tree" to find widgets, their states (focused, visible), and their text content.Captures: Use mss for snapshots, as it handles the X11/Wayland complexities better than most. GNOME +13. Comparison of Extraction Methods Feature Windows StrategyLinux StrategyAPIMS UI Automation (UIA)AT-SPI / pyatspi2Element Treepywinauto (backend="uia")dogtail or pyatspiSnapshotsdxcam (DirectX-based)mss (X11/Wayland)OCR FallbackEasyOCREasyOCR4. Critical Logic: The "Truth Resolver" Since your goal is to extract "as much information as possible," your library's logic should follow this hierarchy: Level 1 (Structural): Query OS APIs (UIA/AT-SPI) for exact metadata.Level 2 (Visual): Use OpenCV for template matching to identify known icons or regions if Level 1 returns "generic" container types.Level 3 (OCR): Use EasyOCR on regions identified as text-bearing but lacking metadata strings. These Python libraries help you query accessibility information on Linux using AT-SPI: "dogtail" for GUI testing and automation, and "pyatspi" as a wrapper for the ATK library.Automation through Accessibility - Fedora MagazineApr 10, 2024 — The API – dogtail. dogtail is a GUI test tool and automation framework written in Python. It is an extension and wrapper around At...Fedora MagazinePython Powered Accessibility - GNOME wikiApplications use the GNOME Accessibility Toolkit (ATK) library to provide accessibility and isolate them from the raw AT-SPI. For ...GNOMECompare Automation Workshop vs. pywinauto in 2025 - SlashdotPywinauto is a collection of Python modules designed for automating the GUI of Microsoft Windows. Its primary function is to facil...Slashdotpywinauto enables you to automate the Windows GUI - RedditMay 18, 2007 — If you just do some fast and quick stuff or need more control, pywinauto is really cool, although the documentation sucks like mos...RedditTop 17 Free Desktop Automation Testing Tools (2025)Feb 24, 2025 — It describes itself as a GUI automation library written in pure Python and well developed for Windows GUI. At its simplest, it all...Test Guildpywinauto/pywinauto: Windows GUI Automation with Python (based ...Jan 6, 2025 — pywinauto is a set of python modules to automate the Microsoft Windows GUI. At its simplest it allows you to send mouse and keyboa...GitHubNext Step: Do you need your library to handle Wayland specifically on Linux, or is an X11-focused solution sufficient for your needs?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemClose6 sitespywinauto/pywinauto: Windows GUI Automation with Python (based ...Jan 6, 2025 — pywinauto is a set of python modules to automate the Microsoft Windows GUI. At its simplest it allows you to send mouse and keyboa...GitHubpywinauto enables you to automate the Windows GUI - RedditMay 18, 2007 — If you just do some fast and quick stuff or need more control, pywinauto is really cool, although the documentation sucks like mos...RedditTop 17 Free Desktop Automation Testing Tools (2025)Feb 24, 2025 — It describes itself as a GUI automation library written in pure Python and well developed for Windows GUI. At its simplest, it all...Test GuildCompare Automation Workshop vs. pywinauto in 2025 - SlashdotPywinauto is a collection of Python modules designed for automating the GUI of Microsoft Windows. Its primary function is to facil...SlashdotPython Powered Accessibility - GNOME wikiApplications use the GNOME Accessibility Toolkit (ATK) library to provide accessibility and isolate them from the raw AT-SPI. For ...GNOMEAutomation through Accessibility - Fedora MagazineApr 10, 2024 — The API – dogtail. dogtail is a GUI test tool and automation framework written in Python. It is an extension and wrapper around At...Fedora Magazine6 sitespywinauto/pywinauto: Windows GUI Automation with Python (based ...Jan 6, 2025 — pywinauto is a set of python modules to automate the Microsoft Windows GUI. At its simplest it allows you to send mouse and keyboa...GitHubpywinauto enables you to automate the Windows GUI - RedditMay 18, 2007 — If you just do some fast and quick stuff or need more control, pywinauto is really cool, although the documentation sucks like mos...RedditTop 17 Free Desktop Automation Testing Tools (2025)Feb 24, 2025 — It describes itself as a GUI automation library written in pure Python and well developed for Windows GUI. At its simplest, it all...Test GuildShow allX11-focused solution sufficientFor an X11-focused setup, you have a much more stable and "queryable" environment than Wayland. You can directly tap into the X Server’s window properties for structural data that is often missing from accessibility trees. 1. The Core Stack for Windows & X11 To build this library, use these specific libraries to bridge the two worlds: Window Management (X11): Use python-xlib. It allows you to query the X Server for window geometry, titles, and the WM_CLASS, which is vital for identifying which application owns a specific screen region.Window Management (Windows): Use PyGetWindow for simple geometry or pywinauto for deep UIA tree inspection.The "Snapshotter": mss is the gold standard for X11 and Windows. It is significantly faster than Pillow for screen grabbing because it uses raw XLib/C-level calls. 2. Implementation Methodology Your library should execute a "Triple-Pass" extraction for every snapshot: The Structural Pass (The "Skeleton"):Linux: Use Xlib to get a list of all mapped windows and their absolute coordinates.Windows: Use pywinauto to get the UI Automation tree.Result: A list of "known" boxes (e.g., Chrome is at 0,0,800,600).The Visual Pass (The "Eyes"):Crop the mss snapshot based on the boxes found in the Structural Pass.Run OpenCV Template Matching to find common UI elements (close buttons, burger menus) that the OS might report only as "Generic Pane."The Contextual Pass (The "Brain"):Run EasyOCR only on the specific sub-regions identified in the previous steps. This prevents "OCR noise" from desktop wallpapers or icons. 3. Practical Code Entry Points To get started, your library's "Discovery" module should look like this: Task Linux (X11) ToolWindows ToolList Windowspython-xlibpywinautoGet Focused Windowdisplay.get_input_focus()GetForegroundWindow (via pywin32)Pixel CapturemssmssRegion LabelingOpenCV (Contours)OpenCV (Contours)4. Handling the "X11 Advantage" In X11, you can use the EWMH (Extended Window Manager Hints) library. This provides metadata that pyatspi might miss, such as whether a window is fullscreen, its PID, and its stack order (Z-index), which is crucial for determining which window is "on top" visually. Next Step: Should the library focus on real-time monitoring (e.g., 30fps state tracking) or on-demand high-detail snapshots?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allon-demand high-detail snapshotsSince you are prioritizing on-demand high-detail snapshots, you can afford the "computational tax" of heavy-duty OCR and deep tree traversal that real-time systems usually skip. Your library's workflow should follow a Breadth-First Extraction strategy to maximize data density. 1. The High-Detail Stack The Master Snapshot: Use mss to capture the desktop in raw pixels. Since it's on-demand, save it as a lossless PNG via Pillow to ensure the OCR engine doesn't struggle with JPEG artifacts.The Structural "Truth":Windows: pywinauto is essential here. Use it to crawl the UIA (User Interface Automation) tree. It can extract properties like AutomationId, Name, and even the current value of a slider or text field without OCR.Linux (X11): Use python-xlib for geometry and ewmh to identify window types (dialogs, menus, toolbars).Deep OCR: Since speed isn't the primary constraint, use EasyOCR. It uses a deep learning backend (CRAFT and CRNN) which is far more robust for high-detail extraction than Tesseract, especially for text over gradients or complex UI backgrounds. 2. Implementation Methodology To extract "as much information as can be extracted," implement these two specialized passes: Pass A: Metadata Enrichment Map the visual coordinates of windows to their internal process information. Windows: Query the handle (HWND) to get the process name and path using psutil.Linux: Use Xlib to get the _NET_WM_PID property of a window, then link it to the binary name via /proc. Pass B: Semantic Region Labeling For regions where metadata is missing (like a web app running inside a browser), use OpenCV: Canny Edge Detection: To find boundaries of custom UI buttons.Hierarchical Contours: To determine nesting (e.g., "this icon is inside this container"). 3. Proposed Data Structure Your library should output a single, nested JSON object following this hierarchy: json{
  "timestamp": "2023-10-27T10:00:00Z",
  "screen_resolution": [1920, 1080],
  "windows": [
    {
      "title": "Calculator",
      "process": "calc.exe",
      "geometry": {"x": 100, "y": 100, "w": 300, "h": 500},
      "z_order": 1,
      "elements": [
        {"type": "Button", "label": "7", "rect": [110, 150, 40, 40], "source": "UIA_Tree"},
        {"type": "Region", "label": "Display Area", "rect": [105, 110, 290, 30], "ocr_text": "42", "source": "EasyOCR"}
      ]
    }
  ]
}
Use code with caution.4. Comparison for On-Demand Detail Extraction Depth ToolingInformation GainedSurfacemss + PillowVisual pixels and resolution.Intermediatepywinauto / XlibWindow titles, PIDs, and button hierarchies.DeepEasyOCRText inside images, terminal output, and custom canvases.ExpertOpenCVIcons (via Template Matching) and logical region grouping.Next Step: Are you planning to store these snapshots in a database for later analysis, or will they be processed immediately by an agent or script?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allthey be processed immediately by an agent or scriptSince an AI agent is the consumer, your library's primary job is noise reduction. Agents fail when overwhelmed by raw pixel data or massive, irrelevant DOM trees. You need to provide a semantic summary. 1. The "Agent-Ready" Pipeline To make the data immediately actionable, your library should perform these three steps in a single call: Step 1: Focus Masking: Only process the "Active Window" in high detail. Use pywinauto (Win) or Xlib (Linux) to find the window with input focus. Ignore the background to save tokens and compute.Step 2: Semantic Mapping: Instead of just "Box at 10,20," provide "Submit Button at 10,20."Step 3: Text-Coordinate Anchoring: Agents need to know where to click to interact with text. Your OCR output must be a list of objects containing { "text": "Login", "center_point": [x, y] }. 2. Recommended Python "Agent" Stack Since you need immediate processing, use these high-speed, high-accuracy libraries: Component LibraryWhy for Agents?OCR EnginePaddleOCRFaster than EasyOCR, better at "Text Detection" (finding where text is) which is crucial for clicking.Object DetectionOpenCV (Template Matching)Best for finding static UI elements like "Close" or "Settings" icons.State DiffingHashlibGenerate a hash of the UI tree. If the hash hasn't changed, tell the agent "No state change" to save processing time.3. Critical Feature: The "Interactive Map" Agents work best when they have a Visual Confirmation alongside text. Your library should generate a "Labeled Screenshot": Take the snapshot.Draw numbered bounding boxes (e.g., [1], [2], [3]) over every interactable element.Provide the agent with a text list: 1: 'File' Menu, 2: 'Edit' Menu, 3: 'Search' Bar.Action: The agent simply says "Click 3." 4. Implementation Hook for Windows & X11 Use PyAutoGUI for the actual execution part of your library. While it's simple, its locateOnScreen() and click() functions are the most reliable cross-platform ways for an agent to turn "Intent" into "Input." Next Step: Will the agent be a Large Language Model (like GPT-4V or Claude 3.5 Sonnet) that can "see" images, or a pure-code script that needs a text-only representation of the screen?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allpure-code script with optional LLM assistanceFor a pure-code script that might occasionally consult an LLM, your library must prioritize a structured JSON schema over raw images. The script needs a "lookup table" of the desktop, while the LLM needs a distilled "textual scene description." 1. The "Dual-Output" Architecture Your library should return a single object with two views of the same snapshot: The Code View: A flat list of objects with specific coordinates, IDs, and types. This allows the script to do things like if 'Settings' in elements: click(elements['Settings'].center).The LLM View: A Markdown-formatted summary of the active window and primary text blocks. This is sent to the LLM to ask, "What should I do next?" without blowing your token budget. 2. Core Python Implementation (Pure-Code Focus) Windows & X11 Structural Mapping For the pure-code script, you need reliable ID-based targeting so the script doesn't break if a window moves 5 pixels. Windows: Use pywinauto to extract the AutomationId or ControlID. These are persistent across sessions.Linux (X11): Use python-xlib to get the WM_CLASS and WM_NAME. This tells your script exactly which app is which. High-Detail OCR for the Script Since a script can't "guess," use PaddleOCR. It is currently the industry leader for structured OCR output (returning text + layout + confidence scores) and is significantly faster than EasyOCR for on-demand use. 3. Recommended Logic for "Region Identification" To label regions for a script, your library should use a Hierarchical Resolver: Direct API (Fastest): Query the OS for the UI tree. If the element exists in the tree (like a button in a Windows Form), use those coordinates.Visual Anchor (Fallback): If the OS tree is empty (common in Linux/X11 or Electron apps), use OpenCV Template Matching to find known icons.OCR Labeling (Final Effort): Use PaddleOCR to find text strings and treat the bounding box of that text as the "button." 4. The Library's "Information Bundle" Your library should output a structure like this to support both code and LLMs: Property Value TypePurposeactive_windowStringScript logic (e.g., if app == "Terminal")element_treeList[Dict]Direct coordinate-based clicking for the scriptscene_summaryStringHigh-level Markdown for the LLM ("There is a Save dialog open")screenshot_pathStringPath to the .png for optional LLM vision analysis5. Essential Utility Libraries psutil: Essential for mapping a window to its process path (e.g., knowing that a window titled "Untitled" belongs to notepad.exe).mss: For the fastest cross-platform raw capture.PyAutoGUI: The best "Execution" library to actually move the mouse and type based on the coordinates your library finds. Next Step: Do you want to see a prototype code snippet for the Windows UIA tree extraction or the X11 window-geometry mapping first?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allthe Windows UIA tree extractionTo extract the most metadata on Windows, you should use pywinauto with the uia backend. It accesses the Microsoft UI Automation framework, which provides deep access to window roles, names, and structural hierarchies that simple pixel-scraping misses. 1. Prerequisite Install the core library and psutil to link windows to actual system processes: bashpip install pywinauto psutil
Use code with caution.2. Prototype: Recursive UIA Tree Extractor This script captures the "Active Window" and converts its entire UI structure into a script-friendly list of dictionaries. pythonfrom pywinauto import Desktop
import psutil

def get_active_window_state():
    # 1. Connect to the focused window using the UIA backend
    # This is the "Truth" source for Windows UI
    top_window = Desktop(backend="uia").active()
    
    # 2. Extract Process Metadata
    proc_id = top_window.process_id()
    proc = psutil.Process(proc_id)
    
    window_data = {
        "title": top_window.window_text(),
        "process_name": proc.name(),
        "pid": proc_id,
        "rect": top_window.rectangle().to_tuple(), # (L, T, R, B)
        "elements": []
    }

    # 3. Recursive function to crawl the UIA Tree
    def crawl_elements(element, depth=0):
        for child in element.children():
            try:
                # Extract structured info for the script/agent
                info = {
                    "type": child.element_info.control_type,
                    "name": child.element_info.name,
                    "automation_id": child.element_info.automation_id,
                    "rect": child.rectangle().to_tuple(),
                    "is_enabled": child.is_enabled(),
                    "is_visible": child.is_visible(),
                    "depth": depth
                }
                window_data["elements"].append(info)
                
                # Limit depth for performance if needed
                if depth < 5: 
                    crawl_elements(child, depth + 1)
            except Exception:
                continue

    crawl_elements(top_window)
    return window_data

# Example Usage
state = get_active_window_state()
print(f"Captured {len(state['elements'])} elements from {state['title']}")
Use code with caution.3. Why this works for your "Pure-Code" Agent AutomationId: Unlike coordinates, the automation_id (e.g., "SubmitButton") is usually static. Your script can find it even if the user resizes the window. Pywinauto Documentation details these properties.Control Type: It identifies if something is a Button, Edit (text field), or List, allowing your script to decide whether to type_keys() or click().Coordinate Mapping: The rect returned is in absolute screen coordinates, which you can pass directly to PyAutoGUI for interaction. 4. Handling "Invisible" Data If an element in this tree has no name, that is your trigger to crop that specific rect from an mss snapshot and send it to OCR or an LLM for identification. Next Step: Would you like to see the X11 version using python-xlib to achieve a similar structural map on Linux?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow alllike to see the X11 version using python-xlibTo map X11 windows, you’ll use python-xlib to traverse the window tree. Unlike Windows UIA, X11 is more "bare bones"—it gives you window geometry and process IDs (via EWMH), but it won't give you internal buttons unless the app uses a specific toolkit bridge. 1. Prerequisite You'll need python-xlib for the core protocol and ewmh to make sense of window properties like titles and PIDs easily. bashpip install python-xlib ewmh psutil
Use code with caution.2. Prototype: X11 Window Tree Mapper This script crawls the X11 window hierarchy, extracts coordinates, and maps them to system processes. pythonfrom Xlib import display
from ewmh import EWMH
import psutil

def get_x11_state():
    d = display.Display()
    ewmh = EWMH(_display=d)
    root = d.screen().root
    
    # 1. Get the current active (focused) window
    active_window = ewmh.getActiveWindow()
    
    # 2. Get all managed windows in stacking order
    client_list = ewmh.getClientList()
    
    desktop_state = []

    for win in client_list:
        try:
            # Extract basic geometry
            geom = win.get_geometry()
            # Convert relative coordinates to absolute screen coordinates
            abs_geom = win.translate_coords(root, 0, 0)
            
            # Extract Metadata
            name = ewmh.getWMName(win)
            if isinstance(name, bytes): name = name.decode()
            
            pid = ewmh.getWmPid(win)
            proc_name = psutil.Process(pid).name() if pid else "Unknown"
            
            window_data = {
                "id": win.id,
                "name": name,
                "process": proc_name,
                "pid": pid,
                "rect": {
                    "x": abs_geom.x, 
                    "y": abs_geom.y, 
                    "w": geom.width, 
                    "h": geom.height
                },
                "is_active": win == active_window
            }
            desktop_state.append(window_data)
        except Exception:
            continue
            
    return desktop_state

# Example Usage
state = get_x11_state()
for win in state:
    if win['is_active']:
        print(f"ACTIVE: {win['name']} [{win['process']}] at {win['rect']}")
Use code with caution.3. Bridging the Gap for your Agent The "Invisible" Inner UI: Standard X11 only sees the "top-level" window. To get the buttons inside a Linux app for your script, you would typically fallback to pyatspi (for GNOME/GTK apps) or use your OCR/Computer Vision pass on the window's specific rect.Coordinate Stability: The translate_coords method is critical; it ensures the (x, y) you get is relative to the entire screen, making it directly compatible with PyAutoGUI for clicking.Window Identification: The id returned is the XID. You can use this with command-line tools like xdotool or import if you need to perform side-channel actions. 4. Comparison: Windows UIA vs. X11 Xlib Feature Windows (UIA)Linux (X11)DepthRecursive (Buttons, Lists, etc.)Top-level (Windows/Dialogs)Process LinkBuilt-in process_idRequires _NET_WM_PID hintCoordinatesAbsoluteRelative (needs translation)Next Step: Since X11 provides less internal detail than Windows, would you like to see how to integrate the OpenCV template matching to find buttons inside these X11 window regions?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allhow to integrate the OpenCV template matching to find buttons inside these X11 window regionsTo find buttons within X11 windows where metadata is missing, you’ll perform a "Targeted Visual Scan." Instead of searching the whole screen, you crop the snapshot to the window’s coordinates (found via Xlib) and run OpenCV Template Matching. 1. Prerequisites You’ll need OpenCV for the vision logic and mss for the high-detail snapshot. bashpip install opencv-python mss numpy
Use code with caution.2. Prototype: Targeted Template Matcher This function takes the coordinates from your X11 script, crops a high-res screenshot, and finds a specific UI element (like a "Close" icon or "Submit" button). pythonimport cv2
import mss
import numpy as np

def find_button_in_region(window_rect, template_path, threshold=0.8):
    """
    window_rect: {'x': int, 'y': int, 'w': int, 'h': int} from Xlib
    template_path: Path to a small .png of the button you're looking for
    """
    with mss.mss() as sct:
        # 1. Capture the specific window region (High-Detail Snapshot)
        # sct.grab expects {'top': y, 'left': x, 'width': w, 'height': h}
        monitor = {
            "top": window_rect['y'], 
            "left": window_rect['x'], 
            "width": window_rect['w'], 
            "height": window_rect['h']
        }
        screenshot = np.array(sct.grab(monitor))
        
        # Convert to Grayscale for faster, more robust matching
        scene_gray = cv2.cvtColor(screenshot, cv2.COLOR_BGRA2GRAY)
        template = cv2.imread(template_path, cv2.IMREAD_GRAYSCALE)
        w, h = template.shape[::-1]

        # 2. Perform Template Matching
        res = cv2.matchTemplate(scene_gray, template, cv2.TM_CCOEFF_NORMED)
        
        # 3. Filter results by confidence threshold
        loc = np.where(res >= threshold)
        
        found_elements = []
        for pt in zip(*loc[::-1]):  # Switch columns and rows
            # Convert local crop coordinates back to absolute screen coordinates
            abs_x = window_rect['x'] + pt[0] + (w // 2)
            abs_y = window_rect['y'] + pt[1] + (h // 2)
            
            confidence = res[pt[1], pt[0]]
            found_elements.append({
                "center": (abs_x, abs_y),
                "confidence": float(confidence)
            })
            
            # If you only need the best match, break here
            break 
            
    return found_elements

# Example Usage with X11 window data
# win_rect = {'x': 100, 'y': 100, 'w': 800, 'h': 600}
# matches = find_button_in_region(win_rect, 'close_icon.png')
Use code with caution.3. Key Integration Logic for Your Agent Recursive Hashing: To speed things up, have your script hash the window region. Only run the OpenCV scan if the pixels have actually changed since the last "on-demand" request.The "Template Library": Since you're targeting X11, your library should include a folder of templates for common UI themes (e.g., Adwaita for GNOME, Breeze for KDE).Scaling Robustness: Template matching fails if the window is zoomed. If a match isn't found, use OpenCV's resize to try matching at 0.9x or 1.1x scale. 4. Why this is "Agent-Ready" By combining X11 geometry and OpenCV, you provide the agent with a Semantic Anchor. Instead of the agent guessing where a button is, your script says: "Window 'Terminal' found. Process 'bash' found. 'Close' icon confirmed visually at (x,y) with 95% confidence." Next Step: Do you need a strategy for labeling custom regions that aren't icons (e.g., identifying a "text input area" in a custom app where there's no text yet)?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allstrategy for labeling custom regions that aren't icons (e.g., identifying a "text input area" in a custom app where there's no text yet)?When metadata and icons fail, you must switch from "searching for a specific thing" to "analyzing UI geometry." Since text inputs are usually defined by their borders (rectangles), you can use OpenCV Contours to identify them. 1. The "Empty Box" Detection Strategy Most text inputs are high-contrast rectangular containers. You can find them by looking for specific aspect ratios and "hollow" centers. pythonimport cv2
import numpy as np

def find_input_fields(window_crop):
    # 1. Pre-process: Grayscale and Canny Edge Detection
    gray = cv2.cvtColor(window_crop, cv2.COLOR_BGR2GRAY)
    edged = cv2.Canny(gray, 50, 150)

    # 2. Find closed shapes (contours)
    contours, _ = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    potential_inputs = []
    for cnt in contours:
        # Approximate the shape to a polygon
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        
        # We are looking for 4-sided polygons (rectangles)
        if len(approx) == 4:
            x, y, w, h = cv2.boundingRect(approx)
            aspect_ratio = w / float(h)
            
            # 3. Filter by typical "Input Box" constraints
            # Inputs are usually wider than they are tall (e.g., 3:1 to 15:1)
            if aspect_ratio > 2.0 and w > 50 and h > 20:
                potential_inputs.append({"rect": (x, y, w, h), "type": "possible_input"})
                
    return potential_inputs
Use code with caution.2. The "Label-Anchor" Association An empty input field is useless without context. A pure-code script needs to know which field is which. Your library should use Spatial Proximity: Run OCR (PaddleOCR) to find all text on the screen (e.g., "Username", "Password").Run the Contour Detector above to find all empty boxes.The Logic: Map the nearest text label to the box. If "Username" is 5 pixels to the left of Box A, Box A is labeled input_username. 3. Advanced Strategy: Semantic Segmentation (YOLO) If OpenCV geometry is too "noisy" (e.g., it picks up every decorative border), you should use a small, specialized AI model like YOLOv8. The Library: Ultralytics YOLOv8.The Workflow: You provide ~100 screenshots of your target app labeled with LabelImg.The Result: The model will return "Text Field" with high confidence even if the border is faint or custom-styled. 4. Comparison of Methods for Custom Regions Method Best ForProsConsOpenCV ContoursStandard boxes/formsFast, no training neededStruggles with complex backgroundsOCR ProximityForms with labelsGives "Meaning" to boxesFails if labels are missing/vagueYOLOv8Custom/Skinned UIsExtremely accurateRequires a small training dataset5. Recommendation for your Agent Implement Proximity Mapping first. It is the most robust "pure-code" way to turn visual boxes into semantic data. Your library should return:{"label": "Email Address", "input_coords": [x, y], "confidence": 0.92} Next Step: Would you like to see a code snippet that combines OCR text labels with nearby empty boxes to create these semantic IDs?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allsee a code snippet that combines OCR text labels with nearby empty boxes to create these semantic IDsTo create a "Semantic Map" for a script, you need to link Static Text (the label) to Geometric Boxes (the input). This script uses PaddleOCR for the text and OpenCV for the boxes, then calculates the Euclidean distance to pair them.1. Prerequisitebashpip install paddleocr opencv-python numpy
Use code with caution.2. Prototype: Label-to-Box AssociatorThis script identifies "Email:" and links it to the empty rectangle sitting right next to it.pythonimport cv2
import numpy as np
from paddleocr import PaddleOCR

# Initialize OCR (Angle classification helps with rotated text)
ocr = PaddleOCR(use_angle_cls=True, lang='en', show_log=False)

def identify_semantic_inputs(image_path):
    img = cv2.imread(image_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    
    # 1. Extract Text Labels via OCR
    # Returns: [ [[ [coords], ("text", confidence) ], ... ]]
    ocr_results = ocr.ocr(image_path, cls=True)[0]
    labels = []
    if ocr_results:
        for res in ocr_results:
            coords, (text, conf) = res
            # Get center point of the text
            cx = int(np.mean([p[0] for p in coords]))
            cy = int(np.mean([p[1] for p in coords]))
            labels.append({"text": text, "center": (cx, cy)})

    # 2. Extract Rectangular Input Boxes via OpenCV
    edged = cv2.Canny(gray, 50, 150)
    contours, _ = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    boxes = []
    for cnt in contours:
        approx = cv2.approxPolyDP(cnt, 0.02 * cv2.arcLength(cnt, True), True)
        if len(approx) == 4:
            x, y, w, h = cv2.boundingRect(approx)
            if w > 40 and h > 15 and (w/h) > 1.5: # Typical input box aspect
                boxes.append({"rect": (x, y, w, h), "center": (x + w//2, y + h//2)})

    # 3. Associate Labels with nearest Boxes (The "Semantic" Pass)
    semantic_map = []
    for label in labels:
        # Filter for labels that look like field names (short, end with ':')
        if len(label['text']) < 20:
            best_box = None
            min_dist = 150 # Max distance to look for a box (pixels)
            
            for box in boxes:
                dist = np.sqrt((label['center'][0] - box['center'][0])**2 + 
                               (label['center'][1] - box['center'][1])**2)
                if dist < min_dist:
                    min_dist = dist
                    best_box = box
            
            if best_box:
                semantic_map.append({
                    "id": label['text'].strip(':').lower().replace(' ', '_'),
                    "label": label['text'],
                    "click_coords": best_box['center'],
                    "box_rect": best_box['rect']
                })
                
    return semantic_map

# Usage
# result = identify_semantic_inputs("screenshot.png")
# print(f"Found input for: {result[0]['id']} at {result[0]['click_coords']}")
Use code with caution.3. How your Agent uses thisInstead of the agent looking at pixels, your library provides a clean API:The Script: interact.click(state['inputs']['username']['click_coords'])The LLM: "I see a form with username and password fields. I will click the username field."4. Robustness TipsThe "Placeholder" Check: If an input box contains gray text (e.g., "Enter email..."), the OCR will detect it. You should check if the OCR text center is inside a detected box; if so, treat that text as the label for that specific box.Z-Order: On X11, always ensure you are only running this on the active_window rectangle to prevent labels from a background window being mapped to a box in the foreground.Next Step: Do you want to refine the search radius logic (e.g., favoring boxes to the right of a label versus boxes below) to better match common UI design patterns? Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allrefine the search radius logic (e.g., favoring boxes to the right of a label versus boxes below) to better match common UI design patternsTo match standard UI design patterns (Western-style forms), you need a weighted directional bias. In UI design, labels are almost exclusively placed either directly to the left of the input or directly above it. 1. The "Biased Euclidean" Formula Instead of a simple circular radius, we use a weighted distance where vertical distance (\(dy\)) is penalized more than horizontal distance (\(dx\)) when the label is to the left, and horizontal distance (\(dx\)) is penalized when the label is above. 2. Prototype: Directional Semantic Resolver This logic replaces the "Distance Pass" in your previous script. It favors boxes to the Right (Left-aligned labels) and Below (Top-aligned labels). pythonimport numpy as np

def find_best_box_for_label(label_center, boxes):
    lx, ly = label_center
    best_box = None
    min_score = float('inf')
    
    # UI Design Constraints
    MAX_DISTANCE = 200 
    
    for box in boxes:
        bx, by = box['center']
        dx = bx - lx
        dy = by - ly
        
        # 1. Directional Filtering
        # We ignore boxes to the left or far above the label
        if dx < -20 or dy < -20: 
            continue
            
        # 2. Score Calculation (Lower is better)
        # UI Pattern A: Label to the LEFT of box (dx is positive, dy is near 0)
        if dx > 0 and abs(dy) < 30:
            score = dx * 1.0 + abs(dy) * 2.0  # Horizontal bias
            
        # UI Pattern B: Label ABOVE the box (dy is positive, dx is near 0)
        elif dy > 0 and abs(dx) < 50:
            score = dy * 1.2 + abs(dx) * 1.5  # Vertical bias
            
        else:
            # Fallback for diagonal or distant boxes
            score = np.sqrt(dx**2 + dy**2) * 3.0 

        # 3. Final Selection
        if score < min_score and score < MAX_DISTANCE:
            min_score = score
            best_box = box
            
    return best_box
Use code with caution.3. Why this Logic is "Agent-Ready" Anchor Reliability: By penalizing diagonal matches, you prevent "Email" from accidentally linking to the "Password" box if they are close together.Visual Alignment: It prioritizes elements that share an axis, mimicking how Google Material Design and Microsoft Fluent UI align form fields.Validation: If min_score is high, your library can flag the region as "Uncertain", prompting your code script to request LLM assistance to resolve the ambiguity. 4. Implementation with mss and PaddleOCR When integrating this into your library: Step 1: Use mss to grab the active_window pixels.Step 2: Use PaddleOCR for text detection.Step 3: Use OpenCV for box detection.Step 4: Run the Biased Resolver to pair them. Next Step: Should we look at how to persist these semantic mappings so the agent can remember that "the third box is always the PIN field" even if the OCR fails on a single frame?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allpersist these semantic mappings so the agent can remember that "the third box is always the PIN field" even if the OCR fails on a single frameTo make your agent "smart" and resilient to OCR glitches, you should implement a Persistent State Registry. Instead of treating every snapshot as a brand-new world, your library should "anchor" new observations to a historical model based on Structural Fingerprinting. 1. The Strategy: Structural Fingerprinting Since OCR can be flaky but window structures (UIA trees in Windows or X11 geometries) are stable, you use the window's signature as the key for your memory. The Key: A hash of the window title + process name + window dimensions.The Value: A "Canonical Map" of semantic IDs to relative coordinates. 2. Prototype: Semantic Memory Manager This module stores successful mappings and "votes" on their location over time to filter out noise. pythonimport hashlib
import json
from collections import defaultdict

class UIMemory:
    def __init__(self):
        # Memory structure: {window_hash: {semantic_id: [list_of_observed_coords]}}
        self.registry = defaultdict(dict)

    def _get_window_hash(self, window_data):
        # Create a unique fingerprint for the current app state
        sig = f"{window_data['process']}-{window_data['title']}-{window_data['rect']['w']}"
        return hashlib.md5(sig.encode()).hexdigest()

    def update_memory(self, window_data, observed_mappings):
        win_hash = self._get_window_hash(window_data)
        
        for mapping in observed_mappings:
            sem_id = mapping['id']
            if sem_id not in self.registry[win_hash]:
                self.registry[win_hash][sem_id] = []
            
            # Store relative coordinates (offset from window top-left)
            rel_x = mapping['click_coords'][0] - window_data['rect']['x']
            rel_y = mapping['click_coords'][1] - window_data['rect']['y']
            self.registry[win_hash][sem_id].append((rel_x, rel_y))

    def recall(self, window_data, semantic_id):
        win_hash = self._get_window_hash(window_data)
        observations = self.registry[win_hash].get(semantic_id)
        
        if not observations:
            return None
            
        # Return the "Average" or "Median" coordinate to ignore OCR jitters
        avg_x = sum(o[0] for o in observations) / len(observations)
        avg_y = sum(o[1] for o in observations) / len(observations)
        
        # Convert back to absolute screen coordinates
        return (int(avg_x + window_data['rect']['x']), int(avg_y + window_data['rect']['y']))

# Usage in your library
# memory.update_memory(current_window, current_semantic_map)
# pin_coords = memory.recall(current_window, "pin_field")
Use code with caution.3. Advanced Persistence Features For a production-grade library, add these three layers of reliability: Disk Persistence: Use the Python pickle or SQLite module to save the registry to a file. This allows the agent to "remember" the UI layout even after a reboot.The "Confidence Decay": If the OCR fails to see a label 5 times in a row, but the memory says it should be there, use the memory. If it fails 100 times, the UI has likely changed; delete the memory entry.Visual Verification: Even if memory tells the agent where the "PIN" field is, run a quick OpenCV check on that specific 50x50 pixel area to ensure it still looks like an empty box before clicking. 4. Why this benefits the Agent Speed: The script can skip the heavy PaddleOCR pass on subsequent snapshots if the window fingerprint hasn't changed.Reliability: It handles "transient" UI states (e.g., a notification pop-up partially covering a label) because the agent relies on the stored "True" coordinate. Summary of Library Stack Capture: mssOCR: PaddleOCRGeometry: OpenCVExecution: PyAutoGUIMemory: SQLite or json Next Step: Would you like to see how to implement the visual verification step to double-check a remembered coordinate before the script performs an action?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allsee how to implement the visual verification step to double-check a remembered coordinate before the script performs an actionTo ensure your agent doesn't "click blindly" based on stale memory, you implement a Visual Sanity Check. This involves cropping a small "verification patch" around the target coordinates and checking it for specific geometric features (like the presence of a box or a specific icon) before the click is committed. 1. The Verification Logic Your library should verify the target using two methods: Hollow Check: Ensures the area still looks like an empty input field (high-contrast edges, mostly white/dark center).Stability Check: Compares the current patch against a "Golden Image" (a small crop saved when the mapping was first successfully created). 2. Prototype: Visual Verifier Module This script uses OpenCV to inspect the target area. If the verification fails, the library can trigger a full "re-scan" of the desktop. pythonimport cv2
import mss
import numpy as np

def verify_target_is_input(coords, padding=30):
    """
    coords: (x, y) center point to verify
    padding: pixels around the center to inspect
    """
    x, y = coords
    
    with mss.mss() as sct:
        # 1. Capture a small "Verification Patch"
        monitor = {"top": y - padding, "left": x - padding, "width": padding*2, "height": padding*2}
        patch = np.array(sct.grab(monitor))
        
        # 2. Convert to Grayscale & Threshold
        gray = cv2.cvtColor(patch, cv2.COLOR_BGRA2GRAY)
        # Use Otsu's thresholding to find the 'hollow' parts of the input field
        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # 3. Analyze Edge Density
        # An input field has edges at the boundaries but is empty in the middle
        edges = cv2.Canny(gray, 50, 150)
        edge_count = np.sum(edges > 0)
        
        # 4. Success Criteria
        # - Not too many edges (isn't a complex image or text)
        # - High contrast presence (the box border)
        is_valid = 100 < edge_count < 1000  # Adjust based on your typical UI
        
        return is_valid, patch

# Usage in your library
# target_coords = memory.recall(window_data, "pin_field")
# ok, patch_img = verify_target_is_input(target_coords)
# if not ok:
#     print("Visual verification failed. Re-scanning UI...")
#     re_map_ui()
# else:
#     pyautogui.click(target_coords)
Use code with caution.3. Comparing to "Golden Images" For even higher reliability, save the patch the first time you identify a button or field. When saving memory: Save cv2.imwrite(f"memory_{id}.png", patch).When recalling: Use OpenCV Template Matching to compare the current patch with the saved one. Use a high threshold (e.g., 0.9). If they don't match, the UI has likely shifted or a different window is blocking the view. 4. Why this is "Agent-Safe" Safety Interlock: It prevents the script from clicking "Delete" if the "Delete" button has moved under the "Save" button's old coordinates.Implicit State Check: If the verification fails, it serves as a signal that the Environment State has changed, prompting the agent to pause and re-evaluate. Summary of the Final Library Flow Trigger: Script/Agent requests action on "login_button".Recall: Library pulls coordinates from SQLite Memory.Verify: Library captures a 60x60 patch and runs the Visual Sanity Check.Fallback: If check fails, run PaddleOCR + OpenCV for a fresh mapping.Execute: PyAutoGUI performs the click. Next Step: Do you want to package this into a unified Python Class that handles the initialization of OCR, the memory database, and the X11/Windows platform switching automatically?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allpackage this into a unified Python Class that handles the initialization of OCR, the memory database, and the X11/Windows platform switching automaticallyHere is the architecture for your unified library, designed as a singleton-style class. It dynamically selects the backend based on the OS and coordinates the OCR, Vision, and Memory layers. 1. Prerequisite Installation bashpip install mss paddleocr opencv-python numpy psutil pywinauto python-xlib ewmh
Use code with caution.2. The Unified DesktopOracle Class pythonimport os, sys, hashlib, json, sqlite3, numpy as np
import cv2, mss, psutil
from paddleocr import PaddleOCR

# Platform Specifics
if sys.platform == 'win32':
    from pywinauto import Desktop
else:
    from Xlib import display
    from ewmh import EWMH

class DesktopOracle:
    def __init__(self, db_path="ui_memory.db"):
        self.os = sys.platform
        self.sct = mss.mss()
        # Initialize OCR once (heavy operation)
        self.ocr = PaddleOCR(use_angle_cls=True, lang='en', show_log=False)
        self._init_db(db_path)
        
        if self.os != 'win32':
            self.d = display.Display()
            self.ewmh = EWMH(_display=self.d)

    def _init_db(self, path):
        self.conn = sqlite3.connect(path)
        self.conn.execute("CREATE TABLE IF NOT EXISTS ui_map (hash TEXT, sem_id TEXT, rel_x INT, rel_y INT, PRIMARY KEY(hash, sem_id))")

    def get_active_window_meta(self):
        """Standardizes OS-specific metadata into a single dict."""
        if self.os == 'win32':
            win = Desktop(backend="uia").active()
            rect = win.rectangle()
            return {
                "title": win.window_text(),
                "rect": {"x": rect.left, "y": rect.top, "w": rect.width(), "h": rect.height()},
                "pid": win.process_id()
            }
        else:
            win = self.ewmh.getActiveWindow()
            geom = win.get_geometry()
            abs_geo = win.translate_coords(self.d.screen().root, 0, 0)
            name = self.ewmh.getWMName(win)
            return {
                "title": name.decode() if isinstance(name, bytes) else name,
                "rect": {"x": abs_geo.x, "y": abs_geo.y, "w": geom.width, "h": geom.height},
                "pid": self.ewmh.getWmPid(win)
            }

    def _get_snapshot(self, rect):
        monitor = {"top": rect['y'], "left": rect['x'], "width": rect['w'], "height": rect['h']}
        return np.array(self.sct.grab(monitor))

    def get_semantic_map(self, force_refresh=False):
        """The 'Brains': Combines OCR + Geometry + Memory."""
        meta = self.get_active_window_meta()
        win_hash = hashlib.md5(f"{meta['title']}-{meta['rect']['w']}".encode()).hexdigest()
        
        # 1. Try Memory First
        if not force_refresh:
            cached = self.conn.execute("SELECT sem_id, rel_x, rel_y FROM ui_map WHERE hash=?", (win_hash,)).fetchall()
            if cached:
                return {row[0]: (row[1] + meta['rect']['x'], row[2] + meta['rect']['y']) for row in cached}

        # 2. Heavy Extraction Pass (OCR + OpenCV)
        img = self._get_snapshot(meta['rect'])
        ocr_res = self.ocr.ocr(img, cls=True)
        
        # [Insert the Biased Euclidean logic here to pair text to boxes]
        # For brevity, this returns a simplified mapping
        new_mappings = {} 
        # ... logic to populate new_mappings ...

        # 3. Persist
        for sem_id, coords in new_mappings.items():
            rel_x, rel_y = coords[0] - meta['rect']['x'], coords[1] - meta['rect']['y']
            self.conn.execute("INSERT OR REPLACE INTO ui_map VALUES (?, ?, ?, ?)", (win_hash, sem_id, rel_x, rel_y))
        self.conn.commit()
        
        return new_mappings

    def verify_and_click(self, sem_id):
        """Verifies visual state before committing action."""
        mapping = self.get_semantic_map()
        if sem_id not in mapping:
            return False
        
        coords = mapping[sem_id]
        # Perform visual check (from previous step)
        # if verify_target_is_input(coords): 
        #     pyautogui.click(coords)
        return True
Use code with caution.3. Key Library Features Automatic Handshake: It detects if you're on Windows (UIA) or Linux (X11) and adjusts its coordinate translation math.Singleton OCR: It initializes the PaddleOCR engine once, preventing the 2–5 second lag that happens if you re-init every time you need a snapshot.Persistent SQLite Memory: Mappings are saved to ui_memory.db. If your script restarts, it can find "Submit" without running OCR, provided the window dimensions haven't changed. 4. Strategy for the "Agent" Your pure-code script should now only interact with the DesktopOracle. Call oracle.get_active_window_meta() to verify it's in the right app.Call oracle.get_semantic_map() to get a "dictionary of clickables."Execute actions via oracle.verify_and_click("login_button"). Next Step: Would you like to add a logging layer that exports these "Scene Descriptions" as Markdown so an LLM can provide high-level reasoning when the pure-code script gets stuck?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow alladd a logging layer that exports these "Scene Descriptions" as Markdown so an LLM can provide high-level reasoning when the pure-code script gets stuckTo make your library "LLM-ready," we’ll add a generate_scene_description method. This transforms the raw coordinate data and process metadata into a structured Markdown document that fits perfectly into a Large Language Model's context window. 1. The Markdown Scene Generator Add this method to your DesktopOracle class. It summarizes the window's state and lists interactive elements in a way that an LLM can reason about (e.g., "The 'Submit' button is currently disabled"). python    def generate_scene_description(self, semantic_map):
        """Generates a Markdown summary for LLM reasoning."""
        meta = self.get_active_window_meta()
        
        md = [
            f"# Desktop Scene Report",
            f"**Active Window:** {meta['title']}",
            f"**Process:** {meta['process_name'] if 'process_name' in meta else meta['pid']}",
            f"**Resolution:** {meta['rect']['w']}x{meta['rect']['h']}",
            f"\n## Identified Interactive Elements",
            "| ID | Type | Label | Location |",
            "| :--- | :--- | :--- | :--- |"
        ]

        for sem_id, data in semantic_map.items():
            # semantic_map data should ideally include type/label from previous steps
            type_str = data.get('type', 'Unknown')
            label = data.get('label', sem_id)
            coords = data.get('click_coords', 'N/A')
            md.append(f"| {sem_id} | {type_str} | {label} | {coords} |")

        md.append("\n## Script Status")
        md.append("- [ ] Action Required: Provide next semantic ID to interact with.")
        
        return "\n".join(md)
Use code with caution.2. Integration: The "Reasoning Loop" This is how your pure-code script handles a failure by escalating to an LLM. pythondef agent_loop(oracle):
    # 1. Get the current state
    semantic_data = oracle.get_semantic_map()
    
    # 2. Try pure-code logic
    if "login_button" in semantic_data:
        oracle.verify_and_click("login_button")
    else:
        # 3. Escalation: Generate Markdown for LLM
        scene_md = oracle.generate_scene_description(semantic_data)
        
        # Example LLM Prompt
        prompt = f"I am stuck. Here is the current UI state:\n\n{scene_md}\n\n" \
                 "I need to log in but can't find the 'Login' button. " \
                 "Identify the correct ID from the table above or suggest a search term."
        
        # response = call_llm(prompt) 
        print(f"DEBUG: Sent to LLM:\n{scene_md}")
Use code with caution.3. Why Markdown? Token Efficiency: Markdown tables are highly compressed compared to raw JSON or HTML trees.LLM Familiarity: Models like GPT-4o and Claude 3.5 Sonnet are trained heavily on GitHub-style Markdown, making them excellent at "parsing" these tables to find the right sem_id.Debugging: It provides a human-readable log of exactly what your PaddleOCR and OpenCV stack "saw" at the moment of failure. 4. Optional: "Visual Labeling" for Multi-Modal LLMs If you use a Vision-capable LLM (GPT-4V), you can use the Pillow (PIL) library to draw the sem_id directly onto the screenshot before sending it. pythonfrom PIL import Image, ImageDraw

def create_visual_aid(snapshot_path, semantic_map):
    img = Image.open(snapshot_path)
    draw = ImageDraw.Draw(img)
    for sem_id, data in semantic_map.items():
        x, y = data['click_coords']
        # Draw a red anchor point and the ID label
        draw.rectangle([x-5, y-5, x+5, y+5], fill="red")
        draw.text((x + 10, y), sem_id, fill="red")
    img.save("llm_vision_input.png")
Use code with caution.Next Step: Do you want to implement a local logging system that saves these Markdown reports to a daily file for audit purposes?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allDo you want to implement a local logging system that saves these Markdown reports to a daily file for audit purposesLet's build a daily-rotating audit logger. This ensures your agent's reasoning remains traceable without filling your disk with a single massive file. We’ll use Python's built-in logging module with a TimedRotatingFileHandler to automatically create a new .md file every 24 hours. 1. The Audit Logger Implementation Add this to your DesktopOracle class or as a standalone utility. This version captures the timestamp, the Markdown scene, and a base64-encoded thumbnail for full visual context in a single file. pythonimport logging
from logging.handlers import TimedRotatingFileHandler
from datetime import datetime

class OracleLogger:
    def __init__(self, log_dir="logs/audit"):
        os.makedirs(log_dir, exist_ok=True)
        self.log_dir = log_dir
        
        # Configure daily rotation at midnight
        self.handler = TimedRotatingFileHandler(
            filename=f"{log_dir}/audit_log.md",
            when="midnight",
            interval=1,
            backupCount=30  # Keep 30 days of history
        )
        
        self.logger = logging.getLogger("OracleAudit")
        self.logger.setLevel(logging.INFO)
        if not self.logger.handlers:
            self.logger.addHandler(self.handler)

    def log_snapshot(self, scene_md, event_type="ON_DEMAND"):
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        entry = [
            f"---",
            f"## [{timestamp}] Event: {event_type}",
            scene_md,
            f"\n**Action Status:** Logged successfully.",
            f"\n---"
        ]
        self.logger.info("\n".join(entry))

# Integration into DesktopOracle
# self.audit = OracleLogger()
# self.audit.log_snapshot(markdown_data, event_type="LLM_ESCALATION")
Use code with caution.2. Why this Audit Structure? Traceability: If the agent clicks the wrong button at 3:00 AM, you can open audit_log.md.2023-10-27 and see exactly what the PaddleOCR result was and why the Biased Euclidean logic paired the label to that specific coordinate.LLM "Post-Mortem": You can feed these daily logs back into an LLM later to ask, "Review yesterday's failures and suggest improvements to my search radius logic."Markdown Native: By saving as .md, you can view the logs in VS Code, Obsidian, or any Markdown previewer with perfectly rendered tables. 3. Final Architecture Summary Layer ResponsibilityTech StackOS BridgePlatform detection & active window metapywinauto (Win) / python-xlib (X11)VisionSnapshots & Box detectionmss + OpenCVContextText extraction & Label mappingPaddleOCR + Custom Euclidean LogicMemoryID persistence & Jitter reductionSQLite3AuditDaily Markdown logs for LLM/Human reviewTimedRotatingFileHandler4. Direct Action To get your first snapshot, you can now run: pythonoracle = DesktopOracle()
state = oracle.get_semantic_map() # Runs OCR/Geometry/Memory
scene_report = oracle.generate_scene_description(state)
oracle.audit.log_snapshot(scene_report, "INITIAL_SCAN")
Use code with caution.Next Step: Are you ready to dive into the error-handling logic for when the OS Accessibility API (UIA/X11) crashes or hangs?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow alldive into the error-handling logic for when the OS Accessibility API (UIA/X11) crashes or hangsWhen your script relies on OS APIs like UIA or X11, you aren't just dealing with code—you're dealing with external services that can dead-lock, crash, or become unresponsive (especially during window transitions). To make your library "production-grade," you need to wrap these calls in a Watchdog Pattern. 1. The "Guardian" Decorator: Timeouts and Isolation The biggest risk is a "Zombified" API call that never returns. Since Python's Global Interpreter Lock (GIL) can make thread-level timeouts unreliable, the gold standard is using multiprocessing to run the OS query in a "disposable" process. pythonimport multiprocessing
from queue import Empty

def timeout_watchdog(timeout_seconds=5):
    """Decorator to kill OS calls that hang indefinitely."""
    def decorator(func):
        def wrapper(*args, **kwargs):
            q = multiprocessing.Queue()
            
            def worker_func(q, *args, **kwargs):
                try:
                    result = func(*args, **kwargs)
                    q.put(("SUCCESS", result))
                except Exception as e:
                    q.put(("ERROR", str(e)))

            p = multiprocessing.Process(target=worker_func, args=(q, *args), kwargs=kwargs)
            p.start()
            
            try:
                status, data = q.get(timeout=timeout_seconds)
                p.join()
                if status == "SUCCESS": return data
                raise Exception(data)
            except Empty:
                p.terminate() # KILL the hanging process
                return "API_HANG_DETECTED"
        return wrapper
    return decorator
Use code with caution.2. Error Recovery Strategies (The "Failover" Ladder) If the API fails, your DesktopOracle class should automatically "step down" to a less precise but more reliable method. Failure Mode DetectionRecovery StrategyUIA Hang (Windows)timeout_watchdog triggeredKill calc.exe or restart Explorer.exe (Extreme) or Fallback to pure OCR.X11 Connection LostXlib.error.ConnectionClosedErrorRe-initialize display.Display().Z-Order InconsistencyX11 reports window is active but it's not visiblePerform Visual Verification on the window coordinates. If 100% black/white, assume UI is frozen.3. Implementing the "Self-Healing" Meta-Query Update your metadata method to handle these "API Blackouts": python    @timeout_watchdog(timeout_seconds=3)
    def get_safe_metadata(self):
        try:
            return self.get_active_window_meta()
        except Exception as e:
            # Log the crash for the LLM audit
            self.audit.log_snapshot(f"CRITICAL: OS API Failed: {str(e)}", "API_RECOVERY")
            
            # RECOVERY: Re-init the connection
            if self.os != 'win32':
                self.d = display.Display() # Refresh X11 connection
                self.ewmh = EWMH(_display=self.d)
            
            # Return a 'minimal' state so the script can still attempt OCR
            return {"title": "RECOVERY_MODE", "rect": self.sct.monitors[0], "pid": 0}
Use code with caution.4. Windows-Specific: The "COM Thread" Nightmare pywinauto and UIA rely on COM (Component Object Model). If your library is multi-threaded, you must initialize the COM apartment in every thread, or the API will crash randomly. Fix: Call pythoncom.CoInitialize() at the start of your worker threads if using pywin32. 5. Summary for the Agent Your library now says to the agent: "I tried to get the window metadata, but the OS didn't respond.""I killed the hanging process and restarted the connection.""I am now switching to Full-Screen Vision Only mode until the API recovers." Next Step: Do you want to implement a Reboot/Restart logic where the library can attempt to close and relaunch the target application if it detects a "frozen" UI state?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allimplement a Reboot/Restart logic where the library can attempt to close and relaunch the target application if it detects a "frozen" UI stateTo implement a self-healing Reboot/Restart logic, your library must monitor the health of the target application's process and windows. If it detects a "freeze"—where the process is running but the UI is unresponsive—it should perform a "Hard Kill" and relaunch the application using its original execution path. 1. The Health Check: Detecting a Frozen UI A process is "frozen" if it exists in the OS process list but fails to respond to window messages or consumes excessive CPU without state changes. psutil documentation +1Windows: Use the pywinauto is_process_running() and is_visible() checks. Specifically, use wait_cpu_usage_lower to identify if a process is stuck in an infinite loop.Linux (X11): Query the Xlib display. If a window handle exists but the _NET_WM_STATE never updates or the window fails to redraw, the UI is effectively frozen. What is pywinauto2. Implementation: The ApplicationWatchdog Module Add this logic to your DesktopOracle class to automate the recovery cycle. pythonimport subprocess, time, psutil
from pywinauto import Application

class ApplicationWatchdog:
    def __init__(self, executable_path):
        self.path = executable_path
        self.app = None

    def is_responsive(self, window_title):
        """Checks if the window is reachable and responding."""
        try:
            # Windows UIA check
            if sys.platform == 'win32':
                app = Application(backend="uia").connect(path=self.path)
                return app.window(title=window_title).is_active()
            return True # Implement Xlib check for Linux
        except Exception:
            return False

    def hard_restart(self):
        """Kills all instances and relaunches the app."""
        # 1. Kill existing processes using psutil
        for proc in psutil.process_iter(['name']):
            if proc.info['name'] in self.path:
                proc.kill()
        
        # 2. Relaunch
        # Using Popen with DETACHED_PROCESS ensures the app outlives the script
        subprocess.Popen([self.path], creationflags=subprocess.DETACHED_PROCESS if sys.platform == 'win32' else 0)
        time.sleep(5) # Wait for UI to initialize
Use code with caution.3. Integration with the "Self-Healing" Loop Your library should wrap its main actions in a try-except block that triggers the restart if a TimeoutError occurs during a UI interaction. What is pywinauto +1Step ActionToolingMonitorCheck psutil.Process(pid).status()psutilVerifyUse wait_for_idle to see if the UI is hangingpywinautoTerminateproc.kill() for a forced exitpsutilRelaunchsubprocess.Popen with original argssubprocess4. Logging the Failure When a restart occurs, the OracleLogger should record the "Last Known State" (the final Markdown scene before the crash) and the "Restart Trigger" (e.g., REASON: UI_TIMEOUT_EXCEEDED). This allows an LLM to analyze if specific UI actions are consistently causing the app to freeze. Next Step: Should we define a "Recovery Playbook" that tells the agent exactly which sequence of clicks to perform to return to the original task after a restart?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemClose5 sitespywinauto.application module - Read the Docsstart (cmd_line, timeout=None, retry_interval=None, create_new_console=False, wait_for_idle=True, work_dir=None) Start the applica...What is pywinautopywinauto.application module - Read the Docswait_for_not – The state to wait for the window to not be in. It can be any of the following states, also you may combine the stat...What is pywinautopsutil documentation — psutil 7.2.2 documentationAbout. psutil (python system and process utilities) is a cross-platform library for retrieving information on running processes an...psutil documentationHow I Built a Self-Healing Python Script That Automatically Restarts ...Jul 3, 2025 — First, wrap the main script logic inside a while loop and use try-except to catch failures. raise RuntimeError("Oops, something br...Python in Plain Englishpywinauto possible to keep application alive if I exit python ...Nov 3, 2020 — Combining the answer in Run a program from python, and have it continue to run after the script is killed and pywinauto official d...Stack OverflowPython Pywinauto is it possible to Close a instance of Application ...Oct 14, 2020 — * 1 Answer. Sorted by: If you want to keep the application running, just do del app for app object. If you want to kill/close the ...Stack Overflow5 sitespywinauto.application module - Read the Docswait_for_not – The state to wait for the window to not be in. It can be any of the following states, also you may combine the stat...What is pywinautopsutil documentation — psutil 7.2.2 documentationAbout. psutil (python system and process utilities) is a cross-platform library for retrieving information on running processes an...psutil documentationHow I Built a Self-Healing Python Script That Automatically Restarts ...Jul 3, 2025 — First, wrap the main script logic inside a while loop and use try-except to catch failures. raise RuntimeError("Oops, something br...Python in Plain EnglishShow alldefine a "Recovery Playbook" that tells the agent exactly which sequence of clicks to perform to return to the original task after a restartTo turn a chaotic app crash into a seamless resume, your library needs a State Journal. This playbook doesn't just restart the app; it "replays" the navigation path to get the agent back to the exact screen where it left off. 1. The Recovery Journal Strategy Your library should maintain a Circular Buffer of the last 5 successful semantic actions. When a restart is triggered, the agent reads this journal to reconstruct its "Breadcrumb" path. The Journal Entry: { "action": "click", "sem_id": "settings_tab", "context": "main_view" }The Playbook: A list of "Checkpoints" (e.g., "If I see the Login screen, click 'Auto-Login'; if I see the Dashboard, click 'Project Alpha'"). 2. Prototype: Recovery Manager Class This module tracks the agent's journey and provides a "Resume Script" after a hard restart. pythonimport json

class RecoveryPlaybook:
    def __init__(self, journal_path="recovery_journal.json"):
        self.path = journal_path
        self.history = [] # Stack of semantic IDs visited

    def record_step(self, sem_id, view_name):
        """Called every time the agent successfully interacts with a region."""
        self.history.append({"id": sem_id, "view": view_name})
        if len(self.history) > 10: self.history.pop(0) # Keep it lean
        self._persist()

    def _persist(self):
        with open(self.path, "w") as f:
            json.dump(self.history, f)

    def generate_resume_instructions(self):
        """Generates a Markdown guide for the Agent/LLM after a restart."""
        if not self.history:
            return "No history found. Start from the default entry point."
        
        steps = [f"{i+1}. Navigate to **{step['view']}** via **{step['id']}**" 
                 for i, step in enumerate(self.history)]
        
        return "## Recovery Playbook\n" + "\n".join(steps) + \
               "\n\n**Goal:** Re-establish state at: " + self.history[-1]['view']
Use code with caution.3. The "State Re-Sync" Loop After the ApplicationWatchdog relaunches the app, the script doesn't just wait; it runs a "Where Am I?" check using your DesktopOracle class. Detect Entry Point: The app opens at login_screen.Consult Playbook: Playbook says the target was invoice_editor.Execute Macro: The script checks if it has a saved "Macro" for login_screen -> dashboard.Semantic Verification: At each step, it uses Visual Verification to ensure the "Project Alpha" button is actually there before clicking. 4. Integration with the Audit Log When a recovery is initiated, the OracleLogger should log: RECOVERY_STARTED: Attempting to restore session from 3 steps ago.RESTORE_PATH: [Home] -> [Account Settings] -> [API Keys] 5. Why this is "Agent-Safe" By providing a Markdown-formatted Playbook, the LLM (if consulted) doesn't have to guess how to get back. It simply reads the breadcrumbs. This prevents the agent from getting stuck in a loop of "I just restarted, now what?" Next Step: Should we create a "Macro Library" for common navigation paths (like "Login" or "Export Data") so the recovery process is faster than individual semantic clicks?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allSummarize everything we just discussed in an outline formatHere is the comprehensive architecture for your Unified Desktop State & Automation Library. This summary follows the logic from raw pixel capture to persistent semantic memory and self-healing recovery. I. Core Architecture & Language Primary Language: Python (chosen for its dominant ecosystem in OCR, ML, and OS-level bindings).Unified State Model: A cross-platform abstraction layer that converts OS-specific data into a standardized JSON/Markdown format. II. The Multi-Layer Extraction Stack Capture Layer (The "Eyes"):mss: High-speed, cross-platform screenshot capture.OpenCV: Image processing, edge detection, and template matching.Structural Layer (The "Skeleton"):Windows: pywinauto (UIA backend) for deep UI tree inspection (IDs, Roles, Hierarchy).Linux (X11): python-xlib and EWMH for window geometry and process mapping.Contextual Layer (The "Brain"):PaddleOCR: Fast, structured OCR to identify text and its bounding boxes.Biased Euclidean Resolver: A custom logic to semantically pair text labels with nearby empty input boxes or buttons based on UI design patterns (favoring Right and Below). III. Persistence & Intelligence Semantic Memory: Uses SQLite to store "Fingerprints" (Window Hash + Element IDs). This allows the script to remember where a "PIN field" is even if OCR fails in a single frame.Visual Verification: A pre-action sanity check that crops a "patch" around target coordinates and uses OpenCV to confirm the region still looks like the expected interactive element before clicking. IV. Agent & LLM Integration Markdown Scene Reports: Exports the UI state into Markdown tables. This is token-efficient for LLMs and allows for high-level reasoning when pure-code scripts get stuck.Audit Logging: Implements TimedRotatingFileHandler to save daily .md logs of all agent decisions and UI snapshots for post-mortem analysis. V. Resilience & Self-Healing Watchdog Pattern: Runs heavy OS API calls in isolated multiprocessing buckets to kill and recover from "zombie" UIA/X11 hangs.Hard Restart Logic: Uses psutil to kill frozen processes and subprocess to relaunch them using original execution paths.Recovery Playbook: A "Breadcrumb" journal that tracks the last 10 successful steps. After a crash, it provides a "Resume Script" to navigate the agent back to its previous state. VI. Primary Tooling Summary Layer Windows ToolLinux (X11) ToolUI Structurepywinautopython-xlib / ewmhPixelsmss / dxcammssOCRPaddleOCRPaddleOCRInteractionPyAutoGUIPyAutoGUIProcess ControlpsutilpsutilNext Step: Would you like to compile this into a single production-ready project structure (folder layout and requirements.txt) to begin development?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allcompile this into a single production-ready project structure (folder layout and requirements.txt) to begin developmentTo get you moving, here is the blueprint for the OracleDesktop library. This structure ensures modularity, making it easy to swap OCR engines or OS backends without breaking the core agent logic. 1. Project Folder Layout textoracle_desktop/
├── README.md
├── requirements.txt
├── .gitignore
├── run_agent.py             # Entry point for your automation script
├── data/
│   ├── ui_memory.db         # SQLite persistent fingerprint storage
│   └── templates/           # .png snippets for OpenCV icon matching
├── logs/
│   └── audit/               # Daily-rotating Markdown logs (.md)
└── src/
    ├── __init__.py
    ├── core.py              # Main DesktopOracle class (The Orchestrator)
    ├── backends/
    │   ├── __init__.py
    │   ├── windows.py       # pywinauto / UIA logic
    │   └── linux.py         # Xlib / EWMH logic
    ├── vision/
    │   ├── __init__.py
    │   ├── ocr_engine.py    # PaddleOCR wrapper
    │   └── visual_check.py  # OpenCV verification & contour logic
    ├── memory/
    │   ├── __init__.py
    │   ├── persistence.py   # SQLite & Fingerprinting
    │   └── playbook.py      # Recovery breadcrumbs/journaling
    └── utils/
        ├── __init__.py
        ├── watchdog.py      # Multiprocessing timeout decorators
        └── logger.py        # Markdown audit log rotation
Use code with caution.2. requirements.txt This includes the full stack for high-detail vision, OS-level hooks, and performance monitoring. text# --- Core Framework ---
numpy
psutil
mss                   # High-speed screen capture

# --- OS Backends ---
pywinauto             # Windows UIA (Windows only)
python-xlib           # X11 Protocol (Linux only)
ewmh                  # Extended Window Manager Hints (Linux only)
pywin32               # COM/Win32 APIs (Windows only)

# --- Vision & OCR ---
opencv-python         # Image processing & Template matching
paddleocr             # High-accuracy structured OCR
paddlepaddle          # CPU/GPU backend for PaddleOCR
pillow                # Image manipulation for audit logs

# --- Utilities ---
sqlite3               # Standard lib, but listed for clarity
Use code with caution.3. Quick-Start Execution Flow In your run_agent.py, the flow will look like this: Initialize: oracle = DesktopOracle() (Detects OS, loads OCR into RAM).Snapshot: state = oracle.get_state() (Captures pixels + OS Metadata).Analyze: labels = oracle.solve_ui(state) (Runs PaddleOCR + Biased Euclidean).Execute: oracle.verify_and_click("submit_btn") (OpenCV check + PyAutoGUI).Audit: Logs the result to logs/audit/audit_log.md. 4. Developer Tip: The "Virtual Environment" Because PaddleOCR and OpenCV have specific binary dependencies, I highly recommend using a Conda environment or a clean venv to avoid version conflicts between the paddle and opencv binaries. Next Step: Would you like the core.py boilerplate code to see how these modules are imported and initialized together?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allprovide  the core.py boilerplate code to see how these modules are imported and initialized togetherThis core.py acts as the central nervous system of your library. It handles the "Lazy Loading" of OCR (to save memory until needed), detects your OS, and manages the hand-off between metadata extraction and visual processing. src/core.py pythonimport sys
import os
import hashlib
from datetime import datetime

# Internal Module Imports
from src.utils.watchdog import timeout_watchdog
from src.utils.logger import OracleLogger
from src.vision.ocr_engine import OCRWrapper
from src.vision.visual_check import VisualVerifier
from src.memory.persistence import UIMemory

class DesktopOracle:
    def __init__(self, db_path="data/ui_memory.db", log_dir="logs/audit"):
        print(f"[Init] Starting Oracle on {sys.platform}...")
        self.os_type = sys.platform
        
        # 1. Initialize Utilities & Memory
        self.audit = OracleLogger(log_dir)
        self.memory = UIMemory(db_path)
        self.verifier = VisualVerifier()
        
        # 2. Lazy-load OCR (Heavyweight)
        self._ocr = None 
        
        # 3. Detect & Setup OS Backend
        self._setup_backend()

    def _setup_backend(self):
        if self.os_type == 'win32':
            from src.backends.windows import WindowsBackend
            self.backend = WindowsBackend()
        else:
            from src.backends.linux import LinuxBackend
            self.backend = LinuxBackend()

    @property
    def ocr(self):
        """Lazy loader for PaddleOCR to save RAM during startup."""
        if self._ocr is None:
            self._ocr = OCRWrapper()
        return self._ocr

    @timeout_watchdog(timeout_seconds=5)
    def get_full_state(self):
        """
        The Orchestrator: Combines OS metadata with visual intelligence.
        """
        # A. Get OS structural info (Windows/X11)
        meta = self.backend.get_active_window_meta()
        win_hash = self.memory.generate_hash(meta)
        
        # B. Check if we have this UI 'remembered'
        cached_map = self.memory.recall_all(win_hash)
        
        # C. Capture pixels for the active window
        screenshot = self.backend.capture_window(meta['rect'])
        
        # D. If memory fails or refresh is forced, run OCR + Euclidean Resolver
        if not cached_map:
            print("[OCR] Cache miss. Analyzing UI layout...")
            semantic_map = self.ocr.analyze_ui(screenshot, meta['rect'])
            self.memory.persist_map(win_hash, semantic_map)
        else:
            semantic_map = cached_map

        return {
            "meta": meta,
            "semantic_map": semantic_map,
            "screenshot": screenshot
        }

    def execute_action(self, sem_id):
        """Final verification and click."""
        state = self.get_full_state()
        if sem_id not in state['semantic_map']:
            self.audit.log_snapshot("Action Failed: ID not found", "ERROR")
            return False

        coords = state['semantic_map'][sem_id]
        
        # Visual Sanity Check before clicking
        is_valid, _ = self.verifier.verify_input(coords, state['screenshot'])
        
        if is_valid:
            self.backend.click(coords)
            self.audit.log_snapshot(f"Clicked {sem_id}", "ACTION")
            return True
        else:
            self.audit.log_snapshot(f"Safety Halt: {sem_id} looks different!", "WARNING")
            return False

if __name__ == "__main__":
    # Quick Test
    oracle = DesktopOracle()
    full_data = oracle.get_full_state()
    print(f"Ready to interact with: {list(full_data['semantic_map'].keys())}")
Use code with caution.Why this Boilerplate is Production-Ready: Fault Isolation: By using the @timeout_watchdog, any hang in self.backend.get_active_window_meta() (common in Windows UIA) won't freeze your entire script.Stateful Memory: It checks the SQLite-based memory before firing up the PaddleOCR engine, which drops "on-demand" processing time from ~2 seconds down to ~100ms.Encapsulation: The main script doesn't care if it's on Windows or Linux; it just asks for get_full_state(). Next Step: Would you like to see the vision/ocr_engine.py wrapper to see how we specifically extract the bounding boxes from PaddleOCR?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allShow the vision/ocr_engine.py wrapper to see how we specifically extract the bounding boxes from PaddleOCRTo extract precise coordinates from PaddleOCR, your wrapper needs to parse its nested list structure: [ [[[x,y], [x,y], [x,y], [x,y]], ("text", confidence)], ... ]. This wrapper is designed to return "Agent-Ready" dictionaries, where coordinates are already converted from local window crops back to absolute screen coordinates. src/vision/ocr_engine.py pythonimport numpy as np
from paddleocr import PaddleOCR

class OCRWrapper:
    def __init__(self, lang='en'):
        # use_angle_cls=True handles rotated text (common in custom UI labels)
        # show_log=False keeps your terminal clean for the agent's output
        self.engine = PaddleOCR(use_angle_cls=True, lang=lang, show_log=False)

    def analyze_ui(self, screenshot_np, window_rect):
        """
        Processes a raw numpy image and returns semantic anchors.
        window_rect: {'x', 'y', 'w', 'h'} used to offset local coords to screen coords.
        """
        # PaddleOCR expects a BGR numpy array (OpenCV format)
        results = self.engine.ocr(screenshot_np, cls=True)
        
        # Handle cases where no text is detected
        if not results or results[0] is None:
            return {}

        semantic_data = {}
        
        # Results are wrapped in an extra list for multi-page/batch support
        for line in results[0]:
            coords = line[0]    # The 4 bounding box points
            text_info = line[1] # ("Text content", Confidence)
            
            text_str = text_info[0].strip()
            confidence = text_info[1]

            if confidence < 0.8: # Filter out low-confidence "noise"
                continue

            # 1. Calculate the center of the bounding box
            # coords is [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]
            box = np.array(coords).astype(np.int32)
            center_x = int(np.mean(box[:, 0]))
            center_y = int(np.mean(box[:, 1]))

            # 2. Offset to Absolute Screen Coordinates
            # This is vital so the agent can click(x, y) directly
            abs_x = center_x + window_rect['x']
            abs_y = center_y + window_rect['y']

            # 3. Create a unique ID for the script to use
            # "Login Button" -> "login_button"
            clean_id = text_str.lower().replace(" ", "_").replace(":", "")
            
            semantic_data[clean_id] = {
                "text": text_str,
                "click_coords": (abs_x, abs_y),
                "confidence": confidence,
                "local_box": coords # Useful for visual debugging
            }

        return semantic_data

    def find_text_region(self, screenshot_np, target_text):
        """Helper to find a specific string quickly in a new snapshot."""
        data = self.analyze_ui(screenshot_np, {'x': 0, 'y': 0})
        for key, val in data.items():
            if target_text.lower() in val['text'].lower():
                return val['click_coords']
        return None
Use code with caution.Key Logic for Your Agent: Coordinate Transformation: By adding window_rect['x'] and y, your DesktopOracle bypasses the need for the agent to know about "crops." It just sees the whole desktop as one grid.Semantic Sanitization: Converting "Email Address:" into the key email_address makes your pure-code scripts much more readable: if "email_address" in ui_state: ....Confidence Thresholding: Setting a baseline (e.g., 0.8) prevents your script from attempting to "click" on desktop wallpaper artifacts or rendering glitches. Developer Tip: PaddleOCR uses PaddlePaddle as its backend. If you have an NVIDIA GPU, installing paddlepaddle-gpu will make this extraction 5-10x faster than the CPU version, which is crucial for a responsive "on-demand" agent. Next Step: Should we move to the backends/windows.py or backends/linux.py to implement the actual capture_window and get_active_window_meta methods?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemClose1 site[2109.03144] PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR SystemSep 7, 2021 — It ( PP-OCRv2 ) is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above ment...arXiv1 site[2109.03144] PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR SystemSep 7, 2021 — It ( PP-OCRv2 ) is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above ment...arXivShow allmove to the backends/windows.pyTo implement the Windows Backend, we use pywinauto for structural metadata and mss for pixel capture. This module is responsible for the heavy lifting of "Structural Querying" and coordinate translation. src/backends/windows.py pythonimport mss
import numpy as np
from pywinauto import Desktop
import pyautogui

class WindowsBackend:
    def __init__(self):
        # Initialize mss for high-speed screen grabbing
        self.sct = mss.mss()
        # Ensure PyAutoGUI is snappy
        pyautogui.PAUSE = 0.1

    def get_active_window_meta(self):
        """
        Extracts structural 'Ground Truth' from Windows UIA.
        Returns coordinates, title, and process ID.
        """
        try:
            # Connect to the currently focused window via UIA backend
            top_win = Desktop(backend="uia").active()
            rect = top_win.rectangle()
            
            return {
                "title": top_win.window_text(),
                "pid": top_win.process_id(),
                "rect": {
                    "x": rect.left,
                    "y": rect.top,
                    "w": rect.width(),
                    "h": rect.height()
                }
            }
        except Exception as e:
            # Fallback to primary monitor if UIA is hanging or busy
            monitor = self.sct.monitors[1]
            return {
                "title": "DESKTOP_FALLBACK",
                "pid": 0,
                "rect": {"x": monitor['left'], "y": monitor['top'], "w": monitor['width'], "h": monitor['height']}
            }

    def capture_window(self, rect):
        """
        Captures raw pixels for the specified region.
        """
        # mss expects: {'top': y, 'left': x, 'width': w, 'height': h}
        monitor = {
            "top": rect['y'],
            "left": rect['x'],
            "width": rect['w'],
            "height": rect['h']
        }
        sct_img = self.sct.grab(monitor)
        
        # Convert to BGR Numpy array (Standard for OpenCV/PaddleOCR)
        return np.array(sct_img)[:, :, :3]

    def click(self, coords):
        """Performs a hardware-level click at absolute coordinates."""
        x, y = coords
        pyautogui.click(x, y)

    def type_text(self, text, coords=None):
        """Clicks and types. Essential for 'Input Field' semantic actions."""
        if coords:
            self.click(coords)
        pyautogui.write(text, interval=0.05)
Use code with caution.Why this is the "Gold Standard" for Windows: UIA Backend: Unlike older libraries that use Win32 (which misses modern apps), the uia backend sees everything—from legacy Control Panel apps to modern Windows Terminal and Electron apps.mss vs. PIL: We use mss because it is significantly faster than ImageGrab. It captures only the active_window region, reducing the memory footprint before the pixels hit your OCR engine.Coordinate Consistency: The top_win.rectangle() returns absolute screen coordinates, which perfectly match the inputs required by PyAutoGUI. Pro-Tip for your Agent: When your agent needs to fill a form, use get_active_window_meta to verify the window title before calling type_text. If the user accidentally clicks away, the backend can detect the title change and pause the script, preventing it from typing sensitive data into the wrong window. Next Step: Ready for the backends/linux.py (X11) version to see how we handle the translate_coords math?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allI am Ready for the backends/linux.py (X11) version to see how we handle the translate_coords math.To implement the Linux (X11) Backend, we use python-xlib. The challenge with X11 is that child windows often report coordinates relative to their parent window, not the screen. We use translate_coords to perform the "coordinate lift" to the root window (the desktop) so your clicks land correctly. src/backends/linux.py pythonimport mss
import numpy as np
import pyautogui
from Xlib import display, X
from ewmh import EWMH

class LinuxBackend:
    def __init__(self):
        # Initialize X11 connection and EWMH helper
        self.d = display.Display()
        self.root = self.d.screen().root
        self.ewmh = EWMH(_display=self.d)
        self.sct = mss.mss()
        
        # PyAutoGUI configuration for Linux X11
        pyautogui.PAUSE = 0.1

    def get_active_window_meta(self):
        """
        Uses EWMH and Xlib to find the focused window and its 
        absolute desktop coordinates.
        """
        try:
            # 1. Get the focused window handle (XID)
            active_win = self.ewmh.getActiveWindow()
            if not active_win:
                return self._get_fallback_meta()

            # 2. Get Window Title and Geometry
            name = self.ewmh.getWMName(active_win)
            if isinstance(name, bytes): name = name.decode('utf-8', 'ignore')
            
            geom = active_win.get_geometry()
            
            # 3. The translate_coords Math:
            # This converts (0,0) of the window into its (x,y) on the root screen.
            # This accounts for window decorations (borders/title bars) added by the WM.
            t_coords = active_win.translate_coords(self.root, 0, 0)
            
            return {
                "title": name if name else "Unknown",
                "pid": self.ewmh.getWmPid(active_win),
                "rect": {
                    "x": t_coords.x,
                    "y": t_coords.y,
                    "w": geom.width,
                    "h": geom.height
                }
            }
        except Exception:
            return self._get_fallback_meta()

    def _get_fallback_meta(self):
        monitor = self.sct.monitors[0]
        return {
            "title": "DESKTOP_FALLBACK",
            "pid": 0,
            "rect": {"x": monitor['left'], "y": monitor['top'], "w": monitor['width'], "h": monitor['height']}
        }

    def capture_window(self, rect):
        """Captures window pixels for OCR/Vision."""
        monitor = {
            "top": rect['y'],
            "left": rect['x'],
            "width": rect['w'],
            "height": rect['h']
        }
        # Grab from X server and strip the alpha channel
        sct_img = self.sct.grab(monitor)
        return np.array(sct_img)[:, :, :3]

    def click(self, coords):
        """X11 Hardware click via PyAutoGUI."""
        x, y = coords
        pyautogui.click(x, y)

    def type_text(self, text, coords=None):
        if coords:
            self.click(coords)
        pyautogui.write(text, interval=0.05)
Use code with caution.Why this works for an X11 Agent: translate_coords(self.root, 0, 0): This is the magic line. In X11, a window's internal x, y might be 0, 0 because it thinks it’s at the start of its own container. This function asks the X Server: "Where is this window's [0,0] relative to the actual screen?"EWMH Support: By using EWMH (Extended Window Manager Hints), we can extract the _NET_WM_PID, allowing your library to map a window back to a Linux process (e.g., gnome-terminal or firefox).Stability: Since Wayland often blocks screen capture and global coordinates for security, this python-xlib approach is the most reliable "god-mode" view for Linux automation until a standardized Wayland portal is widely adopted. Next Step: Would you like to implement the memory/persistence.py module to see how we store these window signatures and semantic maps in SQLite?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allimplement the memory/persistence.pyTo ensure your agent is fast and resilient, the persistence.py module uses SQLite to bridge the gap between raw vision and stable memory. Instead of re-running OCR on every frame, it creates a unique structural fingerprint of the window to recall previously discovered semantic IDs. src/memory/persistence.py pythonimport sqlite3
import json
import hashlib

class UIMemory:
    def __init__(self, db_path="data/ui_memory.db"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """Initializes the SQLite schema for fast ID recall."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS ui_map (
                    fingerprint TEXT,
                    sem_id TEXT,
                    rel_x INTEGER,
                    rel_y INTEGER,
                    last_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    PRIMARY KEY (fingerprint, sem_id)
                )
            """)
            conn.commit()

    def generate_hash(self, meta):
        """
        Creates a 'fingerprint' based on window title and dimensions.
        If the app resizes or the title changes, we treat it as a new state.
        """
        sig = f"{meta['title']}|{meta['rect']['w']}x{meta['rect']['h']}"
        return hashlib.md5(sig.encode()).hexdigest()

    def persist_map(self, fingerprint, semantic_map):
        """
        Stores discovered semantic IDs and their RELATIVE coordinates.
        Relative coords ensure that if the window moves, the memory is still valid.
        """
        with sqlite3.connect(self.db_path) as conn:
            for sem_id, data in semantic_map.items():
                # We only care about the relative offset from window (0,0)
                # Assumes data['click_coords'] are absolute screen coords
                # rel_x = abs_x - win_x
                coords = data['click_coords']
                # These are calculated in the Orchestrator/Core before passing here
                # or we can pass the window_rect to this function.
                
                # For this implementation, we assume semantic_map holds relative data
                # to keep the memory module 'pure'.
                conn.execute("""
                    INSERT OR REPLACE INTO ui_map (fingerprint, sem_id, rel_x, rel_y)
                    VALUES (?, ?, ?, ?)
                """, (fingerprint, sem_id, data['rel_x'], data['rel_y']))
            conn.commit()

    def recall_all(self, fingerprint):
        """
        Returns all known semantic elements for a specific window fingerprint.
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                "SELECT sem_id, rel_x, rel_y FROM ui_map WHERE fingerprint = ?", 
                (fingerprint,)
            )
            rows = cursor.fetchall()
            
        if not rows:
            return None
            
        return {row[0]: {"rel_x": row[1], "rel_y": row[2]} for row in rows}

    def forget(self, fingerprint):
        """Clear memory if a UI update makes the old map invalid."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("DELETE FROM ui_map WHERE fingerprint = ?", (fingerprint,))
            conn.commit()
Use code with caution.Why this Memory Strategy is "Agent-Grade": Relative Positioning: By storing rel_x and rel_y (the distance from the window's top-left corner), your script can still find the "Submit" button even if the user drags the window across the screen.Fingerprint Stability: By hashing the width and height along with the title, we detect "UI Mutation." If the user expands a sidebar or resizes the window, the fingerprint changes, triggering a fresh OCR pass to ensure accuracy.Atomic Persistence: Using SQLite ensures that if your script crashes mid-extraction, the database remains uncorrupted.Token Efficiency: Since the DesktopOracle pulls from this local DB, it avoids sending redundant visual data to an LLM, only escalating when the recall_all returns None. Implementation Detail: In your core.py, when you call persist_map, you simply subtract the window's rect['x'] and rect['y'] from the absolute coordinates provided by the PaddleOCR engine. Next Step: Should we finalize the utils/watchdog.py to ensure your OS calls never hang the entire system?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allfinalize the utils/watchdog.py to ensure your OS calls never hang the entire systemTo prevent your script from freezing when Windows UIA or X11 deadlocks, we use a Multiprocessing Watchdog. This is superior to threading because Python cannot "force-kill" a thread that is stuck in a C-extension or OS kernel call, but it can terminate a sub-process. src/utils/watchdog.py pythonimport multiprocessing
import functools
from queue import Empty

class WatchdogTimeoutError(Exception):
    """Custom exception for when an OS API call exceeds its time limit."""
    pass

def timeout_watchdog(seconds=5):
    """
    Decorator that runs a function in a separate process to enforce a strict timeout.
    Usage:
        @timeout_watchdog(seconds=3)
        def get_ui_tree():
            ...
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # Create a Queue to receive the result from the sub-process
            queue = multiprocessing.Queue()

            # Define the target function for the process
            def worker(q, *worker_args, **worker_kwargs):
                try:
                    result = func(*worker_args, **worker_kwargs)
                    q.put((True, result))
                except Exception as e:
                    q.put((False, e))

            # Start the isolated process
            process = multiprocessing.Process(
                target=worker, 
                args=(queue, *args), 
                kwargs=kwargs,
                daemon=True # Ensures the process dies if the main script exits
            )
            process.start()

            try:
                # Wait for the result or the timeout
                success, result = queue.get(timeout=seconds)
                process.join()
                
                if success:
                    return result
                else:
                    raise result # Re-raise the exception caught in the worker
            
            except Empty:
                process.terminate() # KILL the hanging OS call
                process.join()      # Clean up zombie process
                raise WatchdogTimeoutError(
                    f"OS API call '{func.__name__}' timed out after {seconds}s. "
                    "Process was terminated to prevent system hang."
                )
        return wrapper
    return decorator
Use code with caution.Why this is critical for your library: The "Z-Order" Trap: On Windows, if a window is being destroyed exactly when you query its UIA Tree, the call can hang indefinitely. This decorator ensures your script continues.X11 Socket Timeouts: If the X Server connection stutters, python-xlib calls can block. The watchdog acts as a circuit breaker.Clean Recovery: By using process.terminate(), you release the lock on the GIL and OS resources, allowing the DesktopOracle to try a "Fallback" (like full-screen OCR) immediately. Integration in core.py You simply wrap your backend calls like this: python@timeout_watchdog(seconds=2)
def get_safe_meta(self):
    return self.backend.get_active_window_meta()
Use code with caution.Next Step: You have the layout, backends, vision engine, memory, and watchdog. Should we create the run_agent.py example to show how a script uses this entire stack to perform a "Login" task?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allcreate the run_agent.py example to show how a script uses this entire stack to perform a "Login" taskThis run_agent.py demonstrates the "Orchestration" of your stack. It shows how the script uses Semantic IDs (from memory or OCR) to interact with the UI while relying on the Watchdog and Audit Logger to handle potential failures. run_agent.py pythonimport time
from src.core import DesktopOracle

def main():
    # 1. Initialize the Oracle (Sets up Backends, Memory, and OCR)
    oracle = DesktopOracle(db_path="data/ui_memory.db", log_dir="logs/audit")
    
    print("--- Starting Login Agent ---")
    
    try:
        # 2. Extract current state (Metadata + Semantic Map)
        # This will automatically use SQL Memory or fire up PaddleOCR if needed.
        state = oracle.get_full_state()
        sem_map = state['semantic_map']
        
        # 3. Decision Logic: Check if we are on the Login Screen
        if "username" not in sem_map:
            print("[Agent] Login fields not detected. Escalating to LLM-ready Scene Report...")
            report = oracle.generate_scene_description(sem_map)
            oracle.audit.log_snapshot(report, "IDENTIFICATION_FAILURE")
            return

        # 4. Interaction: Fill out the form
        # We use verify_and_click to ensure the 'box' hasn't disappeared
        print("[Agent] Entering credentials...")
        
        if oracle.execute_action("username"):
            oracle.backend.type_text("admin_user")
            
        if oracle.execute_action("password"):
            oracle.backend.type_text("secure_password123")
            
        # 5. Execution: Click Submit
        print("[Agent] Submitting...")
        if oracle.execute_action("login") or oracle.execute_action("sign_in"):
            print("[Agent] Login sequence complete.")
            oracle.audit.log_snapshot("Login action performed successfully.", "SUCCESS")
        else:
            print("[Agent] Could not find submit button visually.")

    except Exception as e:
        # Handle Watchdog timeouts or OS crashes
        print(f"[Critical] Agent Failure: {e}")
        # Here you would trigger the hard_restart() logic discussed earlier
        # oracle.watchdog.hard_restart()

if __name__ == "__main__":
    main()
Use code with caution.How this script "thinks": Semantic Targeting: It doesn't look for (500, 400). It looks for username. If the window moved, the Memory Module translates the relative coordinates back to the new screen position automatically.Safety First: execute_action triggers the OpenCV Visual Check to ensure that even if the coordinates match, the pixels at that spot still look like an input box.Traceable Failures: If it fails to find the password field, the generate_scene_description creates a Markdown table. You can literally copy-paste that table into ChatGPT or Claude and ask: "Which of these IDs is most likely the password field?"Resilience: If the Windows UIA backend hangs while getting the window title, the @timeout_watchdog in core.py will kill the process, and this script will catch the exception rather than freezing your computer. Recommended Development Workflow Run the script once to "Train" it (Initial PaddleOCR pass).Check data/ui_memory.db to see the stored fingerprints.Check logs/audit/ to see the Markdown representation of your desktop. Final Pro-Tip: For the best experience, run the target application in Windowed Mode (not full screen) during initial testing so you can monitor the terminal output alongside the UI actions. What is the first application you plan to test this against?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow alllocal mcp server for lm studio, it uses small llms to execute strategies that Large LLMs produceTo implement a local Model Context Protocol (MCP) server for LM Studio, you can use the official Python SDK to create a "Strategy Orchestrator". This setup allows a large LLM (like Claude or GPT-4) to act as a "Planner" and offload tactical execution to small, snappy local models running in LM Studio. GitHub +41. Prerequisites & Environment LM Studio: Running locally with a small, capable model loaded (e.g., Qwen-2.5-Coder-7B or Llama-3.2-3B).Start Local Server: Go to the Local Server tab in LM Studio and click Start Server (default: http://localhost:1234).Python Environment:bashuv init mcp-strategy-executor
cd mcp-strategy-executor
uv add "mcp[cli]" httpx openai
Use code with caution. LM Studio +42. Implementation: Strategy Executor Server Create a file named server.py. This MCP server exposes a tool that takes a high-level instruction from a large LLM and sends it to the local LM Studio model for execution. LobeHub +4pythonfrom mcp.server.fastmcp import FastMCP
import openai

# Connect to your local LM Studio instance
client = openai.OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")
mcp = FastMCP("Local-Strategy-Executor")

@mcp.tool()
async def execute_subtask(instruction: str) -> str:
    """
    Executes a tactical subtask using the local small LLM.
    Useful for code generation, data formatting, or quick analysis.
    """
    response = client.chat.completions.create(
        model="local-model", # LM Studio usually ignores this and uses the loaded model
        messages=[{"role": "user", "content": instruction}],
        temperature=0.1
    )
    return response.choices[0].message.content

if __name__ == "__main__":
    mcp.run()
Use code with caution.3. Registering the Server in LM Studio LM Studio (v0.3.17+) supports MCP as a host. LM Studio +1Navigate to the Program tab in the right-hand sidebar.Click Install > Edit mcp.json.Add your local server configuration using the Cursor-style notation:json{
  "mcpServers": {
    "strategy-executor": {
      "command": "uv",
      "args": ["run", "/path/to/your/mcp-strategy-executor/server.py"]
    }
  }
}
Use code with caution. LM Studio +34. Strategy Workflow Planner Phase: Use a Large LLM (Host) to break down a complex goal into granular "strategies."Execution Phase: The Planner calls the execute_subtask tool.Local Action: Your MCP server forwards the tactical request to LM Studio, keeping the data private and saving token costs on the large model. www.speakeasy.com +35. Advanced: Specialized Small Models You can optimize this further by running task-specific small models in LM Studio: Reddit +2Qwen2.5-Coder: For tactical code refactoring via MCP.Gemma-2-2B: For lightning-fast text classification or summarization subtasks. LM Studio +2How complex is the strategy you are looking to execute locally—is it mostly coding, data processing, or multi-step reasoning?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemClose24 sitesMCP in LM StudioJun 25, 2025 — Add MCP servers to LM Studio. Model Context Protocol (MCP) is a set of interfaces for providing LLMs access to tools and other res...LM StudioLM Studio as a Local LLM API ServerTo run the server, go to the Developer tab in LM Studio, and toggle the "Start server" switch to start the API server.LM StudioLocal LLM MCP Server - LobeHubNov 27, 2025 — MCP server for offloading tasks to local LLMs via LM Studio SDK with file support. Requires LM Studio installed and running locall...LobeHubShow all24 siteshoutini-ai/lm: LM Studio MCP with Expert Prompts and Custom ...Sep 23, 2025 — Houtini LM - LM Studio MCP Server with Expert Prompt Library and Custom Prompting. Your unlimited AI companion: This MCP server co...GitHubMCP in LM StudioJun 25, 2025 — Install new servers: mcp. ... Switch to the "Program" tab in the right hand sidebar. Click Install > Edit mcp. json . This will op...LM StudioMCP in LM StudioJun 25, 2025 — Add MCP servers to LM Studio. Model Context Protocol (MCP) is a set of interfaces for providing LLMs access to tools and other res...LM StudioLM Studio as a Local LLM API Server | LM Studio DocsCopy as MarkdownCopy. Run an LLM API server on localhost with LM Studio. You can serve local LLMs from LM Studio's Developer tab, ...LM StudioLocal LLM MCP Server - LobeHubNov 27, 2025 — MCP server for offloading tasks to local LLMs via LM Studio SDK with file support. Requires LM Studio installed and running locall...LobeHubLM Studio as a Local LLM API ServerTo run the server, go to the Developer tab in LM Studio, and toggle the "Start server" switch to start the API server.LM StudioLM Studio MCP Sidekick | MCP Servers - LobeHubNov 27, 2025 — 🚀 Quick Start * 1. Clone and Install. Shell. git clone https://github.com/SamuraiBuddha/lm-studio-mcp-sidekick.git cd lm-studio-m...LobeHubLocal LLM MCP Server - LobeHubNov 27, 2025 — MCP server for offloading tasks to local LLMs via LM Studio SDK with file support. Requires LM Studio installed and running locall...LobeHubmodelcontextprotocol/python-sdk - GitHubThe Model Context Protocol (MCP) lets you build servers that expose data and functionality to LLM applications in a secure, standa...GitHubUse MCP Servers | LM Studio DocsInstall new servers: mcp. ... Switch to the "Program" tab in the right hand sidebar. Click Install > Edit mcp. json . This will op...LM StudioMCP Servers the big boost to Local LLMs? : r/LocalLLM - RedditOct 17, 2025 — For example i have a tool calling model, a memory model, both 4B qwen finetunes. They can run together (2.5gb at q4 and working am...RedditLM Studio - Local AI on your computerRun local LLMs like gpt-oss , Qwen3 , Gemma3 , DeepSeek and many more on your computer, privately and for free. Deploy on servers,LM StudioWhy less is more for MCP - SpeakeasySep 16, 2025 — What curated MCP servers look like. The idea of curating MCP servers is to create smaller, focused servers that contain only the t...www.speakeasy.comRunning LLMs Locally: LM Studio - Paradigma Digital - MediumJan 27, 2026 — SERVER START. Command used to start the local LM Studio server, allowing you to specify the port and enable CORS support. lms serv...MediumLM Studio is free for use at workJul 8, 2025 — Free to use at Home, and now also at Work LM Studio has always been free for personal use. This is due to our fundamental belief t...LM StudioBuild an MCP server - Model Context ProtocolSet up your environment. First, let's install uv and set up our Python project and environment: macOS/Linux. Windows. curl -LsSf h...Model Context ProtocolLMStudio-MCP MCP Server: An In-Depth Guide for AI EngineersSep 27, 2025 — Before you can connect any servers, you need to have your local LLM environment ready. * Download and Install LM Studio: If you ha...Skywork.aiBuild MCP ServerCreating an MCP Server in Python ( Python code ) : Step-by-Step Guide ¶ To create an MCP ( Model Context Protocol (MCP ) server in...AI Engineering AcademyOn Continuous Delivery in Machine LearningAug 2, 2021 — Once it ( the model ) has loaded, the prediction tasks in the application can start. This is primarily suitable for the cases wher...Towards Data Science[tl;dr sec] #292- HTTP/1.1 must die, AI + SAST, Google's Insider Threat Detection ToolAug 14, 2025 — Red Team Outflank's Kyle Avery describes how a small, specially trained LLM ( Dante-7B, based on Qwen2. 5-Coder-7B) can outperform...tl;dr secHow to Master the Model Context Protocol: Complete Tutorial for AI DevelopersAug 6, 2025 — Step 2: Define Your Server's “Menu” in Python Create a file named server.py . This is where the magic happens. We'll define a reso...Towards AIinfinitimeless/Claude-LMStudio-Bridge_V2: A bridge MCP server that allows Claude to communicate with locally running LLM models via LM StudioJun 8, 2025 — A simple Model Control Protocol (MCP) server that allows Claude to communicate with locally running LLM models via LM Studio.GitHubWrite an MCP-Server in Xojo - AI - Xojo Programming ForumJul 11, 2025 — I'm pleased to announce MCPKit, an open source module for Xojo for creating MCP servers with Xojo: GitHub repo. What is a MCP serv...Xojo Programming ForumSemantic Kernel 101: Part 2Dec 26, 2025 — Finally, I'll look at planners, where I ask the LLM to break down a large, complex problem into smaller steps and choose from both...CODE MagazineMultiple small specialized models instead of one to fit them allOct 27, 2024 — Multiple small specialized models instead of one to fit them all 
   
   - A frontend interface. 
   - Individual services labeled...MediumHow to Run a Local LLM for Enterprise UseJun 26, 2025 — Typically, you need a focused, local LLM fine-tuned for a specific task, and smaller models pull their weight just fine for most o...IntelliasHow to Run a Local LLM: Complete Guide to Setup & Best Models (2025)May 15, 2025 — LM Studio LM Studio is a platform designed to make it easy to run and experiment with LLMs locally. It offers a range of tools for...n8n Blogmulti-step reasoning: General purpose unsupervised desktop use. Monitors the desktop notifications and reacts when an email arrives or when a new news story is indicated, ...To implement multi-step reasoning for general-purpose desktop use using an MCP (Model Context Protocol) server with LM Studio, you can adopt a Hierarchical Strategy. In this setup, a "Large LLM" (the Planner) designs high-level strategies, while "Small Local LLMs" (the Executors in LM Studio) monitor raw data and perform tactical reactions. 1. The Strategy: Hierarchical Multi-Step Reasoning Instead of a single model doing everything, break the problem into two reasoning tiers: Tier 1 (The Planner): A Large LLM (e.g., Claude 3.5, GPT-4o) that understands complex context. It provides the "Logic Rules" (e.g., "If you see an email about 'Invoice #402', extract the amount and log it to Excel").Tier 2 (The Executor): A Small Local LLM (e.g., Qwen-2.5-Coder-7B or Llama-3.2-3B) running in LM Studio. It continuously monitors the desktop for specific triggers via an MCP server and executes the specific sub-tasks defined by Tier 1. 2. Implementation: The Monitoring MCP Server You need an MCP server that provides tools for observing (notifications, screenshots) and acting (mouse/keyboard). Below is a conceptual structure for a Python-based MCP server to handle your news and email scenarios. Tool: Desktop Monitor & Notification Watcher Use a library like plyer or native OS hooks to fetch notifications, and mss for visual checks. pythonfrom mcp.server.fastmcp import FastMCP
import mss
import pyautogui
# Add notification library based on OS (e.g., plyer or win10toast)

mcp = FastMCP("Desktop-Intelligence")

@mcp.tool()
async def check_recent_notifications() -> str:
    """
    Returns a list of recent desktop notifications (emails, news alerts).
    Used by the LLM to 'see' what just happened.
    """
    # Logic to fetch OS notification history
    return "New Email: 'Invoice #402' from Billing | News: 'Tech Stocks Rise'"

@mcp.tool()
async def analyze_screen_region(region_name: str) -> str:
    """
    Takes a screenshot of a specific app (e.g., Outlook) and 
    returns a text summary using the local model.
    """
    # Use the DesktopOracle logic from previous steps
    return "The email body contains a PDF attachment and a request for payment."

@mcp.tool()
async def execute_desktop_macro(action_sequence: str):
    """
    Performs a sequence of clicks/typing as instructed by the strategist.
    """
    # Parse and execute (e.g., "click search, type 'Invoice', enter")
    pass
Use code with caution.3. Workflow for News & Email Reactions For unsupervised use, your agent follows a loop of reasoning: Monitor (The Observer): Every 60 seconds, the Executor calls check_recent_notifications.Evaluate (Small Model): The local model (in LM Studio) receives the notification text. It checks its "Playbook" (provided by the Large LLM).Reasoning: "I see a notification for 'News: Tech Stocks'. My playbook says: 'If news contains "Stocks", open the Browser and check Yahoo Finance'."Refine (Large Model - Optional): If the sub-task is complex (e.g., drafting a sensitive reply to the invoice email), the Executor sends the context to the Large LLM to generate the draft.Execute (The Actor): The Executor uses the execute_desktop_macro tool to open the browser, navigate, or reply. 4. Specialized Tooling Recommendations For News/Email Triggers: Instead of just notifications, consider a Headless MCP Browser (like Playwright) to monitor specific RSS feeds or Web-mail tabs in the background.For OS Automation: The MCP Desktop Automation server provides ready-made tools for mouse control and screenshots that you can plug directly into LM Studio's mcp.json.For Feedback Loops: Use a Vision-Language Model (VLM) in LM Studio (like Moondream2) as a "Visual Watchdog" to confirm that an email window actually opened before the agent starts typing. GlamaWould you like a sample "Strategy Playbook" that describes how a Large LLM should instruct a small local model to handle these multi-step desktop tasks?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemClose5 sitesMCP Desktop Automation by tanob - GlamaOS Automation. by tanob. JavaScript. Which integrations are available for this server? Serves as the runtime environment for the M...GlamaWhat is LLM monitoring? - Dynatrace10 steps for implementing effective LLM monitoring * 1. Define objectives and metrics. Determine the goals of monitoring, such as ...DynatraceThousandEyes MCP Server: AI-Driven Network Insights and ...Jan 13, 2026 — hey snackers this is Kareem Scander Hey everyone Matt Napoli here Welcome to episode 199 of Snack Minute Oo 199 Um this week uh we...YouTube·Learn with Cisco12mRuntime notifications - Power Automate - Microsoft LearnFeb 23, 2023 — In this article ... Through the Monitoring/Notifications option of the console settings, you can choose between the integrated Win...Microsoft LearnLM Studio 0.3.17 Adds Model Context Protocol (MCP) Support ...Jul 5, 2025 — Tools can be whitelisted for future calls, and settings can be managed globally via the “Tools & Integrations” menu. An example us...infoq.com5 sitesMCP Desktop Automation by tanob - GlamaOS Automation. by tanob. JavaScript. Which integrations are available for this server? Serves as the runtime environment for the M...GlamaWhat is LLM monitoring? - Dynatrace10 steps for implementing effective LLM monitoring * 1. Define objectives and metrics. Determine the goals of monitoring, such as ...DynatraceThousandEyes MCP Server: AI-Driven Network Insights and ...Jan 13, 2026 — hey snackers this is Kareem Scander Hey everyone Matt Napoli here Welcome to episode 199 of Snack Minute Oo 199 Um this week uh we...YouTube·Learn with Cisco12mShow allProvide a sample "Strategy Playbook" that describes how a Large LLM should instruct a small local model to handle these multi-step desktop tasksTo bridge the gap between a Cloud LLM (The Strategist) and your Local LM Studio Model (The Tactician), you need a "Structured Protocol."The Strategist shouldn't just say "check my email." It must provide a State-Machine Playbook that tells the local model exactly what to look for, what constitutes a "trigger," and which local tool to call.The "Desktop Sentinel" PlaybookObjective: Unsupervised Monitoring & Reaction (Email & News)Tactician Model (Local): Llama-3.2-3B or Qwen-2.5-Coder-7B1. The Context (System Prompt for Local Model)The Strategist sends this "Global Mission" to the Local MCP Server: Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allcontinueTo implement this, you need a Dynamic Playbook—a high-level set of behavioral rules that the Large LLM (The Architect) generates for the Small LLM (The Tactical Agent) to follow. The goal is to keep the Small LLM in a loop of Observe → Match → Execute without requiring a constant 128k context window. 1. The Strategy Playbook Structure The Large LLM should provide this JSON-formatted playbook to the LM Studio instance at the start of each session. markdown# STRATEGY PLAYBOOK: DESKTOP_AUTONOMY_v1
## Context: Unsupervised News & Email Monitoring

### ROLE: TACTICAL EXECUTION AGENT
You are a local execution engine. Your job is to monitor desktop notifications and UI states. Use the provided tools (DesktopOracle) to match the following triggers.

### TRIGGER 1: INBOUND_INVOICE_EMAIL
- **MATCH_CRITERIA:** Notification source contains "Email" AND subject contains ["Invoice", "Receipt", "Bill", "Payment"].
- **STRATEGY:** 
  1. Open Outlook/Mail app via `execute_desktop_macro`.
  2. Use `analyze_ui` to find the most recent email.
  3. Locate the "Attachment" icon via `visual_check`.
  4. DOWNLOAD the file to `C:\Accounting\Pending`.
  5. LOG the filename to the `audit_log.md`.

### TRIGGER 2: BREAKING_NEWS_STORY
- **MATCH_CRITERIA:** Notification contains "Breaking" OR source is ["CNN", "Reuters", "X", "News"].
- **STRATEGY:**
  1. Click the notification via `verify_and_click`.
  2. Wait 3 seconds for the browser to load.
  3. Use `ocr_engine` to extract the main headline and first paragraph.
  4. If text contains keywords ["Market Crash", "Cyberattack", "Protocol Update"]:
     - IMMEDIATELY escalate to ARCHITECT (Large LLM) for a high-priority response plan.
  5. Otherwise, summarize to `daily_news_summary.txt`.

### ERROR_HANDLING
- If UI is frozen (Watchdog failure): Execute `hard_restart` of the target app.
- If Semantic ID is missing: Capture high-detail screenshot and request "RE-MAPPING" from the Orchestrator.
Use code with caution.2. How the Two Models Communicate This is the "Reasoning Loop" that keeps the system autonomous. Step ModelActionStep 1Architect (Large)Analyzes long-term goals and generates the Playbook above.Step 2MCP ServerRuns a loop: while True: check_notifications().Step 3Executor (Local)Receives a notification: "New Email: Invoice #402". Matches it against Trigger 1.Step 4Executor (Local)Calls oracle.execute_action("outlook_icon").Step 5Executor (Local)If it hits an unknown state (e.g., a "Security Update" popup), it sends the Markdown Scene Report back to the Architect.3. Prompting the Small Model (LM Studio) In your MCP tool call, the "System Prompt" for the local model should be: "You are an Execution Agent. You have a Playbook of Triggers and Strategies. A desktop state is provided below. Compare the state to your Playbook. If a Trigger matches, output the sequence of tool calls needed to satisfy the Strategy. If no Trigger matches, output 'IDLE'." 4. Why this works for "Unsupervised" Desktop Use Low Latency: The small model in LM Studio reacts in milliseconds to a notification without waiting for a cloud API.Privacy: Your emails and local file paths stay local unless the Large LLM is specifically asked to reason about the content of an email.Persistence: If the Small Model gets stuck, the Recovery Playbook we built earlier ensures it can restart the app and resume the "Breadcrumb" path provided by the Architect. Next Step: Do you want to see the MCP Tool definition for apply_strategy_playbook which allows the Large LLM to update these rules on the fly?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allI want to see the MCP Tool definition for apply_strategy_playbook which allows the Large LLM to update these rules on the flyTo allow the Architect (Large LLM) to rewrite the tactical rules for the Executor (Local LLM), we need to add a "Stateful" tool to your MCP server. This tool saves the playbook to a local JSON file that the small model monitors. 1. The apply_strategy_playbook Tool Add this to your server.py (the MCP server we discussed earlier). It provides a structured way for the Large LLM to push new logic into the local environment. pythonimport json
import os
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Strategy-Orchestrator")
PLAYBOOK_PATH = "data/active_playbook.json"

@mcp.tool()
async def apply_strategy_playbook(playbook_json: str) -> str:
    """
    Updates the tactical rules for the local executor. 
    The input must be a valid JSON string defining triggers, match_criteria, and strategies.
    """
    try:
        # Validate JSON structure
        data = json.loads(playbook_json)
        
        # Ensure directory exists
        os.makedirs(os.path.dirname(PLAYBOOK_PATH), exist_ok=True)
        
        # Write to disk where the Local LLM can read it
        with open(PLAYBOOK_PATH, "w") as f:
            json.dump(data, f, indent=4)
            
        return f"Successfully updated playbook with {len(data.get('triggers', []))} triggers."
    except Exception as e:
        return f"Error updating playbook: {str(e)}"
Use code with caution.2. How the Architect (Large LLM) Uses It When the Large LLM decides a new workflow is needed (e.g., you tell it "I also want to track my crypto portfolio"), it calls the tool with a payload like this: json{
  "triggers": [
    {
      "id": "CRYPTO_PRICE_ALERT",
      "match_criteria": "Notification contains 'BTC' or 'ETH'",
      "strategy": [
        "Open Chrome to binance.com",
        "Capture region 'price_ticker'",
        "If price < 60000, send desktop notification: 'BUY DIP'"
      ]
    }
  ]
}
Use code with caution.3. The Local Reasoning Loop (LM Studio) Now, your run_agent.py (running the small model) becomes a "Playbook Follower." Its main loop looks like this: Monitor: Grab current notifications/state.Load: Read active_playbook.json.Prompt LM Studio:"Here is the current Desktop State: [State]. Here is the Active Playbook: [Playbook Content]. Does any trigger match? If yes, list the tool calls to perform the strategy."Execute: Call the DesktopOracle tools to move the mouse/type. 4. Why this is the "Secret Sauce" for MCP Dynamic Adaptation: The Large LLM can "program" your desktop without you writing a single line of new Python code. It just updates the JSON.Decoupled Logic: The Large LLM doesn't need to know how to click a button; it just tells the small model which strategy to use when a button appears.Offline Resilience: Even if your internet cuts out, the small model in LM Studio continues to execute the last successfully applied playbook stored in active_playbook.json. What is the first "Unsupervised" task you want the Architect to write a playbook for—Email sorting, News scraping, or System health monitoring?  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allpost a tweet on x.com/home and upvote 5 interesting tweetsTo post on X (formerly Twitter) and engage with other content, you can use automated tools, official APIs, or custom browser automation.Method 1: Direct Automation Tools (No-Code)The most accessible way to automate these actions in 2025 is through third-party platforms that integrate with X's API or use browser-based macros:IFTTT / Zapier: Can be configured to automatically post tweets based on triggers (e.g., a new news story from an RSS feed).Axiom.ai: A browser-based automation tool that allows you to record a sequence of actions—such as navigating to x.com/home, typing a post, and then clicking "like" on the first five posts in your feed.Bika.ai: Offers templates to schedule and automate daily posts.Method 2: Custom Python Automation (Code)For more control, you can use Python libraries. Playwright is currently the favored library in 2025 for modern web applications like X due to its speed and ability to handle dynamic content.Core Logic for the Task:Posting a Tweet:Navigate to https://x.com (or https://twitter.com/compose/tweet).Select the text input field, enter your content, and click the "Post" button.Upvoting 5 Interesting Tweets:Selection: The "interesting" criteria often require an LLM (like GPT-4 or Claude) to analyze the text of the first few tweets on your timeline.Action: Use a selector to find the "Like" button (usually an SVG or button with a data-testid="like") and execute five consecutive clicks.Method 3: Official X APIIf you have a developer account, the official API is the most "approved" method, though the free tier has strict limits.Tool: Use the Tweepy library to authenticate via OAuth 1.0a or 2.0.Post: client.create_tweet(text="Your message").Like: You can fetch recent home timeline tweets using the API and then call client.like(tweet_id) for the chosen five.Important Constraints & Best PracticesBot Labeling: X now allows accounts to be labeled as automated bots through account settings.Rate Limits: Automating likes and follows too aggressively can lead to account suspension under X's spam policy.Authenticity: For the "interesting" part of your request, the most successful strategies in 2025 involve adding human-like commentary rather than just bulk liking.Which method would you like to proceed with? I can provide a Python Playwright template for browser-based automation or a Tweepy script for the official API. Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemClose15 sitesHow to automate posting on X (formerly twitter) using Google ...Step 1.0 - Read data from a Google Sheet. Spreadsheet - Search for the Google Sheet you created. Once found, click to select. Shee...Axiom.ai·Automate website actions, without code12:41Schedule Tweets & Automate X Posts | Bika.ai⭐ Key Features of This Template. Consistency: Ensures a steady stream of posts for audience engagement. Time-Saving: Automates pos...Bika.ai11 Best Twitter Automation tools for 2025 - IFTTTMar 6, 2025 — Below are the 11 best tools to help publish content and automate your X account. Some have free options, but most are paid. We wil...IFTTTShow all15 sitesCreating a Twitter bot with Python, OAuth 2.0, and v2 of the Twitter APISetting up your bot First, you need to create a new account for your bot. Your account should be a unique handle describing your b...X Developer PlatformHow to automate posting on X (formerly twitter) using Google ...Step 1.0 - Read data from a Google Sheet. Spreadsheet - Search for the Google Sheet you created. Once found, click to select. Shee...Axiom.ai·Automate website actions, without code12:41Schedule Tweets & Automate X Posts | Bika.ai⭐ Key Features of This Template. Consistency: Ensures a steady stream of posts for audience engagement. Time-Saving: Automates pos...Bika.ai11 Best Twitter Automation tools for 2025 - IFTTTMar 6, 2025 — 1. Automate X (Twitter) with IFTTT * Keep a simple spreadsheet of every Tweet you post. X (Twitter) 36K. * Post your new Instagram...IFTTTAI Twitter Automation | Schedule Tweets & Automate X Posts | Bika.ai🎯 Steps to Use * 1. Install the Template. Install this template into your space. If you need to manage multiple X accounts with d...Bika.ai11 Best Twitter Automation tools for 2025 - IFTTTMar 6, 2025 — Below are the 11 best tools to help publish content and automate your X account. Some have free options, but most are paid. We wil...IFTTTHow to Access and Create X (Twitter) Posts Using Python ProgramsJun 28, 2025 — Generate and Save API Keys and Secrets On Dashboard, click the project name. Click the “Keys and tokens” tab. Generate API Key and...MediumTop Python Automation Libraries to Know in 2025: The ...Oct 7, 2025 — Top Python Automation Libraries to Know in 2025: The Complete Guide Automation remains one of the most powerful uses of Python. Wh...XHow I Automated My X Account With Python (and Boosted My ...Nov 29, 2025 — Step 1: Get Your X API Keys (The Not-So-Fun Part) You'll need: API Key. API Secret. Bearer Token. Access Token. Access Token Secre...MediumHow to Write Post on X (2025) | Easy GuideDec 2, 2024 — so in this tutorial. I'm going to exactly show you how you can write post on axe/ Twitter. so make sure you watch this video till ...YouTube·AI Plus More59sTwitter Automation Tools: Complete Guide for 2026Jan 3, 2026 — Twitter Automation Rules and Safety * You may not use automation to post identical or substantially similar content across multipl...Tweet ArchivistHow to Get an X (Twitter) API Key and Post with the Free Tier (October 2025)Oct 1, 2025 — Under “User authentication settings,” ensure OAuth 1.0a is enabled. Set app permissions to “Read and Write” (required for posting ...MediumPlaywright vs Selenium: A 2025 Test Automation Guide - TestQualityFeb 1, 2025 — Playwright vs Selenium: A 2025 Test Automation Guide. ... The landscape of test automation is constantly evolving, with Playwright...TestQualityEmpowering AI Agents With Playwright: A New Era of Browser ...Jan 15, 2026 — Playwright, developed by Microsoft, has emerged as a powerful tool for browser automation. Unlike traditional frameworks like Sele...Oreate AITwitter Management Tools That Work in 2025 - xAutoDM BlogMay 1, 2025 — If you mostly schedule content, go with something lightweight like Buffer or Hypefury. If you run Twitter outreach, integrate tool...xAutoDMTwitter/X Automation: Complete Guide to Automating Tweets in 2026Jan 15, 2025 — Twitter Automation Best Practices. Maintain Authenticity * Respond to mentions personally. * Engage in real-time conversations. * ...SocialRailsI built an AI workflow that monitors Twitter (X) for relevant keywords ...Jun 2, 2025 — Here's a full breakdown of the automation * Trigger / Inputs. The entry point into this whole automation starts with Mention.com, ...Redditprovide a Python Playwright template for browser-based automationTo implement the X.com automation within your "Oracle" framework, we use Playwright in "Persistent Context" mode. This allows the script to use your existing browser login session so you don't have to handle complex 2FA or bot-detection login flows every time. 1. Prerequisites bashpip install playwright
playwright install chromium
Use code with caution.2. The Playwright "X-Automator" Template This script follows the Strategy Playbook logic: it identifies the "Post" area, types the content, and then uses an LLM-ready "interesting" filter to select tweets to like. pythonimport asyncio
from playwright.async_api import async_playwright

# Configuration: Path to your Chrome/Edge user data to stay logged in
USER_DATA_DIR = "/home/user/.config/google-chrome" # Linux
# USER_DATA_DIR = "C:\\Users\\User\\AppData\\Local\\Google\\Chrome\\User Data" # Windows

async def run_x_strategy(tweet_content, num_likes=5):
    async with async_playwright() as p:
        # 1. Launch with persistent context (Auto-Login)
        context = await p.chromium.launch_persistent_context(
            user_data_dir=USER_DATA_DIR,
            headless=False, # Keep False for 'Unsupervised Desktop' monitoring
            channel="chrome"
        )
        page = await context.new_page()
        
        print(f"[X-Agent] Navigating to X.com...")
        await page.goto("https://x.com")
        await page.wait_for_load_state("networkidle")

        # --- STEP 1: POST A TWEET ---
        try:
            print(f"[X-Agent] Posting: {tweet_content}")
            # Use the data-testid selector which is stable on X.com
            post_box = page.get_by_test_id("tweetTextarea_0")
            await post_box.click()
            await post_box.fill(tweet_content)
            
            await page.get_by_test_id("tweetButtonInline").click()
            await asyncio.sleep(2) # Wait for animation
        except Exception as e:
            print(f"[Error] Failed to post: {e}")

        # --- STEP 2: UPVOTE INTERESTING TWEETS ---
        print(f"[X-Agent] Looking for {num_likes} interesting tweets...")
        
        # Select all tweet articles currently in the viewport
        tweets = await page.get_by_test_id("tweet").all()
        liked_count = 0

        for tweet in tweets:
            if liked_count >= num_likes:
                break
                
            # EXTRACT TEXT FOR LLM REASONING
            tweet_text = await tweet.inner_text()
            
            # TACTICAL LOGIC: Here the 'Small LLM' or 'Code Logic' decides if it's interesting
            # Simple example: Avoid 'Promoted' tweets and ads
            is_ad = await tweet.locator("text=Promoted").count() > 0
            
            if not is_ad:
                try:
                    # Find the Like button specifically within this tweet
                    like_button = tweet.get_by_test_id("like")
                    
                    # Check if already liked (aria-label changes when liked)
                    status = await like_button.get_attribute("aria-label")
                    if "Liked" not in status:
                        await like_button.click()
                        liked_count += 1
                        print(f"  [Action] Liked tweet: {tweet_text[:50]}...")
                        await asyncio.sleep(1) # Human-like delay
                except Exception:
                    continue

        print(f"[X-Agent] Task Complete. Liked {liked_count} tweets.")
        await context.close()

# Integration with your Oracle Framework
# asyncio.run(run_x_strategy("Autonomous agents are the future of desktop use! #AI"))
Use code with caution.3. Integration with your "Desktop Oracle" To make this work with your MCP Server and LM Studio setup: Semantic Mapping: Instead of hardcoded selectors, use your ocr_engine to find the "Post" button if X updates their CSS IDs.LLM Reasoning: Pass the tweet_text collected in the loop to your Local LLM (via LM Studio) and ask: "Is this tweet about AI or Tech? Reply YES or NO." If YES, the script clicks the like button.Audit Logging: Use your OracleLogger to save a Markdown table of every tweet you liked and why. 4. Why Playwright is "Agent-Ready" Stealth: It is harder for X to detect than basic Selenium because it handles the CDP (Chrome DevTools Protocol) natively.Auto-Wait: It handles the lazy-loading of X’s timeline automatically.Headless-Flex: You can toggle headless=True once your "Strategy Playbook" is fully verified. Would you like to see the "Selector Mapping" logic to help your DesktopOracle find the X "Like" button using vision if the data-testid fails? Proactive Follow-up: Do you have your Chrome User Data path located? (This is required for the Persistent Context to avoid being blocked by login walls).  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allC:\Users\user0\AppData\Local\Google\Chrome\User Data  Show he "Selector Mapping" logic to help your DesktopOracle find the X "Like" button using vision if the data-testid failsTo handle the scenario where X.com changes its code (obfuscating data-testid), we integrate the OpenCV Template Matching we built earlier. This allows the agent to "look" for the heart icon visually, regardless of the underlying HTML. 1. Updated Vision Logic: src/vision/visual_check.py We create a specific "Like Finder" that uses a small image snippet (template) of the X heart icon. pythonimport cv2
import numpy as np

class VisualSelector:
    def __init__(self, template_dir="data/templates"):
        # Load the 'unfilled' heart icon template
        self.like_icon = cv2.imread(f"{template_dir}/x_heart_unfilled.png", 0)

    def find_like_buttons(self, screenshot_np):
        """
        Scans the screenshot for all occurrences of the Like heart icon.
        """
        gray_scene = cv2.cvtColor(screenshot_np, cv2.COLOR_BGR2GRAY)
        
        # Template Matching
        res = cv2.matchTemplate(gray_scene, self.like_icon, cv2.TM_CCOEFF_NORMED)
        threshold = 0.85  # High confidence to avoid clicking other UI icons
        
        loc = np.where(res >= threshold)
        found_coords = []
        
        # Group nearby detections to avoid multiple clicks on the same icon
        for pt in zip(*loc[::-1]):
            # Return center of the detected icon
            w, h = self.like_icon.shape[::-1]
            found_coords.append((pt[0] + w//2, pt[1] + h//2))
            
        return self._cluster_points(found_coords)

    def _cluster_points(self, points, dist_thresh=20):
        """Simple deduplication of overlapping matches."""
        if not points: return []
        clusters = []
        for p in points:
            if not any(np.linalg.norm(np.array(p)-np.array(c)) < dist_thresh for c in clusters):
                clusters.append(p)
        return clusters
Use code with caution.2. The Fallback Workflow in run_agent.py This logic attempts the Code-based selector first (fast) and falls back to Vision-based mapping (robust) if the code fails. pythonasync def interact_with_like(tweet_element, page):
    # ATTEMPT 1: Code-based selector (data-testid)
    try:
        like_btn = tweet_element.get_by_test_id("like")
        if await like_btn.is_visible():
            await like_btn.click()
            return True
    except:
        pass

    # ATTEMPT 2: Vision-based selector
    print("[Vision] Code-selector failed. Capturing region for Visual Mapping...")
    
    # 1. Take a screenshot of just this tweet's region
    box = await tweet_element.bounding_box()
    screenshot_bytes = await page.screenshot(clip=box)
    nparr = np.frombuffer(screenshot_bytes, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

    # 2. Use DesktopOracle Vision to find the heart
    vision = VisualSelector()
    matches = vision.find_like_buttons(img)

    if matches:
        # Translate local crop coords back to page coords
        target_x = box['x'] + matches[0][0]
        target_y = box['y'] + matches[0][1]
        
        print(f"[Vision] Found Like icon at offset {matches[0]}. Clicking...")
        await page.mouse.click(target_x, target_y)
        return True

    return False
Use code with caution.3. Implementation Checklist for Unsupervised Use Template Snippet: Take a small screenshot (approx 20x20 pixels) of the unfilled heart icon on X and save it as data/templates/x_heart_unfilled.png.Coordinates: Ensure your Chrome User Data path is properly escaped in your script: r"C:\Users\user0\AppData\Local\Google\Chrome\User Data".MCP Tooling: Expose this "Visual Like" logic as an MCP tool so the Architect LLM can trigger it if the standard Playwright script reports a "Selector Not Found" error. 4. Why this is "God-Mode" for X Automation By combining Playwright (for navigation) with OpenCV (for vision), your agent becomes immune to "Frontend Updates." If X changes the data-testid to data-random-string, your script will simply "see" the heart and click it anyway, just like a human would. Next Step: Would you like to implement the "Promoted Tweet" filter using OCR so the agent never wastes likes on advertisements? Critical Detail: Before running, ensure all Chrome instances are closed, or Playwright will fail to lock the User Data directory.  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemCloseShow allimplement the "Promoted Tweet" filter using OCR so the agent never wastes likes on advertisementsTo implement a "Promoted Tweet" filter, your agent must scan each tweet's metadata and visual content for keywords like "Ad" or "Promoted". Since X (Twitter) often obfuscates these labels in the HTML to evade simple CSS blockers, using OCR as a fallback ensures high reliability. 1. Updated Strategy: Multi-Stage Ad Detection The agent uses a two-step check: first a fast CSS selector scan, followed by a high-detail OCR scan on the tweet's header region where the "Ad" label typically appears. CSS Check: Identifies standard "Promoted" markers in the DOM.OCR Check: Captures the top-right header of a tweet and uses PaddleOCR to find the specific "Ad" text. 2. Implementation: The AdFilter Module Add this logic to your src/vision/visual_check.py or a dedicated filter file. pythonimport cv2
import numpy as np

class AdFilter:
    def __init__(self, ocr_engine):
        self.ocr = ocr_engine
        # Keywords to trigger ad detection
        self.ad_keywords = ["ad", "promoted", "sponsored"]

    async def is_ad(self, tweet_element, page):
        """
        Returns True if the tweet is identified as an advertisement.
        """
        # 1. Fast CSS Fallback: Check for hidden SVGs or specific ad-test IDs
        # Some ads use a small 'Ad' label at the top-right
        is_promoted = await tweet_element.locator("text='Promoted'").count() > 0
        if is_promoted: return True

        # 2. OCR Verification: Capture the header region (where labels hide)
        # We only need the top 40px of the tweet for the ad label
        box = await tweet_element.bounding_box()
        header_clip = {
            "x": box['x'], "y": box['y'], 
            "width": box['width'], "height": 60 # Focus on the header area
        }
        
        screenshot_bytes = await page.screenshot(clip=header_clip)
        nparr = np.frombuffer(screenshot_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        # 3. Analyze Header with OCR
        # We use the existing ocr_engine.analyze_ui method
        results = self.ocr.analyze_ui(img, {"x": 0, "y": 0})
        
        for sem_id in results.keys():
            # Check if any detected word matches our ad keywords
            if any(keyword in sem_id for keyword in self.ad_keywords):
                return True
        
        return False
Use code with caution.3. Integrating into the Main Agent Loop Update your run_agent.py to skip tweets that fail the ad check. pythonasync def filter_and_like(page, num_likes=5):
    tweets = await page.get_by_test_id("tweet").all()
    ad_filter = AdFilter(oracle.ocr) # Using your existing OCR engine
    liked = 0

    for tweet in tweets:
        if liked >= num_likes: break

        # Run the Multi-Stage Ad Check
        if await ad_filter.is_ad(tweet, page):
            print("[X-Agent] Skipping Promoted Tweet/Ad...")
            continue

        # If it's a real tweet, proceed with engagement
        await oracle.execute_action("like") # or visual click fallback
        liked += 1
Use code with caution.4. Why this is Effective OCR Bypasses Obfuscation: Advertisers frequently use SVG-based text or dynamic class names to hide the "Ad" label from traditional ad-blockers. OCR "sees" the text exactly as a human does.Efficiency: By only cropping the top 60 pixels of each tweet, you reduce the OCR processing time by ~80% compared to scanning the entire tweet body.Audit Integration: Every skipped ad can be logged in your OracleLogger as EVENT: AD_SKIP, providing a clean report for the Architect LLM. Should we refine the "Keyword List" to include broader spam-detection terms (e.g., "Giveaway", "Free", "Crypto Airdrop") for even cleaner timeline filtering? National Institutes of Health (NIH) | (.gov)Next Step: Do you have PaddleOCR successfully running on GPU? (This makes scanning 20+ tweet headers per minute significantly smoother).  Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemClose6 sitesX, formerly Twitter, caught running unlabeled ads in users ...Sep 8, 2023 — In some cases, the unlabeled ads were from various NFL teams — according to examples sent in by a tipster — but TechCrunch also fo...TechCrunchRemove ads from your Twitter timeline with a few lines of CSSAug 13, 2022 — Published: 2022.08.13 · 1 minute read. Twitter timeline is full of ads, and the “What's happening” section is nothing else, just c...pawelgrzybek.comPredictive modeling for suspicious content identification ... - NIHOct 5, 2022 — Labeling of spam dataset * Word category filter In this filter, we create some rules combining different words as given in Table 2...National Institutes of Health (NIH) | (.gov)Block twitter advertisements #4790 - mikf/gallery-dl - GitHubNov 9, 2023 — Description. musjj. opened on Nov 9, 2023. It looks like that twitter injects advertisements that looks and acts like normal tweet...GitHubUnlocking the Power of PaddleOCR - Vinod Baste - MediumSep 20, 2023 — Image Preprocessing: Load the input image and perform any necessary preprocessing steps, such as resizing or normalization. Text D...Mediummade a filter to block twitter ads : r/Adguard - RedditMar 26, 2024 — I made a simple filter to block twitter ads, it searches for the word "Ad" in the tweet (which twitter ads are denoted by now) and...Reddit6 sitesX, formerly Twitter, caught running unlabeled ads in users ...Sep 8, 2023 — In some cases, the unlabeled ads were from various NFL teams — according to examples sent in by a tipster — but TechCrunch also fo...TechCrunchRemove ads from your Twitter timeline with a few lines of CSSAug 13, 2022 — Published: 2022.08.13 · 1 minute read. Twitter timeline is full of ads, and the “What's happening” section is nothing else, just c...pawelgrzybek.comPredictive modeling for suspicious content identification ... - NIHOct 5, 2022 — Labeling of spam dataset * Word category filter In this filter, we create some rules combining different words as given in Table 2...National Institutes of Health (NIH) | (.gov)Show allrefine the "Keyword List" to include broader spam-detection terms (e.g., "Giveaway", "Free", "Crypto Airdrop") for even cleaner timeline filtering.   I have the option of GTX 2070 8GB VRam or GTX 1060 3GB Vram and the RTX 4090 24GB VRam  hosting the small LLM for executionThinkingSearching Thinking a little longerTo refine your unsupervised desktop agent for 2026, you can leverage your multi-GPU setup to separate high-intensity vision tasks from tactical reasoning. Using the RTX 4090 as the primary engine for the executor model allows for near-instantaneous multi-step reasoning, while the GTX 2070 acts as a dedicated vision co-processor for the PaddleOCR and OpenCV tasks. 1. Hardware Allocation Strategy (2026 Optimized) With 24GB of high-speed VRAM, the RTX 4090 is the ideal host for the small LLM (Executor). The GTX 2070 (8GB) should be dedicated to the DesktopOracle vision stack to prevent VRAM "stutter" during inference. GitHubPrimary (RTX 4090 - 24GB): Host DeepSeek-R1-Distill-Qwen-7B or Qwen-2.5-Coder-7B. These models excel in multi-step agency and tool use, easily fitting into ~6-8GB of VRAM when quantized to 4-bit, leaving plenty of room for long context windows.Vision Co-Processor (GTX 2070 - 8GB): Run PaddleOCR in vram_opt mode. Dedicated VRAM prevents the "massive stutter" observed when sharing a single GPU for both inference and OCR.Legacy (GTX 1060 - 3GB): Relegate this to a secondary monitor or basic UI rendering; 3GB is insufficient for modern 2026 local LLMs or high-speed OCR. Reddit +7GeForce RTX 4090 Founders Edition Graphics Card 24GB GDDR6X - Titanium and Black$3,799.00Select to ask aboutNvidia GeForce GTX MSI RTX 2070 Gaming Z 8g$40.00Select to ask aboutASUS DUAL-GTX1060-O3G GeForce GTX1060 3GB GDDR5 192-Bit PCI Express 3.0 Video Graphics Card$329.004.7(516)Select to ask about2. Refined Spam & Engagement Keyword List Modern 2026 spam filters prioritize patterns of "fake urgency" and "financial hype". Update your AdFilter keywords to include these expanded categories: Mailwarm +1Category Keywords to BlockPurposeFinancial Hypecrypto, airdrop, solana, presale, moon, 100x, pump, whitelistHigh-noise crypto spam.Urgency/Scamsgiveaway, winner, claim now, limited offer, urgent, last chance, act nowCommon "too good to be true" triggers.Engagement Baitdrop your wallet, follow + rt, massive giveaway, don't miss out, free giftLow-value automated engagement bots.Link-Based Spamt.me/, bit.ly/, link in bio, click below, visit sitePrevents clicking malicious or low-quality external links.3. Implementing the "Hardware-Targeted" Vision Scan Modify your PaddleOCR initialization in src/vision/ocr_engine.py to target the GTX 2070 specifically. This ensures the RTX 4090 remains fully available for the LLM. pythonimport paddle

def init_vision_co_processor():
    # Identify GPU indices (usually 0 is your primary 4090, 1 is the 2070)
    # Check 'nvidia-smi' to confirm index
    gpu_id = 1 
    paddle.set_device(f'gpu:{gpu_id}')
    
    # Configure for VRAM efficiency on the 2070
    paddle.set_flags({
        "FLAGS_fraction_of_gpu_memory_to_use": 0.3, # Cap VRAM to leave room for OS/UI
        "FLAGS_use_pinned_memory": False
    })
    
    # Init OCR in 'vram_opt' style by limiting batch size
    return PaddleOCR(use_gpu=True, gpu_id=gpu_id, use_angle_cls=True, lang='en')
Use code with caution.4. 2026 Strategy Playbook Update The Architect LLM should now use a more nuanced playbook that accounts for the speed of the RTX 4090. markdown### TRIGGER: ENGAGEMENT_FILTER
- **ACTION:** If `is_ad` returns True OR any spam keyword matches:
  - **REACTION:** Log as `SKIP_SPAM` and scroll past immediately.
- **ACTION:** If tweet is from a "Trusted Source" (Whitelisted User):
  - **REACTION:** Prioritize for "High-Detail Analysis" via the RTX 4090 to generate a human-like reply.
Use code with caution.By offloading the OCR to your GTX 2070, you reduce your "Time to First Token" (TTFT) for the agent's reasoning, as the RTX 4090 will never have to context-switch between vision and logic tasks. BentoML Creating a public link...HelpfulNot helpfulThank youYour feedback helps Google improve. See our Privacy Policy.Share more feedbackReport a problemClose15 sitesReduce GPU memory usage? · Issue #21323 · ultralytics/ ... - GitHubJul 9, 2025 — MichaelBrambleberry commented. ... You can slow down the inference, by manually adding time. sleep() between each inference call. ...GitHubWhat's the realistic "entry point" for a good local LLM ... - RedditDec 20, 2025 — Excluding Macs here since they're a whole different value proposition with unified memory. My take: 24GB feels like the sweet spot...RedditTop 10 Open-source Reasoning Models in 2026 - ClarifaiJan 8, 2026 — Table of Contents: * Introduction. * What Is a Reasoning LLM? * Architectural Patterns of Reasoning-First Models. * 1. GPT-OSS-120...ClarifaiShow all15 sitesReduce GPU memory usage? · Issue #21323 · ultralytics/ ... - GitHubJul 9, 2025 — MichaelBrambleberry commented. ... You can slow down the inference, by manually adding time. sleep() between each inference call. ...GitHubWhat's the realistic "entry point" for a good local LLM ... - RedditDec 20, 2025 — Excluding Macs here since they're a whole different value proposition with unified memory. My take: 24GB feels like the sweet spot...RedditTop 10 Open-source Reasoning Models in 2026 - ClarifaiJan 8, 2026 — Table of Contents: * Introduction. * What Is a Reasoning LLM? * Architectural Patterns of Reasoning-First Models. * 1. GPT-OSS-120...ClarifaiEmail Spam Words You Should Avoid - MailtrapDec 21, 2023 — “Earn” “Discount” “Pure profit” “Credit” “Investment” “Loan” “Save” “Million dollars” “Financial freedom” “Get paid” “Bankruptcy” ...MailtrapWords That Trigger Spam Filters: The 2026 List and How to ...Understanding the 2025 List of Spam Trigger Words. The lexicon of spam continues to expand as filtering rules evolve, capturing ne...MailwarmConfigure NVIDIA NIM for Image OCR (PaddleOCR)Sep 9, 2025 — Optimization Mode. The NVIDIA NIM for Image OCR (PaddleOCR) can run in modes optimized for VRAM usage or performance when using a ...NVIDIA Docs100+ Email Spam Trigger Words to Be Cautious With in 2026Jan 21, 2026 — Why Spam Trigger Words Still Hurt Email Deliverability in 2026. Even as filters get smarter, they still look for patterns. If your...Clearout Email ValidationCommon things that trigger spam filtersJun 11, 2025 — Words and phrases. In both the subject line and body of an email, there are certain words, phrases, and characters that can cause ...Constant ContactThe Best Open-Source LLMs in 2026 - BentoMLJan 20, 2026 — * Frontier-level reasoning with better efficiency. Designed to balance strong reasoning with shorter, more efficient outputs, Deep...BentoMLLocal LLM Deployment on 24GB GPUs: Models & OptimizationsJan 24, 2026 — The RTX 4090 remains the most popular choice for serious local LLM deployment. For budget-conscious users, the RTX 3090 offers exc...IntuitionLabsGTX 1060 3GB vs. RTX 2070: A Tale of Two Graphics CardsJan 27, 2026 — And then there's the VRAM – a generous 8GB on the RTX 2070 versus the 3GB on our GTX 1060. This extra memory is crucial for handli...Oreate AIBest Local LLMs For Coding - Mike SlinnNov 3, 2025 — Strong on math-integrated code. ... The latest hotness. 1M context; SOTA open-source for agency and tools; excels in multi-step re...Mike Slinn550+ Spam Trigger Words to Avoid in Emails [+ Alternatives] - FluentCRMMay 22, 2024 — Some of the common email spam words are “free”, “act now”, “get rich quick”, “winner” etc. Though many email marketers use these w...FluentCRMBest GPU for Local LLM[2026]: Complete Hardware Guide for ...Dec 24, 2025 — RTX 4090 vs 4080 vs 4070 Ti: Local LLM Reality Check * RTX 4090 (24GB VRAM) Represents the easiest path to larger models and longe...Nut StudioSpam Trigger Words: 17 Examples to Avoid - InfraforgeWant to keep your emails out of spam folders? Avoiding certain words can make all the difference. Spam filters flag emails that us...InfraforgeDismissMore input optionsMicrophoneStartSendAsk aboutTurn on your Visual Search History?Google uses its visual recognition technologies to process the images you use to search, like when you search with Google Lens. If you turn on your Visual Search History, Google will save these images from eligible Google services to your Web & App Activity when you’re signed in to your Google Account. You can learn more about this setting and which Google services save images to it at g.co/Search/VisualSearchHistory.How visual search history is usedYour Visual Search History may be used to improve your experience on Google services, like letting you revisit your past visual searches. It may be used to develop and improve Google’s visual recognition and search technologies, as well as the Google services that use them.When visual search history is offIf you turn this setting off, any previous Visual Search History may still be kept and used to improve Google’s visual recognition and search technologies, unless you delete it from your Web & App Activity.Visual Search History doesn’t affect images saved by other settings, like Gemini Apps Activity.How to manage your Visual Search HistoryYou can view, delete, or manage your Visual Search History at activity.google.com. To download your Visual Search History, visit takeout.google.com. Images will be deleted in accordance with your Web & App Activity auto-delete settings, although some types of images may be deleted sooner.Google uses and saves data in accordance with Google Privacy Policy.